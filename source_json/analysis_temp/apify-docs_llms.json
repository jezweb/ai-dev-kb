[
  {
    "codeTitle": "Passing Cookies to New Context in Puppeteer",
    "codeDescription": "This code snippet shows how to create a new incognito browser context in Puppeteer, set previously retrieved cookies, and navigate to a page while maintaining logged-in status.",
    "codeLanguage": "javascript",
    "codeTokens": 172,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_6",
    "pageTitle": "Logging into a Website with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Create a fresh non-persistent browser context\nconst sendEmailContext = await browser.createIncognitoBrowserContext();\n// Create a new page on the new browser context and set its cookies\n// to be the same ones from the page we used to log into the website.\nconst page2 = await sendEmailContext.newPage();\nawait page2.setCookie(...cookies);\n\n// Notice that we are logged in, even though we didn't\n// go through the logging in process again!\nawait page2.goto('https://mail.yahoo.com/');\nawait page2.waitForTimeout(10000);"
      }
    ],
    "relevance": 0.995
  },
  {
    "codeTitle": "Complete Implementation of TypeScript Scraper",
    "codeDescription": "This snippet shows the final code for the TypeScript scraper, including all necessary imports, type definitions, and function implementations.",
    "codeLanguage": "typescript",
    "codeTokens": 344,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_8",
    "pageTitle": "TypeScript Mini-Project Documentation",
    "codeList": [
      {
        "language": "typescript",
        "code": "// index.ts\nimport axios from 'axios';\nimport { SortOrder } from './types';\n\nimport type { ResponseData, Product, UserInput, ModifiedProduct } from './types';\n\nconst fetchData = async () => {\n    const { data } = await axios('https://dummyjson.com/products?limit=100');\n\n    return data as ResponseData;\n};\n\nconst sortData = (products: Product[], order: SortOrder) => {\n    switch (order) {\n        case SortOrder.ASC:\n            return [...products].sort((a, b) => a.price - b.price);\n        case SortOrder.DESC:\n            return [...products].sort((a, b) => b.price - a.price);\n        default:\n            return products;\n    }\n};\n\nasync function scrape(input: UserInput<true>): Promise<ModifiedProduct[]>;\nasync function scrape(input: UserInput<false>): Promise<Product[]>;\nasync function scrape(input: UserInput) {\n    const data = await fetchData();\n\n    const sorted = sortData(data.products, input.sort as SortOrder);\n\n    if (input.removeImages) {\n        return sorted.map((item) => {\n            const { images, ...rest } = item;\n\n            return rest;\n        });\n    }\n\n    return sorted;\n}\n\nconst main = async () => {\n    const INPUT: UserInput<false> = { sort: 'ascending', removeImages: false };\n\n    const result = await scrape(INPUT);\n\n    console.log(result[0].images);\n};\n\nmain();"
      }
    ],
    "relevance": 0.995
  },
  {
    "codeTitle": "Scraping GitHub Repositories with Pagination using Puppeteer",
    "codeDescription": "This code snippet demonstrates how to scrape GitHub repositories from multiple pages using Puppeteer. It includes functions for scraping individual pages, handling pagination, and concurrent scraping of multiple pages.",
    "codeLanguage": "JavaScript",
    "codeTokens": 501,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_6",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\nimport * as cheerio from 'cheerio';\n\nconst repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;\n\n// Scrapes all repositories from a single page\nconst scrapeRepos = async (page) => {\n    const $ = cheerio.load(await page.content());\n\n    return [...$('.list-view-item')].map((item) => {\n        const repoElement = $(item);\n        return {\n            title: repoElement.find('h4').text().trim(),\n            description: repoElement.find('.repos-list-description').text().trim(),\n            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,\n        };\n    });\n};\n\nconst browser = await puppeteer.launch({ headless: false });\nconst firstPage = await browser.newPage();\n\nawait firstPage.goto(REPOSITORIES_URL);\n\nconst lastPageLabel = await firstPage.$eval(\n    'a[aria-label*=\"Page \"]:nth-last-child(2)',\n    (element) => element.getAttribute('aria-label'),\n);\nconst lastPageNumber = Number(lastPageLabel.replace(/\\D/g, ''));\n\n// Push all results from the first page to the repositories array\nrepositories.push(...(await scrapeRepos(page)));\n\nawait firstPage.close();\n\nconst pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);\nconst promises = pageNumbers.map((pageNumber) => (async () => {\n    const paginatedPage = await browser.newPage();\n\n    // Construct the URL by setting the ?page=... parameter to value of pageNumber\n    const url = new URL(REPOSITORIES_URL);\n    url.searchParams.set('page', pageNumber);\n\n    // Scrape the page\n    await paginatedPage.goto(url.href);\n    const results = await scrapeRepos(paginatedPage);\n\n    // Push results to the repositories array\n    repositories.push(...results);\n\n    await paginatedPage.close();\n})(),\n);\nawait Promise.all(promises);\n\n// For brievity logging just the count of repositories scraped\nconsole.log(repositories.length);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.99
  },
  {
    "codeTitle": "Complete Google Search Automation with Playwright",
    "codeDescription": "A full example of automating a Google search, including accepting cookies, typing a query, and clicking the first result using Playwright.",
    "codeLanguage": "JavaScript",
    "codeTokens": 169,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/interacting_with_a_page.md#2025-04-18_snippet_4",
    "pageTitle": "Interacting with a Page using Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\n// Click the \"Accept all\" button\nawait page.click('button:has-text(\"Accept all\")');\n\n// Type the query into the search box\nawait page.type('textarea[title]', 'hello world');\n\n// Press enter\nawait page.keyboard.press('Enter');\n\n// Click the first result\nawait page.click('.g a');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.99
  },
  {
    "codeTitle": "Creating Residential Proxy Configuration in Apify",
    "codeDescription": "Creates a proxy configuration using Apify's residential proxy group. This allows the crawler to rotate through different residential IP addresses to avoid being blocked.",
    "codeLanguage": "javascript",
    "codeTokens": 69,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/rotating_proxies.md#2025-04-18_snippet_0",
    "pageTitle": "Rotating Proxies and Sessions for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "const proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n});"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Implementing Lazy-Loading Pagination with Puppeteer",
    "codeDescription": "This code snippet demonstrates how to implement lazy-loading pagination for web scraping using Puppeteer. It includes logic for auto-scrolling and handling the end of the page.",
    "codeLanguage": "JavaScript",
    "codeTokens": 348,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_8",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\n// Create an array where all scraped products will\n// be pushed to\nconst products = [];\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.aboutyou.com/c/women/clothing-20204');\n\n// Grab the height of result item in pixels, which will be used to scroll down\nconst itemHeight = await page.$eval('a[data-testid*=\"productTile\"]', (elem) => elem.clientHeight);\n\n// Keep track of how many pixels have been scrolled down\nconst totalScrolled = 0;\n\nwhile (products.length < 75) {\n    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);\n\n    await page.mouse.wheel({ deltaY: itemHeight * 3 });\n    totalScrolled += itemHeight * 3;\n    // Allow the products 1 second to load\n    await page.waitForTimeout(1000);\n\n    // Data extraction login will go here\n\n    const innerHeight = await page.evaluate(() => window.innerHeight);\n\n    // if the total pixels scrolled is equal to the true available scroll\n    // height of the page, we've reached the end and should stop scraping.\n    // even if we haven't reach our goal of 75 products.\n    if (totalScrolled >= scrollHeight - innerHeight) {\n        break;\n    }\n}\n\nawait browser.close();"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Enhanced Playwright Scraper with Dynamic Content Handling",
    "codeDescription": "Complete implementation of a Playwright scraper that handles both static and dynamically loaded content, including recommended products section.",
    "codeLanguage": "javascript",
    "codeTokens": 319,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/headless_browser.md#2025-04-18_snippet_2",
    "pageTitle": "Headless Browser Web Scraping Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {\n        console.log(`Fetching URL: ${request.url}`);\n\n        if (request.label === 'start-url') {\n            await enqueueLinks({\n                selector: 'a.product-item__title',\n            });\n            return;\n        }\n\n        const $ = await parseWithCheerio();\n\n        const title = $('h1').text().trim();\n        const vendor = $('a.product-meta__vendor').text().trim();\n        const price = $('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($('span.rating__caption').text(), 10);\n        const description = $('div[class*=\"description\"] div.rte').text().trim();\n        const recommendedProducts = $('.product-recommendations a.product-item__title')\n            .map((i, el) => $(el).text().trim())\n            .toArray();\n\n        await Dataset.pushData({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n            recommendedProducts,\n        });\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n    label: 'start-url',\n}]);\n\nawait crawler.run();"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Implementing Lazy-Loading Pagination with Playwright",
    "codeDescription": "This code snippet demonstrates how to implement lazy-loading pagination for web scraping using Playwright. It includes logic for auto-scrolling and handling the end of the page.",
    "codeLanguage": "JavaScript",
    "codeTokens": 347,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_7",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\n// Create an array where all scraped products will\n// be pushed to\nconst products = [];\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.aboutyou.com/c/women/clothing-20204');\n\n// Grab the height of result item in pixels, which will be used to scroll down\nconst itemHeight = await page.$eval('a[data-testid*=\"productTile\"]', (elem) => elem.clientHeight);\n\n// Keep track of how many pixels have been scrolled down\nconst totalScrolled = 0;\n\nwhile (products.length < 75) {\n    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);\n\n    await page.mouse.wheel(0, itemHeight * 3);\n    totalScrolled += itemHeight * 3;\n    // Allow the products 1 second to load\n    await page.waitForTimeout(1000);\n\n    // Data extraction login will go here\n\n    const innerHeight = await page.evaluate(() => window.innerHeight);\n\n    // if the total pixels scrolled is equal to the true available scroll\n    // height of the page, we've reached the end and should stop scraping.\n    // even if we haven't reach our goal of 75 products.\n    if (totalScrolled >= scrollHeight - innerHeight) {\n        break;\n    }\n}\n\nawait browser.close();"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Complete Shopify Product Scraper Implementation in JavaScript",
    "codeDescription": "This is the full implementation of the Shopify product scraper, combining all previous snippets. It fetches the sales page, extracts product URLs, and then scrapes detailed information for each product, handling errors and logging progress.",
    "codeLanguage": "JavaScript",
    "codeTokens": 429,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_7",
    "pageTitle": "Web Scraping Tutorial: Crawling and Data Extraction",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconsole.log('Fetching products on sale.');\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    productUrls.push(absoluteUrl);\n}\n\nconsole.log(`Found ${productUrls.length} products.`);\n\nconst results = [];\nconst errors = [];\n\nfor (const url of productUrls) {\n    try {\n        console.log(`Fetching URL: ${url}`);\n        const productResponse = await gotScraping(url);\n        const $productPage = cheerio.load(productResponse.body);\n\n        const title = $productPage('h1').text().trim();\n        const vendor = $productPage('a.product-meta__vendor').text().trim();\n        const price = $productPage('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($productPage('span.rating__caption').text(), 10);\n        const description = $productPage('div[class*=\"description\"] div.rte').text().trim();\n\n        results.push({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n        });\n    } catch (error) {\n        errors.push({ url, msg: error.message });\n    }\n}\n\nconsole.log('RESULTS:', results);\nconsole.log('ERRORS:', errors);"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Launching Browser with Playwright/Puppeteer - Basic Example",
    "codeDescription": "Basic example of launching a browser instance using both Playwright and Puppeteer. By default, the browser launches in headless mode (no UI).",
    "codeLanguage": "javascript",
    "codeTokens": 69,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser.md#2025-04-18_snippet_0",
    "pageTitle": "Browser Automation with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nawait chromium.launch();\n\nconsole.log('launched!');"
      },
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nawait puppeteer.launch();\n\nconsole.log('launched!');"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Defining Router and Handlers for Amazon Scraping in JavaScript",
    "codeDescription": "This code snippet defines a Cheerio router with handlers for the initial search page and individual product pages. It demonstrates how to select products, extract data, and add new requests to the crawler.",
    "codeLanguage": "JavaScript",
    "codeTokens": 357,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/modularity.md#2025-04-18_snippet_0",
    "pageTitle": "Understanding Modularity in Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "// routes.js\nimport { createCheerioRouter } from 'crawlee';\nimport { BASE_URL } from './constants.js';\n\nexport const router = createCheerioRouter();\n\nrouter.addDefaultHandler(({ log }) => {\n    log.info('Route reached.');\n});\n\n// Add a handler to our router to handle requests with the 'START' label\nrouter.addHandler('START', async ({ $, crawler, request }) => {\n    const { keyword } = request.userData;\n\n    const products = $('div > div[data-asin]:not([data-asin=\"\"])');\n\n    // loop through the resulting products\n    for (const product of products) {\n        const element = $(product);\n        const titleElement = $(element.find('.a-text-normal[href]'));\n\n        const url = `${BASE_URL}${titleElement.attr('href')}`;\n\n        // scrape some data from each and to a request\n        // to the crawler for its page\n        await crawler.addRequests([{\n            url,\n            label: 'PRODUCT',\n            userData: {\n                // Pass the scraped data about the product to the next\n                // request so that it can be used there\n                data: {\n                    title: titleElement.first().text().trim(),\n                    asin: element.attr('data-asin'),\n                    itemUrl: url,\n                    keyword,\n                },\n            },\n        }]);\n    }\n});\n\nrouter.addHandler('PRODUCT', ({ log }) => log.info('on a product page!'));"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Sending Multiple Emails Concurrently with Playwright",
    "codeDescription": "This code snippet demonstrates how to send multiple emails concurrently using Playwright. It creates an array of promises, each handling the email sending process in a separate browser context with stored cookies.",
    "codeLanguage": "JavaScript",
    "codeTokens": 360,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_7",
    "pageTitle": "Logging into a Website with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Grab the cookies from the default browser context,\n// which was used to log in\nconst cookies = await browser.contexts()[0].cookies();\n\nawait page.close();\n\n// Create an array of promises, running the cookie passing\n// and email sending logic each time\nconst promises = emailsToSend.map(({ to, subject, body }) => (async () => {\n    // Create a fresh non-persistent browser context\n    const sendEmailContext = await browser.newContext();\n    // Add the cookies from the previous one to this one so that\n    // we'll be logged into Yahoo without having to re-do the\n    // logging in automation\n    await sendEmailContext.addCookies(cookies);\n    const page2 = await sendEmailContext.newPage();\n\n    await page2.goto('https://mail.yahoo.com/');\n\n    // Compose an email\n    await page2.click('a[aria-label=\"Compose\"]');\n\n    // Populate the fields with the details from the object\n    await page2.type('input#message-to-field', to);\n    await page2.type('input[data-test-id=\"compose-subject\"]', subject);\n    await page2.type('div[data-test-id=\"compose-editor-container\"] div[contenteditable=\"true\"]', body);\n\n    // Send the email\n    await page2.click('button[title=\"Send this email\"]');\n\n    await sendEmailContext.close();\n})(),\n);\n\n// Wait for all emails to be sent\nawait Promise.all(promises);"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Web Scraping Shopify Products with Python",
    "codeDescription": "A comprehensive Python script that scrapes product information from a Shopify store, including prices, variants, and other details. It uses httpx for HTTP requests, BeautifulSoup for HTML parsing, and includes functions for data extraction and export to CSV and JSON formats.",
    "codeLanguage": "python",
    "codeTokens": 596,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/11_scraping_variants.md#2025-04-18_snippet_2",
    "pageTitle": "Scraping Product Variants with Python",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\nimport csv\nimport json\nfrom urllib.parse import urljoin\n\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n\n    html_code = response.text\n    return BeautifulSoup(html_code, \"html.parser\")\n\ndef parse_product(product, base_url):\n    title_element = product.select_one(\".product-item__title\")\n    title = title_element.text.strip()\n    url = urljoin(base_url, title_element[\"href\"])\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price, \"url\": url}\n\ndef parse_variant(variant):\n    text = variant.text.strip()\n    name, price_text = text.split(\" - \")\n    price = Decimal(\n        price_text\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    return {\"variant_name\": name, \"price\": price}\n\ndef export_csv(file, data):\n    fieldnames = list(data[0].keys())\n    writer = csv.DictWriter(file, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\ndef export_json(file, data):\n    def serialize(obj):\n        if isinstance(obj, Decimal):\n            return str(obj)\n        raise TypeError(\"Object not JSON serializable\")\n\n    json.dump(data, file, default=serialize, indent=2)\n\nlisting_url = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nlisting_soup = download(listing_url)\n\ndata = []\nfor product in listing_soup.select(\".product-item\"):\n    item = parse_product(product, listing_url)\n    product_soup = download(item[\"url\"])\n    vendor = product_soup.select_one(\".product-meta__vendor\").text.strip()\n\n    if variants := product_soup.select(\".product-form__option.no-js option\"):\n        for variant in variants:\n            data.append(item | parse_variant(variant))\n    else:\n        item[\"variant_name\"] = None\n        data.append(item)\n\nwith open(\"products.csv\", \"w\") as file:\n    export_csv(file, data)\n\nwith open(\"products.json\", \"w\") as file:\n    export_json(file, data)"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Exporting Dataset to JSON with Crawlee",
    "codeDescription": "Example showing how to export scraped data to a JSON file using Dataset.exportToJSON() after the crawler completes its run.",
    "codeLanguage": "javascript",
    "codeTokens": 65,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/exporting_data.md#2025-04-18_snippet_2",
    "pageTitle": "Exporting Data with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "// ...\nawait crawler.run();\n// Add this line to export to JSON.\nawait Dataset.exportToJSON('results');"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Complete Lazy Loading Implementation with Playwright",
    "codeDescription": "Complete code for scraping a lazy-loaded e-commerce website using Playwright. It launches a browser, navigates to a product listing page, scrolls down to trigger lazy loading, and extracts product information until reaching a target number or the end of the page.",
    "codeLanguage": "javascript",
    "codeTokens": 468,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_10",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\nimport * as cheerio from 'cheerio';\n\nconst products = [];\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.aboutyou.com/c/women/clothing-20204');\n\n// Grab the height of result item in pixels, which will be used to scroll down\nconst itemHeight = await page.$eval('a[data-testid*=\"productTile\"]', (elem) => elem.clientHeight);\n\n// Keep track of how many pixels have been scrolled down\nlet totalScrolled = 0;\n\nwhile (products.length < 75) {\n    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);\n\n    await page.mouse.wheel(0, itemHeight * 3);\n    totalScrolled += itemHeight * 3;\n    // Allow the products 1 second to load\n    await page.waitForTimeout(1000);\n\n    const $ = cheerio.load(await page.content());\n\n    // Grab the newly loaded items\n    const items = [...$('a[data-testid*=\"productTile\"]')].slice(products.length);\n\n    const newItems = items.map((item) => {\n        const elem = $(item);\n\n        return {\n            brand: elem.find('p[data-testid=\"brandName\"]').text().trim(),\n            price: elem.find('span[data-testid=\"finalPrice\"]').text().trim(),\n        };\n    });\n\n    products.push(...newItems);\n\n    const innerHeight = await page.evaluate(() => window.innerHeight);\n\n    // if the total pixels scrolled is equal to the true available scroll\n    // height of the page, we've reached the end and should stop scraping.\n    // even if we haven't reach our goal of 75 products.\n    if (totalScrolled >= scrollHeight - innerHeight) {\n        break;\n    }\n}\n\nconsole.log(products.slice(0, 75));\n\nawait browser.close();"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Complete Scraper Actor Implementation",
    "codeDescription": "Full TypeScript implementation of the Scraper Actor that uses Cheerio to scrape an e-commerce website, utilizing the shared request queue and dataset.",
    "codeLanguage": "typescript",
    "codeTokens": 55,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_9",
    "pageTitle": "Parallel Website Scraping with Multiple Actor Runs",
    "codeList": [
      {
        "language": "typescript",
        "code": "// ScraperActorMainTs reference"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Saving HTML to File using Python",
    "codeDescription": "Python script demonstrating how to save downloaded HTML content to a file using pathlib",
    "codeLanguage": "python",
    "codeTokens": 78,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/04_downloading_html.md#2025-04-18_snippet_5",
    "pageTitle": "Python HTML Download Guide",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom pathlib import Path\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\nPath(\"products.html\").write_text(response.text)"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Parsing Cookies from API Response with set-cookie-parser",
    "codeDescription": "Demonstrates how to extract and format cookies from an API response using axios and set-cookie-parser. The code makes a request to a target site, parses the returned cookies, and formats them into a usable string for subsequent requests.",
    "codeLanguage": "javascript",
    "codeTokens": 204,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/cookies_headers_tokens.md#2025-04-18_snippet_0",
    "pageTitle": "API Authentication Guide with Cookies, Headers, and Tokens",
    "codeList": [
      {
        "language": "javascript",
        "code": "import axios from 'axios';\n\n// import the set-cookie-parser module\nimport setCookieParser from 'set-cookie-parser';\n\nconst getCookie = async () => {\n    // make a request to the target site\n    const response = await axios.get('https://www.example.com/');\n\n    // parse the cookies from the response\n    const cookies = setCookieParser.parse(response);\n\n    // format the parsed data into a usable string\n    const cookieString = cookies.map(({ name, value }) => `${name}=${value};`).join(' ');\n\n    // log the final cookie string to be used in a 'cookie' header\n    console.log(cookieString);\n};\n\ngetCookie();"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Node.js Context Parsing with Cheerio - Puppeteer Version",
    "codeDescription": "Complete example showing how to use Cheerio to parse page content in the Node.js context with Puppeteer. Extracts product information using Cheerio's jQuery-like syntax.",
    "codeLanguage": "javascript",
    "codeTokens": 207,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_4",
    "pageTitle": "Data Extraction Guide for Playwright and Puppeteer",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\nimport { load } from 'cheerio';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://demo-webstore.apify.org/search/on-sale');\n\nconst $ = load(await page.content());\n\nconst productCards = Array.from($('a[class*=\"ProductCard_root\"]'));\n\nconst products = productCards.map((element) => {\n    const card = $(element);\n\n    const name = card.find('h3[class*=\"ProductCard_name\"]').text();\n    const price = card.find('div[class*=\"ProductCard_price\"]').text();\n\n    return {\n        name,\n        price,\n    };\n});\n\nconsole.log(products);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Navigating to Website with Playwright",
    "codeDescription": "Demonstrates how to open a new page, navigate to Google.com, and implement a wait timeout using Playwright. Includes browser cleanup.",
    "codeLanguage": "javascript",
    "codeTokens": 120,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/index.md#2025-04-18_snippet_2",
    "pageTitle": "Opening and Controlling Pages in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\n// Open a new page\nconst page = await browser.newPage();\n\n// Visit Google\nawait page.goto('https://google.com');\n\n// wait for 10 seconds before shutting down\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Implementing Data Extraction with Crawlee",
    "codeDescription": "A complete example showing how to use CheerioCrawler to scrape product data from a Shopify store and save it to disk using Dataset.pushData(). The code demonstrates URL crawling, data extraction, and storage functionality.",
    "codeLanguage": "javascript",
    "codeTokens": 378,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_6",
    "pageTitle": "Professional Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "// To save data to disk, we need to import Dataset.\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request, enqueueLinks }) => {\n        console.log(`Fetching URL: ${request.url}`);\n\n        if (request.label === 'start-url') {\n            await enqueueLinks({\n                selector: 'a.product-item__title',\n            });\n            // When on the start URL, we don't want to\n            // extract any data after we extract the links.\n            return;\n        }\n\n        // We copied and pasted the extraction code\n        // from the previous lesson with small\n        // refactoring: e.g. `$productPage` to `$`.\n        const title = $('h1').text().trim();\n        const vendor = $('a.product-meta__vendor').text().trim();\n        const price = $('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($('span.rating__caption').text(), 10);\n        const description = $('div[class*=\"description\"] div.rte').text().trim();\n\n        // Instead of printing the results to\n        // console, we save everything to a file.\n        await Dataset.pushData({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n        });\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n    label: 'start-url',\n}]);\n\nawait crawler.run();"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Google Search Crawler with Proxy Rotation",
    "codeDescription": "Complete example of a Google search crawler implementing proxy rotation and handling common blocking scenarios like CAPTCHAs and failed page loads.",
    "codeLanguage": "javascript",
    "codeTokens": 230,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/handle_blocked_requests_puppeteer.md#2025-04-18_snippet_2",
    "pageTitle": "Handling Blocked Requests in Web Crawlers",
    "codeList": [
      {
        "language": "javascript",
        "code": "const crawler = new Apify.PuppeteerCrawler({\n    requestList: someInitializedRequestList,\n    launchPuppeteerOptions: {\n        useApifyProxy: true,\n    },\n    gotoFunction: async ({ request, page, puppeteerPool }) => {\n        const response = page.goto(request.url).catch(() => null);\n        if (!response) {\n            await puppeteerPool.retire(page.browser());\n            throw new Error(`Page didn't load for ${request.url}`);\n        }\n        return response;\n    },\n    handlePageFunction: async ({ request, page, puppeteerPool }) => {\n        if (page.url().includes('sorry')) {\n            await puppeteerPool.retire(page.browser());\n            throw new Error(`We got captcha for ${request.url}`);\n        }\n    },\n    retireInstanceAfterRequestCount: 50,\n});\n\nApify.main(async () => {\n    await crawler.run();\n});"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Complete LangGraph-Apify Integration Example",
    "codeDescription": "A complete Python script that demonstrates how to create a ReAct agent using LangGraph with Apify Actors tools to search for and analyze TikTok profiles. Includes all necessary imports, environment setup, and execution code.",
    "codeLanguage": "python",
    "codeTokens": 262,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_6",
    "pageTitle": "LangGraph Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "import os\n\nfrom langchain_apify import ApifyActorsTool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nos.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nbrowser = ApifyActorsTool(\"apify/rag-web-browser\")\ntiktok = ApifyActorsTool(\"clockworks/free-tiktok-scraper\")\n\ntools = [browser, tiktok]\nagent_executor = create_react_agent(llm, tools)\n\nfor state in agent_executor.stream(\n    stream_mode=\"values\",\n    input={\n        \"messages\": [\n            HumanMessage(content=\"Search the web for OpenAI TikTok profile and analyze their profile.\")\n        ]\n    }):\n    state[\"messages\"][-1].pretty_print()"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Implementing Basic Web Scraper with Crawlee in JavaScript",
    "codeDescription": "This code snippet demonstrates a basic web scraper using Crawlee to crawl a demo webstore, enqueue product links, and extract product data.",
    "codeLanguage": "javascript",
    "codeTokens": 307,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/using_proxies.md#2025-04-18_snippet_0",
    "pageTitle": "Using Proxies in Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "// crawlee.js\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request, enqueueLinks }) => {\n        if (request.label === 'START') {\n            await enqueueLinks({\n                selector: 'a[href*=\"/product/\"]',\n            });\n\n            // When on the START page, we don't want to\n            // extract any data after we extract the links.\n            return;\n        }\n\n        // We copied and pasted the extraction code\n        // from the previous lesson\n        const title = $('h3').text().trim();\n        const price = $('h3 + div').text().trim();\n        const description = $('div[class*=\"Text_body\"]').text().trim();\n\n        // Instead of saving the data to a variable,\n        // we immediately save everything to a file.\n        await Dataset.pushData({\n            title,\n            description,\n            price,\n        });\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://demo-webstore.apify.org/search/on-sale',\n    // By labeling the Request, we can identify it\n    // later in the requestHandler.\n    label: 'START',\n}]);\n\nawait crawler.run();"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Input Validation in Orchestrator Actor",
    "codeDescription": "TypeScript code for initializing the Actor and validating its input parameters, ensuring the required targetActorId is present and setting default values for other parameters.",
    "codeLanguage": "typescript",
    "codeTokens": 172,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_2",
    "pageTitle": "Parallel Website Scraping with Multiple Actor Runs",
    "codeList": [
      {
        "language": "typescript",
        "code": "import { Actor, log } from 'apify';\n\ninterface Input {\n    parallelRunsCount: number;\n    targetActorId: string;\n    targetActorInput: Record<string, any>;\n    targetActorRunOptions: Record<string, any>;\n}\n\nawait Actor.init();\n\nconst {\n    parallelRunsCount = 1,\n    targetActorId,\n    targetActorInput = {},\n    targetActorRunOptions = {},\n} = await Actor.getInput<Input>() ?? {} as Input;\nconst { apifyClient } = Actor;\n\nif (!targetActorId) throw new Error('Missing the \"targetActorId\" input!');"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Navigating to Website with Puppeteer",
    "codeDescription": "Shows how to open a new page, navigate to Google.com, and implement a wait timeout using Puppeteer. Includes browser cleanup.",
    "codeLanguage": "javascript",
    "codeTokens": 120,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/index.md#2025-04-18_snippet_3",
    "pageTitle": "Opening and Controlling Pages in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\n// Open a new page\nconst page = await browser.newPage();\n\n// Visit Google\nawait page.goto('https://google.com');\n\n// wait for 10 seconds before shutting down\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Complete Email Sending Automation with Puppeteer",
    "codeDescription": "This code snippet shows the complete implementation of email sending automation using Puppeteer. It includes logging into Yahoo, storing cookies, and sending multiple emails concurrently.",
    "codeLanguage": "JavaScript",
    "codeTokens": 498,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_10",
    "pageTitle": "Logging into a Website with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst emailsToSend = [\n    {\n        to: 'alice@example.com',\n        subject: 'Hello',\n        body: 'This is a message.',\n    },\n    {\n        to: 'bob@example.com',\n        subject: 'Testing',\n        body: 'I love the academy!',\n    },\n    {\n        to: 'carol@example.com',\n        subject: 'Apify is awesome!',\n        body: 'Some content.',\n    },\n];\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Login logic\nawait page.goto('https://www.yahoo.com/');\n\nawait Promise.all([page.waitForSelector('a[data-ylk*=\"sign-in\"]'), page.click('button[name=\"agree\"]')]);\nawait Promise.all([page.waitForNavigation(), page.click('a[data-ylk*=\"sign-in\"]')]);\n\nawait page.type('input[name=\"username\"]', 'YOUR-LOGIN-HERE');\nawait Promise.all([page.waitForNavigation(), page.click('input[name=\"signin\"]')]);\n\nawait page.type('input[name=\"password\"]', 'YOUR-PASSWORD-HERE');\nawait Promise.all([page.waitForNavigation(), page.click('button[name=\"verifyPassword\"]')]);\n\nconst cookies = await page.cookies();\nawait page.close();\n\n// Email sending logic\nconst promises = emailsToSend.map(({ to, subject, body }) => (async () => {\n    const sendEmailContext = await browser.createIncognitoBrowserContext();\n    const page2 = await sendEmailContext.newPage();\n    await page2.setCookie(...cookies);\n\n    await page2.goto('https://mail.yahoo.com/');\n\n    await page2.click('a[aria-label=\"Compose\"]');\n\n    await page2.type('input#message-to-field', to);\n    await page2.type('input[data-test-id=\"compose-subject\"]', subject);\n    await page2.type('div[data-test-id=\"compose-editor-container\"] div[contenteditable=\"true\"]', body);\n\n    await page2.click('button[title=\"Send this email\"]');\n\n    await sendEmailContext.close();\n})(),\n);\n\nawait Promise.all(promises);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Complete LangChain Integration Example",
    "codeDescription": "Full implementation combining all components for crawling web content, creating a vector index, and performing queries using LangChain and Apify.",
    "codeLanguage": "python",
    "codeTokens": 335,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langchain.md#2025-04-18_snippet_5",
    "pageTitle": "LangChain Integration with Apify Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import os\n\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain_apify import ApifyWrapper\nfrom langchain_core.documents import Document\nfrom langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import ChatOpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\n\nos.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n\napify = ApifyWrapper()\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nprint(\"Call website content crawler ...\")\nloader = apify.call_actor(\n    actor_id=\"apify/website-content-crawler\",\n    run_input={\"startUrls\": [{\"url\": \"https://python.langchain.com/docs/get_started/introduction\"}], \"maxCrawlPages\": 10, \"crawlerType\": \"cheerio\"},\n    dataset_mapping_function=lambda item: Document(page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}),\n)\nprint(\"Compute embeddings...\")\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=InMemoryVectorStore,\n    embedding=OpenAIEmbeddings()\n).from_loaders([loader])\nquery = \"What is LangChain?\"\nresult = index.query_with_sources(query, llm=llm)\n\nprint(\"answer:\", result[\"answer\"])\nprint(\"source:\", result[\"sources\"])"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Complete Web Scraper with Data Export",
    "codeDescription": "Full implementation of a web scraper using PlaywrightCrawler that crawls product data and exports it to CSV. Includes product detail extraction and handling of recommended products.",
    "codeLanguage": "javascript",
    "codeTokens": 366,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/exporting_data.md#2025-04-18_snippet_4",
    "pageTitle": "Exporting Data with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    // We removed the headless: false option to hide the browser windows.\n    requestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {\n        console.log(`Fetching URL: ${request.url}`);\n\n        if (request.label === 'start-url') {\n            await enqueueLinks({\n                selector: 'a.product-item__title',\n            });\n            return;\n        }\n\n        // Fourth, parse the browser's page with Cheerio.\n        const $ = await parseWithCheerio();\n\n        const title = $('h1').text().trim();\n        const vendor = $('a.product-meta__vendor').text().trim();\n        const price = $('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($('span.rating__caption').text(), 10);\n        const description = $('div[class*=\"description\"] div.rte').text().trim();\n        const recommendedProducts = $('.product-recommendations a.product-item__title')\n            .map((i, el) => $(el).text().trim())\n            .toArray();\n\n        await Dataset.pushData({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n            recommendedProducts,\n        });\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n    label: 'start-url',\n}]);\n\nawait crawler.run();\nawait Dataset.exportToCSV('results');"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Implementing Payment Verification with Presence Check in JavaScript",
    "codeDescription": "Demonstrates proper payment verification by checking for the presence of a success indicator element. Uses page.waitForSelector to ensure robust verification.",
    "codeLanguage": "javascript",
    "codeTokens": 87,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/tips_and_tricks_robustness.md#2025-04-18_snippet_0",
    "pageTitle": "Web Scraping and Automation Robustness Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function isPaymentSuccessful() {\n    try {\n        await page.waitForSelector('#PaymentAccepted');\n    } catch (error) {\n        return OUTPUT.paymentFailure;\n    }\n\n    return OUTPUT.paymentSuccess;\n}"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Implementing Proxy Rotation with PuppeteerCrawler in JavaScript",
    "codeDescription": "Example showing how to configure PuppeteerCrawler with proxy rotation and session management to avoid rate limiting. Uses ProxyConfiguration and SessionPool with maxUsageCount setting to rotate proxies after 15 requests.",
    "codeLanguage": "javascript",
    "codeTokens": 216,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/rate_limiting.md#2025-04-18_snippet_0",
    "pageTitle": "Rate Limiting in Web Scraping Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { PuppeteerCrawler } from 'crawlee';\nimport { Actor } from 'apify';\n\nconst myCrawler = new PuppeteerCrawler({\n    proxyConfiguration: await Actor.createProxyConfiguration({\n        groups: ['RESIDENTIAL'],\n    }),\n    sessionPoolOptions: {\n        // Note that a proxy is tied to a session\n        sessionOptions: {\n            // Let's say the website starts blocking requests after\n            // 20 requests have been sent in the span of 1 minute from\n            // a single user.\n            // We can stay on the safe side and retire the browser\n            // and rotate proxies after 15 pages (requests) have been opened.\n            maxUsageCount: 15,\n        },\n    },\n    // ...\n});"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Installing Crawlee via NPM",
    "codeDescription": "Command to install Crawlee package from npm registry into your project",
    "codeLanguage": "shell",
    "codeTokens": 35,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_0",
    "pageTitle": "Professional Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "shell",
        "code": "npm install crawlee"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Complete Page Function Implementation",
    "codeDescription": "Full implementation of a page function that handles pagination and data scraping, including waiting for dynamic content and processing item details.",
    "codeLanguage": "javascript",
    "codeTokens": 404,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_9",
    "pageTitle": "Scraping with Web Scraper Tutorial",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { request,\n        log,\n        skipLinks,\n        jQuery: $,\n        waitFor,\n    } = context;\n\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        let timeoutMillis; // undefined\n        const buttonSelector = 'div.show-more > button';\n        for (;;) {\n            log.info('Waiting for the \"Show more\" button.');\n            try {\n                // Default timeout first time.\n                await waitFor(buttonSelector, { timeoutMillis });\n                // 2 sec timeout after the first.\n                timeoutMillis = 2000;\n            } catch (err) {\n                // Ignore the timeout error.\n                log.info('Could not find the \"Show more button\", '\n                    + 'we\\'ve reached the end.');\n                break;\n            }\n            log.info('Clicking the \"Show more\" button.');\n            $(buttonSelector).click();\n        }\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Implementing Page Function for Apify Scraper in JavaScript",
    "codeDescription": "This code snippet shows a complete implementation of a pageFunction for an Apify scraper. It handles both START and DETAIL page types, extracts URL and unique identifier, and returns the scraped data.",
    "codeLanguage": "javascript",
    "codeTokens": 182,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/getting_started.md#2025-04-18_snippet_8",
    "pageTitle": "Building Apify Scrapers Tutorial Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { request, log, skipLinks } = context;\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        // Do some stuff later.\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n        };\n    }\n}"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Parsing Product Variant Prices with Python",
    "codeDescription": "This function parses the variant name and price from a given variant text. It splits the text into name and price, removes currency symbols and commas, and converts the price to a Decimal object.",
    "codeLanguage": "python",
    "codeTokens": 116,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/11_scraping_variants.md#2025-04-18_snippet_1",
    "pageTitle": "Scraping Product Variants with Python",
    "codeList": [
      {
        "language": "python",
        "code": "def parse_variant(variant):\n    text = variant.text.strip()\n    name, price_text = text.split(\" - \")\n    price = Decimal(\n        price_text\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    return {\"variant_name\": name, \"price\": price}"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Complete Page Function with jQuery Scraping",
    "codeDescription": "Full implementation of a page function that handles multiple page types, injects jQuery, and scrapes data using jQuery selectors within page.evaluate().",
    "codeLanguage": "javascript",
    "codeTokens": 490,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_11",
    "pageTitle": "Scraping with Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    switch (context.request.userData.label) {\n        case 'START': return handleStart(context);\n        case 'DETAIL': return handleDetail(context);\n        default: throw new Error(`Unknown label: ${context.request.userData.label}`);\n    }\n\n    async function handleStart({ log, page }) {\n        log.info('Store opened!');\n        let timeout; // undefined\n        const buttonSelector = 'div.show-more > button';\n        for (;;) {\n            log.info('Waiting for the \"Show more\" button.');\n            try {\n                await page.waitFor(buttonSelector, { timeout });\n                timeout = 2000;\n            } catch (err) {\n                log.info('Could not find the \"Show more button\", '\n                    + 'we\\'ve reached the end.');\n                break;\n            }\n            log.info('Clicking the \"Show more\" button.');\n            await page.click(buttonSelector);\n        }\n    }\n\n    async function handleDetail(contextInner) {\n        const {\n            request,\n            log,\n            skipLinks,\n            page,\n            Apify,\n        } = contextInner;\n\n        // Inject jQuery\n        await Apify.utils.puppeteer.injectJQuery(page);\n\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        // Use jQuery only inside page.evaluate (inside browser)\n        const results = await page.evaluate(() => {\n            return {\n                title: $('header h1').text(),\n                description: $('header span.actor-description').text(),\n                modifiedDate: new Date(\n                    Number(\n                        $('ul.ActorHeader-stats time').attr('datetime'),\n                    ),\n                ).toISOString(),\n                runCount: Number(\n                    $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                        .text()\n                        .match(/[\\d,]+/)[0]\n                        .replace(/,/g, ''),\n                ),\n            };\n        });\n\n        return {\n            url,\n            uniqueIdentifier,\n            // Add results from browser to output\n            ...results,\n        };\n    }\n}"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Retrieving Cookies with Puppeteer",
    "codeDescription": "This code snippet shows how to retrieve cookies from the current page in Puppeteer after logging in.",
    "codeLanguage": "javascript",
    "codeTokens": 60,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_4",
    "pageTitle": "Logging into a Website with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Grab the cookies from the page used to log in\nconst cookies = await page.cookies();"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Complete Example: Integrating Apify with OpenAI Vector Store and Assistant",
    "codeDescription": "A complete end-to-end example combining all previous steps into a single script. This demonstrates the entire workflow from creating an assistant to loading data and querying it.",
    "codeLanguage": "python",
    "codeTokens": 513,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_18",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "from apify_client import ApifyClient\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"YOUR-OPENAI-API-KEY\")\napify_client = ApifyClient(\"YOUR-APIFY-API-TOKEN\")\n\nmy_assistant = client.beta.assistants.create(\n    instructions=\"As a customer support agent at Apify, your role is to assist customers\",\n    name=\"Support assistant\",\n    tools=[{\"type\": \"file_search\"}],\n    model=\"gpt-4o-mini\",\n)\n\n# Create a vector store\nvector_store = client.beta.vector_stores.create(name=\"Support assistant vector store\")\n\n# Update the assistant to use the new Vector Store\nassistant = client.beta.assistants.update(\n    assistant_id=my_assistant.id,\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)\n\nrun_input = {\"startUrls\": [{\"url\": \"https://docs.apify.com/platform\"}], \"maxCrawlPages\": 10, \"crawlerType\": \"cheerio\"}\nactor_call_website_crawler = apify_client.actor(\"apify/website-content-crawler\").call(run_input=run_input)\n\ndataset_id = actor_call_website_crawler[\"defaultDatasetId\"]\n\nrun_input_vs = {\n    \"datasetId\": dataset_id,\n    \"assistantId\": my_assistant.id,\n    \"datasetFields\": [\"text\", \"url\"],\n    \"openaiApiKey\": \"YOUR-OPENAI-API-KEY\",\n    \"vectorStoreId\": vector_store.id,\n}\n\napify_client.actor(\"jiri.spilka/openai-vector-store-integration\").call(run_input=run_input_vs)\n\n# Create a thread and a message\nthread = client.beta.threads.create()\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id, role=\"user\", content=\"How can I scrape a website using Apify?\"\n)\n\n# Run with assistant and poll for the results\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n    tool_choice={\"type\": \"file_search\"}\n)\n\nprint(\"Assistant response:\")\nfor m in client.beta.threads.messages.list(thread_id=run.thread_id):\n    print(m.content[0].text.value)"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Downloading a Text File using request-promise in JavaScript",
    "codeDescription": "This code demonstrates how to download a text file using the request-promise module in JavaScript. It sends a GET request to a specified URL and stores the response in the fileData variable.",
    "codeLanguage": "javascript",
    "codeTokens": 77,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_1",
    "pageTitle": "Submitting a Form with File Attachment in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "const fileData = await request('https://some-site.com/file.txt');"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Running Google Search Scraper with Node.js",
    "codeDescription": "Example of calling Apify task synchronously via API to scrape Google search results using got HTTP client. The code demonstrates how to authenticate, make the API request, and process the response data.",
    "codeLanguage": "javascript",
    "codeTokens": 266,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/run_actor_and_retrieve_data_via_api.md#2025-04-18_snippet_4",
    "pageTitle": "Actor and Task API Integration Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Use your favorite HTTP client\nimport got from 'got';\n\n// Specify your API token\n// (find it at https://console.apify.com/account#/integrations)\nconst myToken = '<YOUR_APIFY_TOKEN>';\n\n// Start apify/google-search-scraper Actor\n// and pass some queries into the JSON body\nconst response = await got({\n    url: `https://api.apify.com/v2/acts/apify~google-search-scraper/run-sync-get-dataset-items?token=${myToken}`,\n    method: 'POST',\n    json: {\n        queries: 'web scraping\\nweb crawling',\n    },\n    responseType: 'json',\n});\n\nconst items = response.body;\n\n// Log each non-promoted search result for both queries\nitems.forEach((item) => {\n    const { nonPromotedSearchResults } = item;\n    nonPromotedSearchResults.forEach((result) => {\n        const { title, url, description } = result;\n        console.log(`${title}: ${url} --- ${description}`);\n    });\n});"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Implementing Scrape Function in TypeScript",
    "codeDescription": "This snippet defines the main scrape function that fetches data, sorts it, and optionally removes images based on user input. It uses function overloads to handle different return types.",
    "codeLanguage": "typescript",
    "codeTokens": 281,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_5",
    "pageTitle": "TypeScript Mini-Project Documentation",
    "codeList": [
      {
        "language": "typescript",
        "code": "// index.ts\n// ...\nimport { SortOrder } from './types';\n\nimport type { ResponseData, Product, UserInput, ModifiedProduct } from './types';\n// ...\n\n// Return a promise of either a \"Product\" array, or a \"ModifiedProduct\" array\nasync function scrape(input: UserInput): Promise<Product[] | ModifiedProduct[]> {\n    // Fetch the data\n    const data = await fetchData();\n\n    // Sort the products based on the input's \"sort\" property. We have\n    // to cast it to \"SortOrder\" because despite being equal, technically\n    // the string \"ascending\" isn't the same type as SortOrder.ASC\n    const sorted = sortData(data.products, input.sort as SortOrder);\n\n    // If the user wants to remove images, map through each product removing\n    // the images and return the result\n    if (input.removeImages) {\n        return sorted.map((item) => {\n            const { images, ...rest } = item;\n\n            return rest;\n        });\n    }\n\n    // Otherwise, just return the sorted products\n    return sorted;\n}"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Passing Data Between Requests in Apify",
    "codeDescription": "Shows how to pass data from one request to another by using the userData object. This example adds a new request for a seller detail page while including the item data from the current page.",
    "codeLanguage": "js",
    "codeTokens": 92,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/request_labels_in_apify_actors.md#2025-04-18_snippet_2",
    "pageTitle": "Request Labels and Data Passing in Apify Actors",
    "codeList": [
      {
        "language": "js",
        "code": "await requestQueue.addRequest({\n    url: sellerDetailUrl,\n    userData: {\n        label: 'SELLERDETAIL',\n        data: itemObject,\n    },\n});"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Recursively Replacing Nested Shadow DOMs with HTML",
    "codeDescription": "This snippet handles websites with nested shadow DOMs by recursively replacing them with their HTML content. It includes helper functions to extract HTML from shadow roots and process elements deeply.",
    "codeLanguage": "javascript",
    "codeTokens": 193,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_shadow_doms.md#2025-04-18_snippet_2",
    "pageTitle": "How to Scrape Sites with a Shadow DOM",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Returns HTML of given shadow DOM.\nconst getShadowDomHtml = (shadowRoot) => {\n    let shadowHTML = '';\n    for (const el of shadowRoot.childNodes) {\n        shadowHTML += el.nodeValue || el.outerHTML;\n    }\n    return shadowHTML;\n};\n\n// Recursively replaces shadow DOMs with their HTML.\nconst replaceShadowDomsWithHtml = (rootElement) => {\n    for (const el of rootElement.querySelectorAll('*')) {\n        if (el.shadowRoot) {\n            replaceShadowDomsWithHtml(shadowRoot);\n            el.innerHTML += getShadowDomHtml(el.shadowRoot);\n        }\n    }\n};\n\nreplaceShadowDomsWithHtml(document.body);"
      }
    ],
    "relevance": 0.972
  },
  {
    "codeTitle": "Main Execution Script for Amazon Scraper",
    "codeDescription": "This script sets up and runs the Amazon scraper. It initializes the CheerioCrawler, adds the initial request based on the input keyword, and starts the crawling process.",
    "codeLanguage": "JavaScript",
    "codeTokens": 220,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/scraping_amazon.md#2025-04-18_snippet_5",
    "pageTitle": "Building an Amazon Web Scraper with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { CheerioCrawler, KeyValueStore, log } from 'crawlee';\nimport { router } from './routes.js';\n\n// Grab our keyword from the input\nconst { keyword = 'iphone' } = (await KeyValueStore.getInput()) ?? {};\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n});\n\n// Add our initial requests\nawait crawler.addRequests([\n    {\n        // Turn the inputted keyword into a link we can make a request with\n        url: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,\n        label: 'START',\n        userData: {\n            keyword,\n        },\n    },\n]);\n\nlog.info('Starting the crawl.');\nawait crawler.run();\nlog.info('Crawl finished.');"
      }
    ],
    "relevance": 0.972
  },
  {
    "codeTitle": "Complete Orchestrator Actor Implementation",
    "codeDescription": "Full TypeScript implementation of the Orchestrator Actor, including input validation, state management, parallel run orchestration, and waiting for all runs to complete.",
    "codeLanguage": "typescript",
    "codeTokens": 59,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_5",
    "pageTitle": "Parallel Website Scraping with Multiple Actor Runs",
    "codeList": [
      {
        "language": "typescript",
        "code": "// OrchestratorActorMainTs reference"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Implementing Incremental Crawling with Apify SDK and Crawlee",
    "codeDescription": "Demonstrates an example of incremental crawling using Apify SDK and Crawlee. This script crawls Apify Docs, saving new page titles to a dataset and utilizing a persistent request queue for incremental scraping across multiple runs.",
    "codeLanguage": "typescript",
    "codeTokens": 363,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_8",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "typescript",
        "code": "// Basic example of incremental crawling with Crawlee.\nimport { Actor } from 'apify';\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\ninterface Input {\n    startUrls: string[];\n    persistRquestQueueName: string;\n}\n\nawait Actor.init();\n\n// Structure of input is defined in input_schema.json\nconst {\n    startUrls = ['https://docs.apify.com/'],\n    persistRequestQueueName = 'persist-request-queue',\n} = (await Actor.getInput<Input>()) ?? ({} as Input);\n\n// Open or create request queue for incremental scrape.\n// By opening same request queue, the crawler will continue where it left off and skips already visited URLs.\nconst requestQueue = await Actor.openRequestQueue(persistRequestQueueName);\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    requestQueue, // Pass incremental request queue to the crawler.\n    requestHandler: async ({ enqueueLinks, request, $, log }) => {\n        log.info('enqueueing new URLs');\n        await enqueueLinks();\n\n        // Extract title from the page.\n        const title = $('title').text();\n        log.info(`New page with ${title}`, { url: request.loadedUrl });\n\n        // Save the URL and title of the loaded page to the output dataset.\n        await Dataset.pushData({ url: request.loadedUrl, title });\n    },\n});\n\nawait crawler.run(startUrls);\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Complete Web Scraper Implementation with Node.js and Cheerio",
    "codeDescription": "This is the full implementation of a web scraper using Node.js, Got Scraping for HTTP requests, and Cheerio for HTML parsing. It extracts product titles and prices from a Shopify store.",
    "codeLanguage": "JavaScript",
    "codeTokens": 239,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/node_continued.md#2025-04-18_snippet_2",
    "pageTitle": "Extracting Data with Node.js and Cheerio",
    "codeList": [
      {
        "language": "javascript",
        "code": "// main.js\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\n// Download HTML with Got Scraping\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// Parse HTML with Cheerio\nconst $ = cheerio.load(html);\n\n// Find all products on the page\nconst products = $('.product-item');\n\nconst results = [];\nfor (const product of products) {\n    const titleElement = $(product).find('a.product-item__title');\n    const title = titleElement.text().trim();\n\n    const priceElement = $(product).find('span.price');\n    const price = priceElement.contents()[2].nodeValue.trim();\n\n    results.push({ title, price });\n}\n\nconsole.log(results);"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Extracting Actor Details from Apify Store Page in JavaScript",
    "codeDescription": "This function scrapes various details about an actor from its page on the Apify Store, including title, description, modified date, and run count. It uses Puppeteer's page.$eval method for extracting data from specific elements.",
    "codeLanguage": "javascript",
    "codeTokens": 293,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_3",
    "pageTitle": "Scraping with Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { page, request } = context;\n    const { url } = request;\n\n    // ...\n\n    const uniqueIdentifier = url\n        .split('/')\n        .slice(-2)\n        .join('/');\n\n    const title = await page.$eval(\n        'header h1',\n        ((el) => el.textContent),\n    );\n    const description = await page.$eval(\n        'header span.actor-description',\n        ((el) => el.textContent),\n    );\n\n    const modifiedTimestamp = await page.$eval(\n        'ul.ActorHeader-stats time',\n        (el) => el.getAttribute('datetime'),\n    );\n    const modifiedDate = new Date(Number(modifiedTimestamp));\n\n    const runCountText = await page.$eval(\n        'ul.ActorHeader-stats > li:nth-of-type(3)',\n        ((el) => el.textContent),\n    );\n    const runCount = Number(runCountText.match(/[\\d,]+/)[0].replace(',', ''));\n\n    return {\n        url,\n        uniqueIdentifier,\n        title,\n        description,\n        modifiedDate,\n        runCount,\n    };\n}"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Sending Multiple Emails Concurrently with Puppeteer",
    "codeDescription": "This code snippet shows how to send multiple emails concurrently using Puppeteer. It creates an array of promises, each handling the email sending process in a separate incognito browser context with stored cookies.",
    "codeLanguage": "JavaScript",
    "codeTokens": 328,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_8",
    "pageTitle": "Logging into a Website with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Create an array of promises, running the cookie passing\n// and email sending logic each time\nconst promises = emailsToSend.map(({ to, subject, body }) => (async () => {\n    // Create a fresh non-persistent browser context\n    const sendEmailContext = await browser.createIncognitoBrowserContext();\n    // Create a new page on the new browser context and set its cookies\n    // to be the same ones from the page we used to log into the website.\n    const page2 = await sendEmailContext.newPage();\n    await page2.setCookie(...cookies);\n\n    await page2.goto('https://mail.yahoo.com/');\n\n    // Compose an email\n    await page2.click('a[aria-label=\"Compose\"]');\n\n    // Populate the fields with the details from the object\n    await page2.type('input#message-to-field', to);\n    await page2.type('input[data-test-id=\"compose-subject\"]', subject);\n    await page2.type('div[data-test-id=\"compose-editor-container\"] div[contenteditable=\"true\"]', body);\n\n    // Send the email\n    await page2.click('button[title=\"Send this email\"]');\n\n    await sendEmailContext.close();\n})(),\n);\n\n// Wait for all emails to be sent\nawait Promise.all(promises);"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Product Scraping with Python BeautifulSoup",
    "codeDescription": "Main script that downloads HTML content from a webpage, parses product information, and exports data to CSV and JSON files. Uses httpx for HTTP requests and BeautifulSoup for HTML parsing.",
    "codeLanguage": "python",
    "codeTokens": 345,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_0",
    "pageTitle": "Python Web Scraping with BeautifulSoup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\nimport csv\nimport json\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\ndata = []\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    data.append({\"title\": title, \"min_price\": min_price, \"price\": price})\n\nwith open(\"products.csv\", \"w\") as file:\n    writer = csv.DictWriter(file, fieldnames=[\"title\", \"min_price\", \"price\"])\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\ndef serialize(obj):\n    if isinstance(obj, Decimal):\n        return str(obj)\n    raise TypeError(\"Object not JSON serializable\")\n\nwith open(\"products.json\", \"w\") as file:\n    json.dump(data, file, default=serialize)"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Dataset Operations in JavaScript",
    "codeDescription": "Shows how to push data to an Apify dataset in JavaScript.",
    "codeLanguage": "javascript",
    "codeTokens": 72,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_6",
    "pageTitle": "Basic Apify SDK Commands Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n\n// Append result object to the default dataset associated with the run\nawait Actor.pushData({ someResult: 123 });\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Passing Cookies to New Context in Playwright",
    "codeDescription": "This code snippet demonstrates how to create a new browser context in Playwright, add previously retrieved cookies, and navigate to a page while maintaining logged-in status.",
    "codeLanguage": "javascript",
    "codeTokens": 169,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_5",
    "pageTitle": "Logging into a Website with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Create a fresh non-persistent browser context\nconst sendEmailContext = await browser.newContext();\n// Add the cookies from the previous one to this one so that\n// we'll be logged into Yahoo without having to re-do the\n// logging in automation\nawait sendEmailContext.addCookies(cookies);\nconst page2 = await sendEmailContext.newPage();\n\n// Notice that we are logged in, even though we didn't\n// go through the logging in process again!\nawait page2.goto('https://mail.yahoo.com/');\nawait page2.waitForTimeout(10000);"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Extracting Google Search Results in Web Scraper",
    "codeDescription": "This code extracts search results from a Google search results page. It parses the DOM to retrieve the name, link, and text for each search result and returns them as an array of objects.",
    "codeLanguage": "javascript",
    "codeTokens": 237,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/processing_multiple_pages_web_scraper.md#2025-04-18_snippet_2",
    "pageTitle": "Processing the Same URL Multiple Times in Web Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const $ = context.jQuery;\n\n    if (context.request.userData.label === 'enqueue') {\n        // copy from the previous part\n    } else if (context.request.userData.label === 'result') {\n        // create result array\n        const result = [];\n\n        // process all the results\n        $('.rc').each((index, elem) => {\n\n            // wrap element in jQuery\n            const gResult = $(elem);\n\n            // lookup link and text\n            const link = gResult.find('.r a');\n            const text = gResult.find('.s .st');\n\n            // extract data and add it to result array\n            result.push({\n                name: link.text(),\n                link: link.attr('href'),\n                text: text.text(),\n            });\n        });\n        // Now we finally return\n\n        return result;\n    }\n}"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Creating a Request Queue with Labeled Request in Apify",
    "codeDescription": "Shows how to create a request queue and add a request with a 'START' label in the userData attribute. This allows for identification and specific handling of different request types later in the crawler.",
    "codeLanguage": "js",
    "codeTokens": 113,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/request_labels_in_apify_actors.md#2025-04-18_snippet_0",
    "pageTitle": "Request Labels and Data Passing in Apify Actors",
    "codeList": [
      {
        "language": "js",
        "code": "// Create a request list.\nconst requestQueue = await Apify.openRequestQueue();\n// Add the request to the queue\nawait requestQueue.addRequest({\n    url: 'https://www.example.com/',\n    userData: {\n        label: 'START',\n    },\n});"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Complete Refactored Scraping Script",
    "codeDescription": "Full refactored version of the scraping script with modular functions for downloading, parsing, and exporting data.",
    "codeLanguage": "python",
    "codeTokens": 402,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_5",
    "pageTitle": "Python Web Scraping with BeautifulSoup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\nimport csv\nimport json\n\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n\n    html_code = response.text\n    return BeautifulSoup(html_code, \"html.parser\")\n\ndef parse_product(product):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price}\n\ndef export_csv(file, data):\n    fieldnames = list(data[0].keys())\n    writer = csv.DictWriter(file, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\ndef export_json(file, data):\n    def serialize(obj):\n        if isinstance(obj, Decimal):\n            return str(obj)\n        raise TypeError(\"Object not JSON serializable\")\n\n    json.dump(data, file, default=serialize, indent=2)\n\nlisting_url = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nlisting_soup = download(listing_url)\n\ndata = []\nfor product in listing_soup.select(\".product-item\"):\n    item = parse_product(product)\n    data.append(item)\n\nwith open(\"products.csv\", \"w\") as file:\n    export_csv(file, data)\n\nwith open(\"products.json\", \"w\") as file:\n    export_json(file, data)"
      }
    ],
    "relevance": 0.968
  },
  {
    "codeTitle": "Starting a Simple Express.js Web Server in an Apify Actor",
    "codeDescription": "This code snippet demonstrates how to create a basic web server using Express.js within an Apify Actor. It sets up a single route that responds with 'Hello world' and runs the server on the port specified by the ACTOR_WEB_SERVER_PORT environment variable.",
    "codeLanguage": "JavaScript",
    "codeTokens": 200,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/container_web_server.md#2025-04-18_snippet_0",
    "pageTitle": "Running a Container Web Server in Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "// npm install express\nimport { Actor } from 'apify';\nimport express from 'express';\n\nawait Actor.init();\n\nconst app = express();\nconst port = process.env.ACTOR_WEB_SERVER_PORT;\n\napp.get('/', (req, res) => {\n    res.send('Hello world from Express app!');\n});\n\napp.listen(port, () => console.log(`Web server is listening\n    and can be accessed at\n    ${process.env.ACTOR_WEB_SERVER_URL}!`));\n\n// Let the Actor run for an hour\nawait new Promise((r) => setTimeout(r, 60 * 60 * 1000));\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Configuring Custom Headers for API Requests",
    "codeDescription": "Shows how to define custom headers for API requests, including common headers like User-Agent and Referer that are typically required for authentication.",
    "codeLanguage": "javascript",
    "codeTokens": 142,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/cookies_headers_tokens.md#2025-04-18_snippet_1",
    "pageTitle": "API Authentication Guide with Cookies, Headers, and Tokens",
    "codeList": [
      {
        "language": "javascript",
        "code": "const HEADERS = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko)'\n        + 'Chrome/96.0.4664.110 YaBrowser/22.1.0.2500 Yowser/2.5 Safari/537.36',\n    Referer: 'https://soundcloud.com',\n    // ...\n};"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Parsing HTML with Cheerio in Node.js",
    "codeDescription": "This snippet shows how to use Got-scraping to download HTML and then parse it with Cheerio. It demonstrates extracting the text content of the h1 element from the parsed HTML.",
    "codeLanguage": "javascript",
    "codeTokens": 171,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/node_js_scraper.md#2025-04-18_snippet_1",
    "pageTitle": "Scraping with Node.js",
    "codeList": [
      {
        "language": "javascript",
        "code": "// main.js\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\n// Download HTML with Got Scraping\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// Parse HTML with Cheerio\nconst $ = cheerio.load(html);\nconst headingElement = $('h1');\nconst headingText = headingElement.text();\n\n// Print page title to terminal\nconsole.log(headingText);"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Optimized Scraping Function for Apify Store Actor Details in JavaScript",
    "codeDescription": "This function demonstrates an optimized approach to scraping actor details from the Apify Store. It uses parallel promises to fetch multiple pieces of information simultaneously, improving performance. It also includes logic for handling different page types.",
    "codeLanguage": "javascript",
    "codeTokens": 418,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_4",
    "pageTitle": "Scraping with Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    // page is Puppeteer's page\n    const { request, log, skipLinks, page } = context;\n\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        // Do some stuff later.\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        // Get attributes in parallel to speed up the process.\n        const titleP = page.$eval(\n            'header h1',\n            (el) => el.textContent,\n        );\n        const descriptionP = page.$eval(\n            'header span.actor-description',\n            (el) => el.textContent,\n        );\n        const modifiedTimestampP = page.$eval(\n            'ul.ActorHeader-stats time',\n            (el) => el.getAttribute('datetime'),\n        );\n        const runCountTextP = page.$eval(\n            'ul.ActorHeader-stats > li:nth-of-type(3)',\n            (el) => el.textContent,\n        );\n\n        const [\n            title,\n            description,\n            modifiedTimestamp,\n            runCountText,\n        ] = await Promise.all([\n            titleP,\n            descriptionP,\n            modifiedTimestampP,\n            runCountTextP,\n        ]);\n\n        const modifiedDate = new Date(Number(modifiedTimestamp));\n        const runCount = Number(runCountText.match(/[\\d,]+/)[0].replace(',', ''));\n\n        return {\n            url,\n            uniqueIdentifier,\n            title,\n            description,\n            modifiedDate,\n            runCount,\n        };\n    }\n}"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "JavaScript Actor Integration Example",
    "codeDescription": "Complete example showing how to run an Actor and retrieve results using the JavaScript Apify client library.",
    "codeLanguage": "JavaScript",
    "codeTokens": 168,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/run_actor_and_retrieve_data_via_api.md#2025-04-18_snippet_2",
    "pageTitle": "Actor and Task API Integration Guide",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { ApifyClient } from 'apify-client';\n\nconst client = new ApifyClient({ token: 'YOUR_API_TOKEN' });\n\nconst input = { queries: 'Food in NYC' };\n\n// Run the Actor and wait for it to finish\n// .call method waits infinitely long using smart polling\n// Get back the run API object\nconst run = await client.actor('apify/google-search-scraper').call(input);\n\n// Fetch and print Actor results from the run's dataset (if any)\nconst { items } = await client.dataset(run.defaultDatasetId).listItems();\nitems.forEach((item) => {\n    console.dir(item);\n});"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Executing Browser-Side Code with Playwright",
    "codeDescription": "This snippet shows how to correctly execute browser-side code using page.evaluate() in Playwright, changing the background color of a web page.",
    "codeLanguage": "JavaScript",
    "codeTokens": 116,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/index.md#2025-04-18_snippet_1",
    "pageTitle": "Executing Scripts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\nawait page.evaluate(() => {\n    document.body.style.background = 'green';\n});\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Complete Actor Data Extraction with URL and Identifier",
    "codeDescription": "A complete implementation that extracts all required data points from an Actor page, including URL, unique identifier, title, description, modified date, and run count, using jQuery and JavaScript string manipulation.",
    "codeLanguage": "javascript",
    "codeTokens": 214,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_4",
    "pageTitle": "Scraping with Web Scraper Tutorial",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { request, jQuery: $ } = context;\n    const { url } = request;\n\n    // ... rest of the code\n\n    const uniqueIdentifier = url.split('/').slice(-2).join('/');\n\n    return {\n        url,\n        uniqueIdentifier,\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n        runCount: Number(\n            $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                .text()\n                .match(/[\\d,]+/)[0]\n                .replace(/,/g, ''),\n        ),\n    };\n}"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Complete Lazy Loading Implementation with Puppeteer",
    "codeDescription": "Complete code for scraping a lazy-loaded e-commerce website using Puppeteer. Similar to the Playwright version, it scrolls through the page to load products and extracts their details, but uses Puppeteer-specific API methods for browser automation.",
    "codeLanguage": "javascript",
    "codeTokens": 464,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_11",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\nimport * as cheerio from 'cheerio';\n\nconst products = [];\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.aboutyou.com/c/women/clothing-20204');\n\n// Grab the height of result item in pixels, which will be used to scroll down\nconst itemHeight = await page.$eval('a[data-testid*=\"productTile\"]', (elem) => elem.clientHeight);\n\n// Keep track of how many pixels have been scrolled down\nlet totalScrolled = 0;\n\nwhile (products.length < 75) {\n    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);\n\n    await page.mouse.wheel({ deltaY: itemHeight * 3 });\n    totalScrolled += itemHeight * 3;\n    // Allow the products 1 second to load\n    await page.waitForTimeout(1000);\n\n    const $ = cheerio.load(await page.content());\n\n    // Grab the newly loaded items\n    const items = [...$('a[data-testid*=\"productTile\"]')].slice(products.length);\n\n    const newItems = items.map((item) => {\n        const elem = $(item);\n\n        return {\n            brand: elem.find('p[data-testid=\"brandName\"]').text().trim(),\n            price: elem.find('span[data-testid=\"finalPrice\"]').text().trim(),\n        };\n    });\n\n    products.push(...newItems);\n\n    const innerHeight = await page.evaluate(() => window.innerHeight);\n\n    // if the total pixels scrolled is equal to the true available scroll\n    // height of the page, we've reached the end and should stop scraping.\n    // even if we haven't reach our goal of 75 products.\n    if (totalScrolled >= scrollHeight - innerHeight) {\n        break;\n    }\n}\n\nconsole.log(products.slice(0, 75));\n\nawait browser.close();"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Implementing Router Handlers for Amazon Scraper",
    "codeDescription": "This comprehensive snippet shows the implementation of all router handlers for the Amazon scraper. It includes handlers for the start page, product pages, and offer pages.",
    "codeLanguage": "JavaScript",
    "codeTokens": 453,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/scraping_amazon.md#2025-04-18_snippet_4",
    "pageTitle": "Building an Amazon Web Scraper with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { createCheerioRouter, Dataset } from 'crawlee';\nimport { BASE_URL, labels } from './constants';\n\nexport const router = createCheerioRouter();\n\nrouter.addHandler(labels.START, async ({ $, crawler, request }) => {\n    const { keyword } = request.userData;\n\n    const products = $('div > div[data-asin]:not([data-asin=\"\"])');\n\n    for (const product of products) {\n        const element = $(product);\n        const titleElement = $(element.find('.a-text-normal[href]'));\n\n        const url = `${BASE_URL}${titleElement.attr('href')}`;\n\n        await crawler.addRequests([\n            {\n                url,\n                label: labels.PRODUCT,\n                userData: {\n                    data: {\n                        title: titleElement.first().text().trim(),\n                        asin: element.attr('data-asin'),\n                        itemUrl: url,\n                        keyword,\n                    },\n                },\n            },\n        ]);\n    }\n});\n\nrouter.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {\n    const { data } = request.userData;\n\n    const element = $('div#productDescription');\n\n    await crawler.addRequests([\n        {\n            url: `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${data.asin}&pc=dp`,\n            label: labels.OFFERS,\n            userData: {\n                data: {\n                    ...data,\n                    description: element.text().trim(),\n                },\n            },\n        },\n    ]);\n});\n\nrouter.addHandler(labels.OFFERS, async ({ $, request }) => {\n    const { data } = request.userData;\n\n    for (const offer of $('#aod-offer')) {\n        const element = $(offer);\n\n        await Dataset.pushData({\n            ...data,\n            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\n            offer: element.find('.a-price .a-offscreen').text().trim(),\n        });\n    }\n});"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Using CheerioCrawler with Proxy Configuration and Session Management",
    "codeDescription": "Shows how to configure CheerioCrawler with proxy configuration and session pooling. The crawler makes HTTP requests to API endpoints and processes JSON responses while maintaining the same session across requests.",
    "codeLanguage": "javascript",
    "codeTokens": 169,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_5",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport { CheerioCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    sessionPoolOptions: { maxPoolSize: 1 },\n    async requestHandler({ json }) {\n        // ...\n        console.log(json);\n    },\n});\n\nawait crawler.run([\n    'https://api.apify.com/v2/browser-info',\n    'https://proxy.apify.com/?format=json',\n]);\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Logging into Yahoo with Playwright",
    "codeDescription": "This code snippet demonstrates how to log into Yahoo using Playwright. It navigates through the login process, including accepting cookies, entering credentials, and verifying successful login.",
    "codeLanguage": "javascript",
    "codeTokens": 274,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_0",
    "pageTitle": "Logging into a Website with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\n// Launch a browser and open a page\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.yahoo.com/');\n\n// Agree to the cookies terms, then click on the \"Sign in\" button\nawait page.click('button[name=\"agree\"]');\nawait page.waitForSelector('a:has-text(\"Sign in\")');\n\nawait page.click('a:has-text(\"Sign in\")');\nawait page.waitForLoadState('load');\n\n// Type in the username and continue forward\nawait page.type('input[name=\"username\"]', 'YOUR-LOGIN-HERE');\nawait page.click('input[name=\"signin\"]');\n\n// Type in the password and continue forward\nawait page.type('input[name=\"password\"]', 'YOUR-PASSWORD-HERE');\nawait page.click('button[name=\"verifyPassword\"]');\nawait page.waitForLoadState('load');\n\n// Wait for 10 seconds so we can see that we have in fact\n// successfully logged in\nawait page.waitForTimeout(10000);"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Scraping Detailed Product Information from Shopify Store in JavaScript",
    "codeDescription": "This code iterates through product URLs, fetches each product page, and extracts detailed information such as title, vendor, price, review count, and description using Cheerio.",
    "codeLanguage": "JavaScript",
    "codeTokens": 237,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_6",
    "pageTitle": "Web Scraping Tutorial: Crawling and Data Extraction",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "const results = [];\nconst errors = [];\n\nfor (const url of productUrls) {\n    try {\n        const productResponse = await gotScraping(url);\n        const $productPage = cheerio.load(productResponse.body);\n\n        const title = $productPage('h1').text().trim();\n        const vendor = $productPage('a.product-meta__vendor').text().trim();\n        const price = $productPage('span.price').contents()[2].nodeValue;\n        const reviewCount = parseInt($productPage('span.rating__caption').text(), 10);\n        const description = $productPage('div[class*=\"description\"] div.rte').text().trim();\n\n        results.push({\n            title,\n            vendor,\n            price,\n            reviewCount,\n            description,\n        });\n    } catch (error) {\n        errors.push({ url, msg: error.message });\n    }\n}"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Submitting a Form with Puppeteer in JavaScript",
    "codeDescription": "This code demonstrates how to submit a form using Puppeteer. It simulates a click on the submit button to send the form data, including the uploaded file.",
    "codeLanguage": "javascript",
    "codeTokens": 67,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_7",
    "pageTitle": "Submitting a Form with File Attachment in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "await page.click('input[type=submit]');"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Scraping Dynamic Tokens with Puppeteer",
    "codeDescription": "Shows how to use Puppeteer to dynamically scrape authentication tokens by intercepting and analyzing page responses. This example specifically targets the client_id parameter from network requests.",
    "codeLanguage": "javascript",
    "codeTokens": 276,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/cookies_headers_tokens.md#2025-04-18_snippet_3",
    "pageTitle": "API Authentication Guide with Cookies, Headers, and Tokens",
    "codeList": [
      {
        "language": "javascript",
        "code": "// import the puppeteer module\nimport puppeteer from 'puppeteer';\n\nconst scrapeClientId = async () => {\n    const browser = await puppeteer.launch({ headless: false });\n    const page = await browser.newPage();\n\n    // initialize a variable that will eventually hold the client_id\n    let clientId = null;\n\n    // handle each response\n    page.on('response', async (res) => {\n        // try to grab the 'client_id' parameter from each URL\n        const id = new URL(res.url()).searchParams.get('client_id') ?? null;\n\n        // if the parameter exists, set our clientId variable to the newly parsed value\n        if (id) clientId = id;\n    });\n\n    // visit the page\n    await page.goto('https://soundcloud.com/tiesto/tracks');\n\n    // wait for a selector that ensures the page has time to load and make requests to its API\n    await page.waitForSelector('.profileHeader__link');\n\n    await browser.close();\n    console.log(clientId); // log the retrieved client_id\n};\n\nscrapeClientId();"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "URL Parameter Extraction from Requests",
    "codeDescription": "Complete implementation showing how to extract and parse URL parameters from network requests. Includes browser setup and request handling.",
    "codeLanguage": "javascript",
    "codeTokens": 186,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_2",
    "pageTitle": "Reading and Intercepting Network Requests with Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Listen for all requests\npage.on('request', (req) => {\n    // If the URL doesn't include our keyword, ignore it\n    if (!req.url().includes('followings')) return;\n\n    // Convert the request URL into a URL object\n    const url = new URL(req.url());\n\n    // Print the search parameters in object form\n    console.log(Object.fromEntries(url.searchParams));\n});\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Checking Node.js and npm versions in Shell",
    "codeDescription": "These commands are used to verify the installation of Node.js and npm by printing their versions. The lesson requires Node.js version 16 or higher.",
    "codeLanguage": "Shell",
    "codeTokens": 54,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/computer_preparation.md#2025-04-18_snippet_0",
    "pageTitle": "Computer Preparation for Web Scraping",
    "codeList": [
      {
        "language": "Shell",
        "code": "node -v\nnpm -v"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Node.js Context Parsing with Cheerio - Playwright Version",
    "codeDescription": "Complete example showing how to use Cheerio to parse page content in the Node.js context with Playwright. Extracts product information using Cheerio's jQuery-like syntax.",
    "codeLanguage": "javascript",
    "codeTokens": 206,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_3",
    "pageTitle": "Data Extraction Guide for Playwright and Puppeteer",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\nimport { load } from 'cheerio';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://demo-webstore.apify.org/search/on-sale');\n\nconst $ = load(await page.content());\n\nconst productCards = Array.from($('a[class*=\"ProductCard_root\"]'));\n\nconst products = productCards.map((element) => {\n    const card = $(element);\n\n    const name = card.find('h3[class*=\"ProductCard_name\"]').text();\n    const price = card.find('div[class*=\"ProductCard_price\"]').text();\n\n    return {\n        name,\n        price,\n    };\n});\n\nconsole.log(products);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.963
  },
  {
    "codeTitle": "Initializing Web Scraping Environment in JavaScript",
    "codeDescription": "Sets up the basic environment for web scraping using got-scraping for HTTP requests and cheerio for HTML parsing. It fetches the HTML content of a product detail page.",
    "codeLanguage": "JavaScript",
    "codeTokens": 143,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_0",
    "pageTitle": "Web Scraping Tutorial: Crawling and Data Extraction",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst productUrl = 'https://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones';\nconst response = await gotScraping(productUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\n// Attribute extraction code will go here."
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Configuring Fingerprint Generation in Crawlee (JavaScript)",
    "codeDescription": "This snippet demonstrates how to configure fingerprint generation options in Crawlee using the PlaywrightCrawler. It specifies browser, device, and operating system parameters for fingerprint generation.",
    "codeLanguage": "javascript",
    "codeTokens": 126,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/generating_fingerprints.md#2025-04-18_snippet_0",
    "pageTitle": "Generating Fingerprints for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    browserPoolOptions: {\n        fingerprintOptions: {\n            fingerprintGeneratorOptions: {\n                browsers: [{ name: 'firefox', minVersion: 80 }],\n                devices: ['desktop'],\n                operatingSystems: ['windows'],\n            },\n        },\n    },\n});"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Identifying and Retrying Failed Requests with Apify",
    "codeDescription": "This TypeScript snippet demonstrates how to identify failed requests from a finished scraper run by examining error messages and retry counts. It lists all requests from a queue, filters those that failed, resets their status, and prepares them for reprocessing without having to rerun the entire scraper.",
    "codeLanguage": "typescript",
    "codeTokens": 439,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/api/retry_failed_requests.md#2025-04-18_snippet_0",
    "pageTitle": "Retrying Failed Requests in Web Scraping",
    "codeList": [
      {
        "language": "typescript",
        "code": "// The code is similar for both Crawlee-only but uses a different API\nimport { Actor } from 'apify';\n\nconst REQUEST_QUEUE_ID = 'pFCvCasdvsyvyZdfD'; // Replace with your valid request queue ID\nconst allRequests = [];\nlet exclusiveStartId = null;\n// List all requests from the queue, we have to do it in a loop because the request queue list is paginated\nfor (; ;) {\n    const { items: requests } = await Actor.apifyClient\n        .requestQueue(REQUEST_QUEUE_ID)\n        .listRequests({ exclusiveStartId, limit: 1000 });\n    allRequests.push(...requests);\n    // If we didn't get the full 1,000 requests, we have all and can finish the loop\n    if (requests.length < 1000) {\n        break;\n    }\n\n    // Otherwise, we need to set the exclusiveStartId to the last request id to get the next batch\n    exclusiveStartId = requests[requests.length - 1].id;\n}\n\nconsole.log(`Loaded ${allRequests.length} requests from the queue`);\n\n// Now we filter the failed requests\nconst failedRequests = allRequests.filter((request) => (request.errorMessages?.length || 0) > (request.retryCount || 0));\n\n// We need to update them 1 by 1 to the pristine state\nfor (const request of failedRequests) {\n    request.retryCount = 0;\n    request.errorMessages = [];\n    // This tells the request queue to handle it again\n    request.handledAt = null;\n    await Actor.apifyClient.requestQueue(REQUEST_QUEUE_ID).updateRequest(request);\n}\n\n// And now we can resurrect our scraper again; it will only process the failed requests."
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Basic HTML Text Extraction with BeautifulSoup",
    "codeDescription": "Demonstrates how to fetch a webpage and extract text content from product items using BeautifulSoup's select method.",
    "codeLanguage": "python",
    "codeTokens": 109,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/06_locating_elements.md#2025-04-18_snippet_0",
    "pageTitle": "Locating HTML Elements with Python using BeautifulSoup",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    print(product.text)"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Extracting Actor Details with Cheerio in JavaScript",
    "codeDescription": "This function uses Cheerio to extract various details about an actor from its detail page, including the title, description, modified date, and run count.",
    "codeLanguage": "javascript",
    "codeTokens": 210,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_4",
    "pageTitle": "Cheerio Scraper Tutorial Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { $ } = context;\n    const { url } = request;\n    // ... rest of your code can come here\n\n    const uniqueIdentifier = url\n        .split('/')\n        .slice(-2)\n        .join('/');\n\n    return {\n        url,\n        uniqueIdentifier,\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n        runCount: Number(\n            $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                .text()\n                .match(/[\\d,]+/)[0]\n                .replace(/,/g, ''),\n        ),\n    };\n}"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Handling Asynchronous Operations in Crawlee's PlaywrightCrawler",
    "codeDescription": "This code snippet demonstrates how to properly handle asynchronous operations in Crawlee's PlaywrightCrawler to prevent 'Target closed' errors. It shows how to use preNavigationHooks and requestHandler to ensure all promises are awaited before the page is closed.",
    "codeLanguage": "javascript",
    "codeTokens": 291,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/how_to_fix_target_closed.md#2025-04-18_snippet_0",
    "pageTitle": "Fixing 'Target closed' Error in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "const crawler = new PlaywrightCrawler({\n    // ...other options\n    preNavigationHooks: [\n        async ({ page, context }) => {\n            // Some action that takes time, we don't await here\n            // Try/catch all non awaited code because it can cause unhandled rejection which crashes the whole process\n            const responsePromise = page.waitForResponse('https://example.com/resource').catch((e) => e);\n            // Attach the promise to the context which is accessible to requestHandler\n            context.responsePromise = responsePromise;\n        },\n    ],\n    requestHandler: async ({ request, page, context }) => {\n        // We first wait for the response before doing anything else\n        const response = await context.responsePromise;\n        // Check if it errored out, otherwise proceed with parsing it\n        if (typeof response === 'string' || response instanceof Error) {\n            throw new Error(`Failed to load resource from response`, { cause: response });\n        }\n        // Now process the response and continue with the code synchronously\n    },\n});"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Complete Web Scraping Script with Puppeteer",
    "codeDescription": "This is a full script using Puppeteer to navigate to Google, perform a search, click on a result, extract the page title, and take a screenshot. It showcases the implementation of various Page methods in a practical web scraping scenario.",
    "codeLanguage": "javascript",
    "codeTokens": 251,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/page_methods.md#2025-04-18_snippet_3",
    "pageTitle": "Page Methods in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\n// Create a page and visit Google\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\n// Agree to the cookies policy\nawait page.click('button + button');\n\n// Type the query and visit the results page\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Wait for the first result to appear on the page,\n// then click on it\nawait page.waitForSelector('.g a');\nawait Promise.all([page.waitForNavigation(), page.click('.g a')]);\n\n// Grab the page's title and log it to the console\nconst title = await page.title();\nconsole.log(title);\n\n// Take a screenshot and write it to the filesystem\nawait page.screenshot({ path: 'screenshot.png' });\n\nawait browser.close();"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Using CheerioCrawler with Apify Proxy in JavaScript",
    "codeDescription": "This example shows how to use CheerioCrawler with Apify Proxy. It sets up the Actor, creates a proxy configuration, and initializes a crawler to fetch and log the body of a specified URL.",
    "codeLanguage": "javascript",
    "codeTokens": 143,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_1",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport { CheerioCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new CheerioCrawler({\n    proxyConfiguration,\n    async requestHandler({ body }) {\n        // ...\n        console.log(body);\n    },\n});\n\nawait crawler.run(['https://proxy.apify.com']);\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Organizing Web Scraping Logic with Function Decomposition in JavaScript",
    "codeDescription": "Demonstrates how to structure a pageFunction by breaking it into separate handler functions for different page types. Includes implementation of pagination handling and detail page scraping with proper error handling and data extraction.",
    "codeLanguage": "javascript",
    "codeTokens": 448,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_10",
    "pageTitle": "Scraping with Web Scraper Tutorial",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    switch (context.request.userData.label) {\n        case 'START': return handleStart(context);\n        case 'DETAIL': return handleDetail(context);\n        default: throw new Error('Unknown request label.');\n    }\n\n    async function handleStart({ log, waitFor }) {\n        log.info('Store opened!');\n        let timeoutMillis; // undefined\n        const buttonSelector = 'div.show-more > button';\n        for (;;) {\n            log.info('Waiting for the \"Show more\" button.');\n            try {\n                // Default timeout first time.\n                await waitFor(buttonSelector, { timeoutMillis });\n                // 2 sec timeout after the first.\n                timeoutMillis = 2000;\n            } catch (err) {\n                // Ignore the timeout error.\n                log.info('Could not find the \"Show more button\", '\n                    + 'we\\'ve reached the end.');\n                break;\n            }\n            log.info('Clicking the \"Show more\" button.');\n            $(buttonSelector).click();\n        }\n    }\n\n    async function handleDetail({\n        request,\n        log,\n        skipLinks,\n        jQuery: $,\n    }) {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Extracting Product Variants with Python and Beautiful Soup",
    "codeDescription": "This code snippet demonstrates how to extract product variant information from an e-commerce website using Python and Beautiful Soup. It iterates through products, downloads detail pages, and parses variant data.",
    "codeLanguage": "python",
    "codeTokens": 181,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/11_scraping_variants.md#2025-04-18_snippet_0",
    "pageTitle": "Scraping Product Variants with Python",
    "codeList": [
      {
        "language": "python",
        "code": "listing_url = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nlisting_soup = download(listing_url)\n\ndata = []\nfor product in listing_soup.select(\".product-item\"):\n    item = parse_product(product, listing_url)\n    product_soup = download(item[\"url\"])\n    vendor = product_soup.select_one(\".product-meta__vendor\").text.strip()\n\n    if variants := product_soup.select(\".product-form__option.no-js option\"):\n        for variant in variants:\n            data.append(item | {\"variant_name\": variant.text.strip()})\n    else:\n        item[\"variant_name\"] = None\n        data.append(item)"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Configuring JavaScript Code Input for Apify Actor",
    "codeDescription": "This snippet demonstrates how to set up a string input field for JavaScript code in an Apify Actor input schema. It includes a title, description, and a prefilled JavaScript function.",
    "codeLanguage": "json",
    "codeTokens": 105,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_3",
    "pageTitle": "Actor Input Schema Specification",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"title\": \"Page function\",\n    \"type\": \"string\",\n    \"description\": \"Function executed for each request\",\n    \"editor\": \"javascript\",\n    \"prefill\": \"async () => { return $('title').text(); }\"\n}"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Identifying and Accessing iFrames with Puppeteer",
    "codeDescription": "This snippet demonstrates how to launch a Puppeteer browser, navigate to IMDB, and identify the Twitter widget iFrame. It uses a loop to iterate through child frames and find the one with a URL containing 'twitter'.",
    "codeLanguage": "JavaScript",
    "codeTokens": 213,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/scraping_iframes.md#2025-04-18_snippet_0",
    "pageTitle": "Scraping iFrames with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch();\n\nconst page = await browser.newPage();\n\nawait page.goto('https://www.imdb.com');\nawait page.waitForTimeout(5000); // we need to wait for Twitter widget to load\n\nlet twitterFrame; // this will be populated later by our identified frame\n\nfor (const frame of page.mainFrame().childFrames()) {\n    // Here you can use few identifying methods like url(),name(),title()\n    if (frame.url().includes('twitter')) {\n        console.log('we found the Twitter iframe');\n        twitterFrame = frame;\n        // we assign this frame to myFrame to use it later\n    }\n}\n\nawait browser.close();"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Merging Data from Different Requests in Apify",
    "codeDescription": "Demonstrates how to access and merge data passed from a previous request with newly scraped data. This technique enables combining information from multiple pages into a single result object.",
    "codeLanguage": "js",
    "codeTokens": 66,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/request_labels_in_apify_actors.md#2025-04-18_snippet_3",
    "pageTitle": "Request Labels and Data Passing in Apify Actors",
    "codeList": [
      {
        "language": "js",
        "code": "const result = { ...request.userData.data, ...sellerDetail };"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Puppeteer Implementation with Navigation Handling",
    "codeDescription": "Complete example of browser automation using Puppeteer, showcasing proper navigation waiting and element interaction patterns.",
    "codeLanguage": "javascript",
    "codeTokens": 206,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/waiting.md#2025-04-18_snippet_2",
    "pageTitle": "Browser Automation: Waiting for Content and Events",
    "codeList": [
      {
        "language": "javascript",
        "code": "import * as fs from 'fs/promises';\nimport puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\n// Create a page and visit Google\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\n// Agree to the cookies policy\nawait page.click('button + button');\n\n// Type the query and visit the results page\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Wait for the first result to appear on the page,\n// then click on it\nawait page.waitForSelector('.g a');\nawait Promise.all([page.waitForNavigation(), page.click('.g a')]);\n\n// Our title extraction and screenshotting logic\n// will go here\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Key-Value Store Operations in JavaScript",
    "codeDescription": "Demonstrates various key-value store operations including saving objects, binary files, and accessing different stores.",
    "codeLanguage": "javascript",
    "codeTokens": 171,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_4",
    "pageTitle": "Basic Apify SDK Commands Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n\n// Save object to store (stringified to JSON)\nawait Actor.setValue('my_state', { something: 123 });\n\n// Save binary file to store with content type\nawait Actor.setValue('screenshot.png', buffer, { contentType: 'image/png' });\n\n// Get a record from the store (automatically parsed from JSON)\nconst value = await Actor.getValue('my_state');\n\n// Access another key-value store by its name\nconst store = await Actor.openKeyValueStore('screenshots-store');\nawait store.setValue('screenshot.png', buffer, { contentType: 'image/png' });\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.957
  },
  {
    "codeTitle": "Implementing Metamorph in JavaScript",
    "codeDescription": "Example of implementing a hotel review scraper using metamorph to transform into web-scraper Actor. Shows initialization, input handling, and metamorph operation with new input configuration.",
    "codeLanguage": "javascript",
    "codeTokens": 206,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/metamorph.md#2025-04-18_snippet_0",
    "pageTitle": "Actor Metamorph Operation Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n\n// Get input of your Actor.\nconst { hotelUrl } = await Actor.getInput();\n\n// Create input for apify/web-scraper\nconst newInput = {\n    startUrls: [{ url: hotelUrl }],\n    pageFunction: () => {\n        // Here you pass the page function that\n        // scrapes all the reviews ...\n    },\n    // ... and here would be all the additional\n    // input parameters.\n};\n\n// Transform the Actor run to apify/web-scraper\n// with the new input.\nawait Actor.metamorph('apify/web-scraper', newInput);\n\n// The line here will never be reached, because the\n// Actor run will be interrupted.\nawait Actor.exit();"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Reading Decrypted Secret Input in Actor (JavaScript)",
    "codeDescription": "This code demonstrates how to read the Actor input using Actor.getInput(). The secret fields are automatically decrypted when accessed this way, starting from apify package version 3.1.0.",
    "codeLanguage": "javascript",
    "codeTokens": 76,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/secret_input.md#2025-04-18_snippet_1",
    "pageTitle": "Using Secret Input Fields in Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "> await Actor.getInput();\n{\n    username: 'username',\n    password: 'password'\n}"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Incorrect Usage of Browser Context in Node.js",
    "codeDescription": "This snippet demonstrates a common mistake where browser-specific code is incorrectly run in the Node.js context, resulting in an error.",
    "codeLanguage": "JavaScript",
    "codeTokens": 121,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/index.md#2025-04-18_snippet_0",
    "pageTitle": "Executing Scripts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// This code is incorrect!\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// visit google\nawait page.goto('https://www.google.com/');\n\n// change background to green\ndocument.body.style.background = 'green';\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Blocking Resources with CDP Session in Puppeteer",
    "codeDescription": "Shows how to use a Chrome DevTools Protocol (CDP) session in Puppeteer to block resources while maintaining browser cache functionality.",
    "codeLanguage": "javascript",
    "codeTokens": 163,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_8",
    "pageTitle": "Reading and Intercepting Network Requests with Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Define our blocked extensions\nconst blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];\n\n// Use CDP session to block resources\nawait page.client().send('Network.setBlockedURLs', { urls: blockedExtensions });\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Scraping GitHub Repositories with Pagination using Playwright",
    "codeDescription": "This code snippet demonstrates how to scrape GitHub repositories from multiple pages using Playwright. It includes functions for scraping individual pages, handling pagination, and concurrent scraping of multiple pages.",
    "codeLanguage": "JavaScript",
    "codeTokens": 500,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_5",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\nimport * as cheerio from 'cheerio';\n\nconst repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;\n\n// Scrapes all repositories from a single page\nconst scrapeRepos = async (page) => {\n    const $ = cheerio.load(await page.content());\n\n    return [...$('.list-view-item')].map((item) => {\n        const repoElement = $(item);\n        return {\n            title: repoElement.find('h4').text().trim(),\n            description: repoElement.find('.repos-list-description').text().trim(),\n            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,\n        };\n    });\n};\n\nconst browser = await chromium.launch({ headless: false });\nconst firstPage = await browser.newPage();\n\nawait firstPage.goto(REPOSITORIES_URL);\n\nconst lastPageElement = firstPage.locator('a[aria-label*=\"Page \"]:nth-last-child(2)');\nconst lastPageLabel = await lastPageElement.getAttribute('aria-label');\nconst lastPageNumber = Number(lastPageLabel.replace(/\\D/g, ''));\n\n// Push all results from the first page to the repositories array\nrepositories.push(...(await scrapeRepos(firstPage)));\n\nawait firstPage.close();\n\nconst pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);\nconst promises = pageNumbers.map((pageNumber) => (async () => {\n    const paginatedPage = await browser.newPage();\n\n    // Construct the URL by setting the ?page=... parameter to value of pageNumber\n    const url = new URL(REPOSITORIES_URL);\n    url.searchParams.set('page', pageNumber);\n\n    // Scrape the page\n    await paginatedPage.goto(url.href);\n    const results = await scrapeRepos(paginatedPage);\n\n    // Push results to the repositories array\n    repositories.push(...results);\n\n    await paginatedPage.close();\n})(),\n);\nawait Promise.all(promises);\n\n// For brievity logging just the count of repositories scraped\nconsole.log(repositories.length);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Processing URLs and Crawling Product Pages with Node.js and Cheerio",
    "codeDescription": "This code snippet demonstrates how to extract product URLs from a store page, then crawl each product page to extract its title. It uses got-scraping for HTTP requests and Cheerio for HTML parsing.",
    "codeLanguage": "JavaScript",
    "codeTokens": 355,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/first_crawl.md#2025-04-18_snippet_0",
    "pageTitle": "Web Crawling Tutorial with Node.js, Cheerio, and HTTP Client",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst productLinks = $('a.product-item__title');\n\n// Prepare an empty array for our product URLs.\nconst productUrls = [];\n\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n\n    // Collect absolute product URLs.\n    productUrls.push(absoluteUrl);\n}\n\n// Loop over the stored URLs to process\n// each product page individually.\nfor (const url of productUrls) {\n    // Download HTML.\n    const productResponse = await gotScraping(url);\n    const productHtml = productResponse.body;\n\n    // Load into Cheerio to parse the HTML.\n    const $productPage = cheerio.load(productHtml);\n\n    // Extract the product's title from the <h1> tag.\n    const productPageTitle = $productPage('h1').text().trim();\n\n    // Print the title to the terminal to see\n    // confirm we downloaded the correct pages.\n    console.log(productPageTitle);\n}"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Advanced Error Reporting with Apify SDK in JavaScript",
    "codeDescription": "Illustrates a comprehensive error reporting system using Apify SDK, including saving snapshots, creating error reports, and storing them in a named dataset.",
    "codeLanguage": "javascript",
    "codeTokens": 440,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/analyzing_pages_and_fixing_errors.md#2025-04-18_snippet_3",
    "pageTitle": "Analyzing and Fixing Errors in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport { puppeteerUtils } from 'crawlee';\n\nawait Actor.init();\n// ...\n// Let's create reporting dataset\n// If you already have one, this will continue adding to it\nconst reportingDataset = await Actor.openDataset('REPORTING');\n\ntry {\n    // Sensitive code block\n    // ...\n} catch (error) {\n    // Change the way you save it depending on what tool you use\n    const randomNumber = Math.random();\n    const key = `ERROR-LOGIN-${randomNumber}`;\n    // The store gets removed with the run after data retention period so the links will stop working eventually\n    // You can store the snapshots infinitely in a named KV store by adding `keyValueStoreName` option\n    await puppeteerUtils.saveSnapshot(page, { key });\n\n    // To create the reporting URLs, we need to know the Key-Value store and run IDs\n    const { actorRunId, defaultKeyValueStoreId } = Actor.getEnv();\n\n    // We create a report object\n    const report = {\n        errorType: 'login',\n        errorMessage: error.toString(),\n        // .html and .jpg file extensions are added automatically by the saveSnapshot function\n        htmlSnapshotUrl: `https://api.apify.com/v2/key-value-stores/${defaultKeyValueStoreId}/records/${key}.html`,\n        screenshotUrl: `https://api.apify.com/v2/key-value-stores/${defaultKeyValueStoreId}/records/${key}.jpg`,\n        runUrl: `https://console.apify.com/actors/runs/${actorRunId}`,\n    };\n\n    // And we push the report to our reporting dataset\n    await reportingDataset.pushData(report);\n\n    // You know where the code crashed so you can explain here\n    throw new Error('Request failed during login with an error', { cause: error });\n}\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Scraping F1 Teams Count with Python and Beautiful Soup",
    "codeDescription": "This exercise solution demonstrates how to scrape and count the number of F1 teams listed on the Formula 1 website.",
    "codeLanguage": "python",
    "codeTokens": 103,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_4",
    "pageTitle": "Parsing HTML with Python for Web Scraping",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://www.formula1.com/en/teams\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\nprint(len(soup.select(\".outline\")))"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Crawling Product Detail Pages in JavaScript",
    "codeDescription": "Implements a web crawler that visits a category page, extracts product URLs, and then visits each product page to extract the title. It demonstrates error handling and URL parsing.",
    "codeLanguage": "JavaScript",
    "codeTokens": 281,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_2",
    "pageTitle": "Web Scraping Tutorial: Crawling and Data Extraction",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    productUrls.push(absoluteUrl);\n}\n\nfor (const url of productUrls) {\n    try {\n        const productResponse = await gotScraping(url);\n        const productHtml = productResponse.body;\n        const $productPage = cheerio.load(productHtml);\n        const productPageTitle = $productPage('h1').text().trim();\n        console.log(productPageTitle);\n    } catch (error) {\n        console.error(error.message, url);\n    }\n}"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Defining and Enqueuing Pivot Ranges in JavaScript",
    "codeDescription": "This snippet demonstrates how to set up initial pivot price ranges, create filter URLs, and enqueue requests for a web scraping project using Apify and Crawlee.",
    "codeLanguage": "javascript",
    "codeTokens": 485,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/crawling/crawling-with-search.md#2025-04-18_snippet_0",
    "pageTitle": "Crawling with Search Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport { CheerioCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst MAX_PRODUCTS_PAGINATION = 1000;\n\n// Just an example, choose what makes sense for your site\nconst PIVOT_PRICE_RANGES = [\n    { min: 0, max: 9.99 },\n    { min: 10, max: 99.99 },\n    { min: 100, max: 999.99 },\n    { min: 1000, max: 9999.99 },\n    { min: 10000, max: null }, // open-ended\n];\n\n// Let's create a helper function for creating the filter URLs, you can move those to a utils.js file\nconst createFilterUrl = ({ min, max }) => {\n    const minString = `min_price=${min}`;\n    // We don't want to pass the parameter at all if it is null (open-ended)\n    const maxString = max ? `&max_price=${max}` : '';\n    return `https://www.mysite.com/products?${minString}${maxString}`;\n};\n\n// And another helper for getting filters back from the URL, we could also pass them in userData\nconst getFiltersFromUrl = (url) => {\n    const min = Number(url.match(/min_price=([0-9.]+)/)[1]);\n    // Max price might be empty\n    const maxMatch = url.match(/max_price=([0-9.]+)/);\n    const max = maxMatch ? Number(maxMatch[1]) : null;\n    return { min, max };\n};\n\n// Actor setup things here\nconst crawler = new CheerioCrawler({\n    async requestHandler(context) {\n        // ...\n    },\n});\n\n// Let's create the pivot requests\nconst initialRequests = [];\nfor (const { min, max } of PIVOT_PRICE_RANGES) {\n    initialRequests.push({\n        url: createFilterUrl({ min, max }),\n        label: 'FILTER',\n    });\n}\n// Let's start the crawl\nawait crawler.run(initialRequests);\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Parsing Product Inventory with RegEx and BeautifulSoup",
    "codeDescription": "Script that scrapes product information from a Shopify store using BeautifulSoup and regular expressions to extract product titles and inventory numbers. Uses httpx for HTTP requests and re for regular expression parsing.",
    "codeLanguage": "python",
    "codeTokens": 190,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/07_extracting_data.md#2025-04-18_snippet_5",
    "pageTitle": "Data Extraction and Price Scraping with Python",
    "codeList": [
      {
        "language": "python",
        "code": "import re\nimport httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    units_text = product.select_one(\".product-item__inventory\").text\n    if re_match := re.search(r\"\\d+\", units_text):\n        units = int(re_match.group())\n    else:\n        units = 0\n\n    print(title, units)"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Updating Request Handler to Track Successful Requests",
    "codeDescription": "Modifies the request handler for offers to increment the total saved count in the Stats utility class for each offer processed.",
    "codeLanguage": "javascript",
    "codeTokens": 174,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/saving_stats.md#2025-04-18_snippet_3",
    "pageTitle": "Saving Run Statistics for Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "router.addHandler(labels.OFFERS, async ({ $, request }) => {\n    const { data } = request.userData;\n\n    const { asin } = data;\n\n    for (const offer of $('#aod-offer')) {\n        tracker.incrementASIN(asin);\n        // Add 1 to totalSaved for every offer\n        Stats.success();\n\n        const element = $(offer);\n\n        await dataset.pushData({\n            ...data,\n            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\n            offer: element.find('.a-price .a-offscreen').text().trim(),\n        });\n    }\n});"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Initializing Basic Actor Structure",
    "codeDescription": "Basic structure for an Apify Actor with initialization and exit calls.",
    "codeLanguage": "javascript",
    "codeTokens": 56,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/integrating_webhooks.md#2025-04-18_snippet_0",
    "pageTitle": "Integrating Webhooks with Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "// main.js\nimport { Actor } from 'apify';\n\nawait Actor.init();\n\n// ...\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Uploading a File with Puppeteer in JavaScript",
    "codeDescription": "This snippet shows how to upload a file using Puppeteer. It first selects the file input element, then uses the uploadFile() method to attach the previously downloaded file.",
    "codeLanguage": "javascript",
    "codeTokens": 80,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_6",
    "pageTitle": "Submitting a Form with File Attachment in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "const fileInput = await page.$('input[type=file]');\nawait fileInput.uploadFile('./file.pdf');"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Running Website Content Crawler with Apify Python SDK",
    "codeDescription": "Code that calls the Website Content Crawler Actor to crawl the Qdrant documentation website and extract text content using the Apify Python SDK.",
    "codeLanguage": "python",
    "codeTokens": 85,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/qdrant.md#2025-04-18_snippet_2",
    "pageTitle": "Qdrant Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "actor_call = client.actor(\"apify/website-content-crawler\").call(\n    run_input={\"startUrls\": [{\"url\": \"https://qdrant.tech/documentation/\"}]}\n)"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Implementing PuppeteerCrawler to Scrape Beer Data",
    "codeDescription": "This code uses PuppeteerCrawler to process each URL from the RequestList, extract beer data using Puppeteer, and save it to a dataset.",
    "codeLanguage": "JavaScript",
    "codeTokens": 176,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_from_sitemaps.md#2025-04-18_snippet_5",
    "pageTitle": "Scraping Data from Sitemaps Using Crawlee",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "const crawler = new PuppeteerCrawler({\n    requestList,\n    async requestHandler({ page }) {\n        const beerPage = await page.evaluate(() => {\n            return document.getElementsByClassName('productreviews').length;\n        });\n        if (!beerPage) return;\n\n        const data = await page.evaluate(() => {\n            const title = document.getElementsByTagName('h1')[0].innerText;\n            const [brewery, beer] = title.split(':');\n            const description = document.getElementsByClassName('productreviews')[0].innerText;\n\n            return { brewery, beer, description };\n        });\n\n        await Dataset.pushData(data);\n    },\n});"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Browser Context Data Extraction with jQuery",
    "codeDescription": "Demonstrates how to inject jQuery into the page and use it within page.evaluate() to extract product data. Uses jQuery selectors to find and extract product names and prices.",
    "codeLanguage": "javascript",
    "codeTokens": 175,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_2",
    "pageTitle": "Data Extraction Guide for Playwright and Puppeteer",
    "codeList": [
      {
        "language": "javascript",
        "code": "await page.addScriptTag({ url: 'https://code.jquery.com/jquery-3.6.0.min.js' });\n\nconst products = await page.evaluate(() => {\n    const productCards = Array.from($('a[class*=\"ProductCard_root\"]'));\n\n    return productCards.map((element) => {\n        const card = $(element);\n\n        const name = card.find('h3[class*=\"ProductCard_name\"]').text();\n        const price = card.find('div[class*=\"ProductCard_price\"]').text();\n\n        return {\n            name,\n            price,\n        };\n    });\n});\n\nconsole.log(products);"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Saving Error Snapshots with Puppeteer in JavaScript",
    "codeDescription": "Demonstrates how to save screenshots and HTML snapshots when an error occurs during web scraping using Puppeteer and the Apify SDK.",
    "codeLanguage": "javascript",
    "codeTokens": 216,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/analyzing_pages_and_fixing_errors.md#2025-04-18_snippet_2",
    "pageTitle": "Analyzing and Fixing Errors in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { puppeteerUtils } from 'crawlee';\n\n// ...\n// storeId is ID of current key value store, where we save snapshots\nconst storeId = Actor.getEnv().defaultKeyValueStoreId;\ntry {\n    // Sensitive code block\n    // ...\n} catch (error) {\n    // Change the way you save it depending on what tool you use\n    const randomNumber = Math.random();\n    const key = `ERROR-LOGIN-${randomNumber}`;\n    await puppeteerUtils.saveSnapshot(page, { key });\n    const screenshotLink = `https://api.apify.com/v2/key-value-stores/${storeId}/records/${key}.jpg`;\n\n    // You know where the code crashed so you can explain here\n    throw new Error('Request failed during login with an error', { cause: error });\n}\n// ..."
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Creating and Attaching Vector Store to the OpenAI Assistant",
    "codeDescription": "Creates a new OpenAI Vector Store and updates the assistant to use this vector store for the file_search tool, enabling it to access custom data.",
    "codeLanguage": "python",
    "codeTokens": 101,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_13",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "vector_store = client.beta.vector_stores.create(name=\"Support assistant vector store\")\n\nassistant = client.beta.assistants.update(\n    assistant_id=my_assistant.id,\n    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n)"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Extracting Link URLs in Node.js using Cheerio",
    "codeDescription": "Node.js script that uses Cheerio to extract all link URLs from a specific webpage. It demonstrates downloading HTML content, parsing it with Cheerio, and extracting href attributes from all anchor tags.",
    "codeLanguage": "JavaScript",
    "codeTokens": 185,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/finding_links.md#2025-04-18_snippet_2",
    "pageTitle": "Finding Links in HTML for Web Scraping",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { gotScraping } from 'got-scraping';\nimport cheerio from 'cheerio';\n\nconst url = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconsole.log('Downloading HTML...');\nconst response = await gotScraping(url);\nconst html = response.body;\n\nconsole.log('Processing data...');\nconst $ = cheerio.load(html);\n\n// Here's the magic!\nconst links = $('a');\n\n// Print all URLs to the console\nfor (const link of links) {\n    const url = $(link).attr('href');\n    console.log(url);\n}\n\nconsole.log('Done.');"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Generating Dynamic Headers with got-scraping",
    "codeDescription": "Demonstrates how to use got-scraping to automatically generate request-specific headers with custom options for browsers, devices, locales, and operating systems.",
    "codeLanguage": "javascript",
    "codeTokens": 152,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/general_api_scraping/cookies_headers_tokens.md#2025-04-18_snippet_2",
    "pageTitle": "API Authentication Guide with Cookies, Headers, and Tokens",
    "codeList": [
      {
        "language": "javascript",
        "code": "const response = await gotScraping({\n    url: 'https://example.com',\n    headerGeneratorOptions: {\n        browsers: [\n            {\n                name: 'chrome',\n                minVersion: 87,\n                maxVersion: 89,\n            },\n        ],\n        devices: ['desktop'],\n        locales: ['de-DE', 'en-US'],\n        operatingSystems: ['windows', 'linux'],\n    },\n    headers: {\n        'some-header': 'Hello, Academy!',\n    },\n});"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Using gotScraping with Proxy Configuration and Session Management",
    "codeDescription": "Shows how to use the got-scraping library with Apify's proxy configuration. The example creates a proxy URL with a named session and uses it to make multiple HTTP requests with the same IP address, then compares the client IPs to verify they match.",
    "codeLanguage": "javascript",
    "codeTokens": 221,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_7",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport { gotScraping } from 'got-scraping';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl('my_session');\n\nconst response1 = await gotScraping({\n    url: 'https://api.apify.com/v2/browser-info',\n    proxyUrl,\n    responseType: 'json',\n});\n\nconst response2 = await gotScraping({\n    url: 'https://api.apify.com/v2/browser-info',\n    proxyUrl,\n    responseType: 'json',\n});\n\nconsole.log(response1.body.clientIp);\nconsole.log('Should be the same as');\nconsole.log(response2.body.clientIp);\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Pre-injecting Scripts with Playwright",
    "codeDescription": "Demonstrates how to inject custom code before page load using Playwright's page.addInitScript(). The example shows overriding the native addEventListener function to prevent event listeners from being added.",
    "codeLanguage": "javascript",
    "codeTokens": 135,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/injecting_code.md#2025-04-18_snippet_0",
    "pageTitle": "Injecting Code in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.addInitScript(() => {\n    // Override the prototype\n    Node.prototype.addEventListener = () => { /* do nothing */ };\n});\n\nawait page.goto('https://google.com');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Picking Session for Proxy Rotation in Node.js",
    "codeDescription": "Implements an algorithm to select a session from existing working sessions or create a new one with a random user agent. This function helps in managing and rotating proxy sessions efficiently.",
    "codeLanguage": "JavaScript",
    "codeTokens": 446,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/filter_blocked_requests_using_sessions.md#2025-04-18_snippet_2",
    "pageTitle": "Filtering Blocked Proxies with Sessions in Node.js",
    "codeList": [
      {
        "language": "javascript",
        "code": "const pickSession = (sessions, maxSessions = 100) => {\n\n    // sessions is our sessions object, at the beginning instantiated as {}\n    // maxSessions is a constant which should be the number of working proxies we aspire to have.\n    // The lower the number, the faster you will use the working proxies\n    // but the faster the new one will not be picked\n    // 100 is reasonable default\n    // Since sessions is an object, we prepare an array of the session names\n    const sessionsKeys = Object.keys(sessions);\n\n    console.log(`Currently we have ${sessionsKeys.length} working sessions`);\n\n    // We define a random floating number from 0 to 1 that will serve\n    // both as a chance to pick the session and its possible name\n    const randomNumber = Math.random();\n\n    // The chance to pick a session will be higher when we have more working sessions\n    const chanceToPickSession = sessionsKeys.length / maxSessions;\n\n    console.log(`Chance to pick a working session is ${Math.round(chanceToPickSession * 100)}%`);\n\n    // If the chance is higher than the random number, we pick one from the working sessions\n    const willPickSession = chanceToPickSession > randomNumber;\n\n    if (willPickSession) {\n        // We randomly pick one of the working sessions and return it\n        const indexToPick = Math.floor(sessionsKeys.length * Math.random());\n\n        const nameToPick = sessionsKeys[indexToPick];\n\n        console.log(`We picked a working session: ${nameToPick} on index ${indexToPick}`);\n\n        return sessions[nameToPick];\n    }\n    // We create a new session object, assign a random userAgent to it and return it\n\n    console.log(`Creating new session: ${randomNumber}`);\n\n    return {\n        name: randomNumber.toString(),\n        userAgent: Apify.utils.getRandomUserAgent(),\n    };\n\n};"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Pushing Data to Dataset in JavaScript",
    "codeDescription": "This code snippet demonstrates how to use Actor.pushData() to store data into a dataset using the Apify SDK in JavaScript. It includes various data types such as numeric, text, boolean, date, array, and object fields.",
    "codeLanguage": "javascript",
    "codeTokens": 184,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/index.md#2025-04-18_snippet_0",
    "pageTitle": "Dataset Schema Specification for Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n// Initialize the JavaScript SDK\nawait Actor.init();\n\n/**\n * Actor code\n */\nawait Actor.pushData({\n    numericField: 10,\n    pictureUrl: 'https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_92x30dp.png',\n    linkUrl: 'https://google.com',\n    textField: 'Google',\n    booleanField: true,\n    dateField: new Date(),\n    arrayField: ['#hello', '#world'],\n    objectField: {},\n});\n\n\n// Exit successfully\nawait Actor.exit();"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Type Definitions for TypeScript Scraper",
    "codeDescription": "This snippet contains the type definitions used in the TypeScript scraper, including interfaces for Product, ResponseData, and UserInput, as well as an enum for SortOrder.",
    "codeLanguage": "typescript",
    "codeTokens": 189,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_9",
    "pageTitle": "TypeScript Mini-Project Documentation",
    "codeList": [
      {
        "language": "typescript",
        "code": "// types.ts\nexport interface Product {\n    id: number;\n    title: string;\n    description: string;\n    price: number;\n    discountPercentage: number;\n    rating: number;\n    stock: number;\n    brand: string;\n    category: string;\n    thumbnail: string;\n    images: string[];\n}\n\nexport interface ResponseData {\n    products: Product[];\n}\n\nexport type ModifiedProduct = Omit<Product, 'images'>;\n\nexport enum SortOrder {\n    ASC = 'ascending',\n    DESC = 'descending',\n}\n\nexport interface UserInput<RemoveImages extends boolean = boolean> {\n    sort: 'ascending' | 'descending';\n    removeImages: RemoveImages;\n}"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Full Web Scraper Page Function with Conditional Logic",
    "codeDescription": "The complete page function for Web Scraper that handles both the initial page and detail pages. Implements conditional logic based on request labels, with specific data extraction for Actor detail pages.",
    "codeLanguage": "javascript",
    "codeTokens": 281,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_5",
    "pageTitle": "Scraping with Web Scraper Tutorial",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    // use jQuery as $\n    const { request, log, skipLinks, jQuery: $ } = context;\n\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        // Do some stuff later.\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Locating Child Elements with BeautifulSoup",
    "codeDescription": "Shows how to locate and extract specific child elements (title and price) from product cards using multiple selectors.",
    "codeLanguage": "python",
    "codeTokens": 152,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/06_locating_elements.md#2025-04-18_snippet_1",
    "pageTitle": "Locating HTML Elements with Python using BeautifulSoup",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor product in soup.select(\".product-item\"):\n    titles = product.select(\".product-item__title\")\n    first_title = titles[0].text\n\n    prices = product.select(\".price\")\n    first_price = prices[0].text\n\n    print(first_title, first_price)"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Complete CrewAI Integration Script",
    "codeDescription": "Full implementation of the CrewAI integration with Apify for TikTok profile analysis",
    "codeLanguage": "python",
    "codeTokens": 417,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_7",
    "pageTitle": "CrewAI Integration with Apify Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import os\nfrom crewai import Agent, Task, Crew\nfrom crewai_tools import ApifyActorsTool\nfrom langchain_openai import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\"\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nbrowser_tool = ApifyActorsTool(actor_name=\"apify/rag-web-browser\")\ntiktok_tool = ApifyActorsTool(actor_name=\"clockworks/free-tiktok-scraper\")\n\nsearch_agent = Agent(\n    role=\"Web Search Specialist\",\n    goal=\"Find the TikTok profile URL on the web\",\n    backstory=\"Expert in web searching and data retrieval\",\n    tools=[browser_tool],\n    llm=llm,\n    verbose=True\n)\n\nanalysis_agent = Agent(\n    role=\"TikTok Profile Analyst\",\n    goal=\"Extract and analyze data from the TikTok profile\",\n    backstory=\"Skilled in social media data extraction and analysis\",\n    tools=[tiktok_tool],\n    llm=llm,\n    verbose=True\n)\n\nsearch_task = Task(\n    description=\"Search the web for the OpenAI TikTok profile URL.\",\n    agent=search_agent,\n    expected_output=\"A URL linking to the OpenAI TikTok profile.\"\n)\nanalysis_task = Task(\n    description=\"Extract data from the OpenAI TikTok profile URL and provide a profile summary and details about the latest post.\",\n    agent=analysis_agent,\n    context=[search_task],\n    expected_output=\"A summary of the OpenAI TikTok profile including followers and likes, plus details about their most recent post.\"\n)\n\ncrew = Crew(\n    agents=[search_agent, analysis_agent],\n    tasks=[search_task, analysis_task],\n    process=\"sequential\"\n)\n\nresult = crew.kickoff()\nprint(result)"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Configuring Main Crawler Script",
    "codeDescription": "Main crawler configuration script that sets up CheerioCrawler with input handling and request routing for Amazon product searches.",
    "codeLanguage": "javascript",
    "codeTokens": 269,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/initializing_and_setting_up.md#2025-04-18_snippet_1",
    "pageTitle": "Crawlee Web Scraper Setup Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "// main.js\nimport { CheerioCrawler, KeyValueStore, log } from 'crawlee';\nimport { router } from './routes.js';\n\n// Grab our keyword from the input\nconst { keyword } = await KeyValueStore.getInput();\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n\n    // If you have access to Apify Proxy, you can use residential proxies and\n    // high retry count which helps with blocking\n    // If you don't, your local IP address will likely be fine for a few requests if you scrape slowly.\n    // proxyConfiguration: await Actor.createProxyConfiguration({ groups: ['RESIDENTIAL'] }),\n    // maxRequestRetries: 10,\n});\n\nlog.info('Starting the crawl.');\nawait crawler.run([{\n    // Turn the keyword into a link we can make a request with\n    url: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,\n    label: 'START',\n    userData: {\n        keyword,\n    },\n}]);\nlog.info('Crawl finished.');"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Extracting Page Title with Puppeteer/Playwright",
    "codeDescription": "This snippet demonstrates how to grab the title of a web page using the page.title() method in both Puppeteer and Playwright. It retrieves the title and logs it to the console.",
    "codeLanguage": "javascript",
    "codeTokens": 87,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/page_methods.md#2025-04-18_snippet_0",
    "pageTitle": "Page Methods in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Grab the title and set it to a variable\nconst title = await page.title();\n\n// Log the title to the console\nconsole.log(title);"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Configuring PlaywrightCrawler for Cloudflare Bypass",
    "codeDescription": "Code snippet showing how to configure a PlaywrightCrawler instance to handle Cloudflare challenges by removing default blocked status code handling. This allows the crawler to continue operation even when encountering 403 status codes during challenge evaluation.",
    "codeLanguage": "javascript",
    "codeTokens": 91,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/cloudflare_challenge.md#2025-04-18_snippet_0",
    "pageTitle": "Cloudflare Browser Check Bypass Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "const crawler = new PlaywrightCrawler({\n    ...otherOptions,\n    sessionPoolOptions: {\n        blockedStatusCodes: [],\n    },\n});"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Using the ASPX Form Submission Function in Web Scraper",
    "codeDescription": "Example of how to use the enqueueAspxForm utility function in a Web Scraper Page function. This demonstrates submitting a form on an architect finder website with proper parameters.",
    "codeLanguage": "javascript",
    "codeTokens": 115,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/submitting_forms_on_aspx_pages.md#2025-04-18_snippet_1",
    "pageTitle": "Submitting forms on .ASPX pages in Web Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "await enqueueAspxForm({\n    url: 'http://architectfinder.aia.org/frmSearch.aspx',\n    userData: { label: 'SEARCH-RESULT' },\n}, 'form[name=\"aspnetForm\"]', '#ctl00_ContentPlaceHolder1_btnSearch', false);"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "GO Actor Dockerfile Example",
    "codeDescription": "An example Dockerfile for a Go Actor. It uses the official Go Alpine image, copies the source code, downloads dependencies, and builds the executable. The CMD instruction specifies how to run the compiled executable.",
    "codeLanguage": "Dockerfile",
    "codeTokens": 103,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/docker_file.md#2025-04-18_snippet_3",
    "pageTitle": "Writing Dockerfiles for Apify Actors",
    "codeList": [
      {
        "language": "Dockerfile",
        "code": "FROM golang:1.17.1-alpine\n\nWORKDIR /app\nCOPY . .\n\nRUN go mod download\n\nRUN go build -o /example-actor\nCMD [\"/example-actor\"]"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Retrieving JavaScript Objects with Puppeteer",
    "codeDescription": "This code snippet shows how to use Puppeteer to execute JavaScript in the browser context and retrieve a window object. It accesses the '__sc_hydration' object directly from the window.",
    "codeLanguage": "javascript",
    "codeTokens": 75,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/js_in_html.md#2025-04-18_snippet_1",
    "pageTitle": "Scraping Hidden JavaScript Objects in HTML",
    "codeList": [
      {
        "language": "javascript",
        "code": "const data = await page.evaluate(() => window.__sc_hydration);\n\nconsole.log(data);"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Complete Page Function for Scraping Actor Details",
    "codeDescription": "This function handles both the initial page and detail pages. It extracts actor details on detail pages and logs information on the initial page.",
    "codeLanguage": "javascript",
    "codeTokens": 270,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_5",
    "pageTitle": "Cheerio Scraper Tutorial Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    // $ is Cheerio\n    const { request, log, skipLinks, $ } = context;\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        // Do some stuff later.\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Implementing Page Function for Apify Store Scraping in JavaScript",
    "codeDescription": "This code snippet shows the implementation of a page function for scraping Apify Store actors. It handles both the start page and detail pages, extracting relevant information and enqueueing new requests.",
    "codeLanguage": "JavaScript",
    "codeTokens": 371,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_7",
    "pageTitle": "Cheerio Scraper Tutorial Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { request, log, skipLinks, $ } = context;\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n\n        const dataJson = $('#__NEXT_DATA__').html();\n        // We requested HTML, but the data are actually JSON.\n        const data = JSON.parse(dataJson);\n\n        for (const item of data.props.pageProps.items) {\n            const { name, username } = item;\n            const actorDetailUrl = `https://apify.com/${username}/${name}`;\n            await context.enqueueRequest({\n                url: actorDetailUrl,\n                userData: {\n                    label: 'DETAIL',\n                },\n            });\n        }\n    }\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Initializing and Running Cheerio Crawler for Amazon Scraping in JavaScript",
    "codeDescription": "This code snippet shows the main file that initializes the Cheerio crawler, configures it with the router, and starts the crawling process for Amazon product search.",
    "codeLanguage": "JavaScript",
    "codeTokens": 207,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/modularity.md#2025-04-18_snippet_2",
    "pageTitle": "Understanding Modularity in Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "// main.js\nimport { CheerioCrawler, log, KeyValueStore } from 'crawlee';\nimport { router } from './routes.js';\nimport { BASE_URL } from './constants.js';\n\nconst { keyword = 'iphone' } = (await KeyValueStore.getInput()) ?? {};\n\nconst crawler = new CheerioCrawler({\n    requestHandler: router,\n});\n\nawait crawler.addRequests([\n    {\n        // Use BASE_URL here instead\n        url: `${BASE_URL}/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,\n        label: 'START',\n        userData: {\n            keyword,\n        },\n    },\n]);\n\nlog.info('Starting the crawl.');\nawait crawler.run();\nlog.info('Crawl finished.');"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Switching to PuppeteerCrawler for Dynamic Content",
    "codeDescription": "Modifies the crawler to use PuppeteerCrawler instead of CheerioCrawler to handle dynamic content. This version still doesn't handle lazy-loaded images.",
    "codeLanguage": "javascript",
    "codeTokens": 279,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/dealing_with_dynamic_pages.md#2025-04-18_snippet_3",
    "pageTitle": "Scraping Dynamic Pages with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { PuppeteerCrawler } from 'crawlee';\n\nconst BASE_URL = 'https://demo-webstore.apify.org';\n\n// Switch CheerioCrawler to PuppeteerCrawler\nconst crawler = new PuppeteerCrawler({\n    // Replace \"$\" with \"page\"\n    requestHandler: async ({ parseWithCheerio, request }) => {\n        // Create the $ Cheerio object based on the page's content\n        const $ = await parseWithCheerio();\n\n        const products = $('a[href*=\"/product/\"]');\n\n        const results = [...products].map((product) => {\n            const elem = $(product);\n\n            const title = elem.find('h3').text();\n            const price = elem.find('div[class*=\"price\"]').text();\n            const image = elem.find('img[src]').attr('src');\n\n            return {\n                title,\n                price,\n                image: new URL(image, BASE_URL).href,\n            };\n        });\n\n        console.log(results);\n    },\n});\n\nawait crawler.run([{ url: 'https://demo-webstore.apify.org/search/new-arrivals' }]);"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Adding Data to Default Dataset in JavaScript",
    "codeDescription": "Demonstrates how to use the Apify JavaScript SDK to add single and multiple items to the default dataset in an Actor. It emphasizes the importance of using 'await' with pushData() to ensure data storage completion.",
    "codeLanguage": "javascript",
    "codeTokens": 143,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_9",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Import the JavaScript SDK into your project\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\n// Add one item to the default dataset\nawait Actor.pushData({ foo: 'bar' });\n\n// Add multiple items to the default dataset\nawait Actor.pushData([{ foo: 'hotel' }, { foo: 'cafe' }]);\n\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Complete Web Scraping Script with Playwright",
    "codeDescription": "This is a full script using Playwright to navigate to Google, perform a search, click on a result, extract the page title, and take a screenshot. It demonstrates the use of various Page methods in a real-world scenario.",
    "codeLanguage": "javascript",
    "codeTokens": 235,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/page_methods.md#2025-04-18_snippet_2",
    "pageTitle": "Page Methods in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\n// Create a page and visit Google\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\n// Agree to the cookies policy\nawait page.click('button:has-text(\"Accept all\")');\n\n// Type the query and visit the results page\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Click on the first result\nawait page.click('.g a');\nawait page.waitForLoadState('load');\n\n// Grab the page's title and log it to the console\nconst title = await page.title();\nconsole.log(title);\n\n// Take a screenshot and write it to the filesystem\nawait page.screenshot({ path: 'screenshot.png' });\n\nawait browser.close();"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Playwright Implementation with Load State Handling",
    "codeDescription": "Complete example of browser automation using Playwright, including handling of cookies, search interactions, and proper page load state management.",
    "codeLanguage": "javascript",
    "codeTokens": 197,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/waiting.md#2025-04-18_snippet_1",
    "pageTitle": "Browser Automation: Waiting for Content and Events",
    "codeList": [
      {
        "language": "javascript",
        "code": "import * as fs from 'fs/promises';\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\n// Create a page and visit Google\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\n// Agree to the cookies policy\nawait page.click('button:has-text(\"Accept all\")');\n\n// Type the query and visit the results page\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Click on the first result\nawait page.click('.g a');\nawait page.waitForLoadState('load');\n\n// Our title extraction and screenshotting logic\n// will go here\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Querying Elements with Cheerio in Node.js",
    "codeDescription": "This snippet demonstrates how to use Cheerio to query and select elements from parsed HTML, similar to using document.querySelectorAll() in the browser.",
    "codeLanguage": "JavaScript",
    "codeTokens": 63,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/node_continued.md#2025-04-18_snippet_0",
    "pageTitle": "Extracting Data with Node.js and Cheerio",
    "codeList": [
      {
        "language": "javascript",
        "code": "// In Node.js with Cheerio\nconst products = $('.product-item');"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Complete Actor Implementation",
    "codeDescription": "Final implementation combining all components including initialization, both client and API implementations, and execution logic.",
    "codeLanguage": "javascript",
    "codeTokens": 313,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/using_api_and_client.md#2025-04-18_snippet_4",
    "pageTitle": "Using Apify API and JavaScript Client Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport axios from 'axios';\n\nawait Actor.init();\n\nconst { useClient, memory, fields, maxItems } = await Actor.getInput();\n\nconst TASK = 'YOUR_USERNAME~demo-actor-task';\n\nconst withClient = async () => {\n    const client = Actor.newClient();\n    const task = client.task(TASK);\n\n    const { id } = await task.call({ memory });\n\n    const dataset = client.run(id).dataset();\n\n    const items = await dataset.downloadItems('csv', {\n        limit: maxItems,\n        fields,\n    });\n\n    return Actor.setValue('OUTPUT', items, { contentType: 'text/csv' });\n};\n\nconst withAPI = async () => {\n    const uri = `https://api.apify.com/v2/actor-tasks/${TASK}/run-sync-get-dataset-items?`;\n    const url = new URL(uri);\n\n    url.search = new URLSearchParams({\n        memory,\n        format: 'csv',\n        limit: maxItems,\n        fields: fields.join(','),\n        token: process.env.APIFY_TOKEN,\n    });\n\n    const { data } = await axios.post(url.toString());\n\n    return Actor.setValue('OUTPUT', data, { contentType: 'text/csv' });\n};\n\nif (useClient) {\n    await withClient();\n} else {\n    await withAPI();\n}\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Web Scraping with JavaScript using Axios and Cheerio",
    "codeDescription": "This code snippet demonstrates a basic web scraper built with the Apify SDK. It fetches HTML content from a specified URL using Axios, parses it with Cheerio, extracts all headings (H1-H6) with their text content, and stores the data in an Apify Dataset.",
    "codeLanguage": "JavaScript",
    "codeTokens": 407,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/creating_actors.md#2025-04-18_snippet_0",
    "pageTitle": "Creating Actors in Apify Console",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "// Axios - Promise based HTTP client for the browser and node.js (Read more at https://axios-http.com/docs/intro).\nimport { Actor } from 'apify';\nimport axios from 'axios';\n// Cheerio - The fast, flexible & elegant library for parsing and manipulating HTML and XML (Read more at https://cheerio.js.org/).\nimport * as cheerio from 'cheerio';\n// Apify SDK - toolkit for building Apify Actors (Read more at https://docs.apify.com/sdk/js/).\n\n// The init() call configures the Actor for its environment. It's recommended to start every Actor with an init().\nawait Actor.init();\n\n// Structure of input is defined in input_schema.json\nconst input = await Actor.getInput();\nconst { url } = input;\n\n// Fetch the HTML content of the page.\nconst response = await axios.get(url);\n\n// Parse the downloaded HTML with Cheerio to enable data extraction.\nconst $ = cheerio.load(response.data);\n\n// Extract all headings from the page (tag name and text).\nconst headings = [];\n$('h1, h2, h3, h4, h5, h6').each((i, element) => {\n    const headingObject = {\n        level: $(element).prop('tagName').toLowerCase(),\n        text: $(element).text(),\n    };\n    console.log('Extracted heading', headingObject);\n    headings.push(headingObject);\n});\n\n// Save headings to Dataset - a table-like storage.\nawait Actor.pushData(headings);\n\n// Gracefully exit the Actor process. It's recommended to quit all Actors with an exit().\nawait Actor.exit();"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Loading Data from Apify Actor into LlamaIndex Documents",
    "codeDescription": "Python code demonstrating how to use the ApifyActor class to run the Website Content Crawler actor and convert its output into LlamaIndex Document objects. The code requires an Apify API token and specifies a target URL to crawl.",
    "codeLanguage": "python",
    "codeTokens": 176,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/llama.md#2025-04-18_snippet_1",
    "pageTitle": "LlamaIndex Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "from llama_index.core import Document\nfrom llama_index.readers.apify import ApifyActor\n\nreader = ApifyActor(\"<My Apify API token>\")\n\ndocuments = reader.load_data(\n    actor_id=\"apify/website-content-crawler\",\n    run_input={\n        \"startUrls\": [{\"url\": \"https://docs.llamaindex.ai/en/latest/\"}]\n    },\n    dataset_mapping_function=lambda item: Document(\n        text=item.get(\"text\"),\n        metadata={\n            \"url\": item.get(\"url\"),\n        },\n    ),\n)"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Full Repository Scraping - Playwright Implementation",
    "codeDescription": "Complete implementation using Playwright to scrape repository data including titles, descriptions, and links from a single page.",
    "codeLanguage": "javascript",
    "codeTokens": 302,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_3",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\nimport * as cheerio from 'cheerio';\n\nconst repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;\n\nconst scrapeRepos = async (page) => {\n    const $ = cheerio.load(await page.content());\n\n    return [...$('.list-view-item')].map((item) => {\n        const repoElement = $(item);\n        return {\n            title: repoElement.find('h4').text().trim(),\n            description: repoElement.find('.repos-list-description').text().trim(),\n            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,\n        };\n    });\n};\n\nconst browser = await chromium.launch({ headless: false });\nconst firstPage = await browser.newPage();\nawait firstPage.goto(REPOSITORIES_URL);\n\nconst lastPageElement = firstPage.locator('a[aria-label*=\"Page \"]:nth-last-child(2)');\nconst lastPageLabel = await lastPageElement.getAttribute('aria-label');\nconst lastPageNumber = Number(lastPageLabel.replace(/\\D/g, ''));\n\nrepositories.push(...(await scrapeRepos(firstPage)));\n\nconsole.log(repositories);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Defining Actor Input Schema in JSON",
    "codeDescription": "This code snippet demonstrates how to define an input schema for a simple web crawler Actor. It specifies start URLs and a page function as required inputs, with prefill values and custom editors.",
    "codeLanguage": "json",
    "codeTokens": 261,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_0",
    "pageTitle": "Actor Input Schema Specification",
    "codeList": [
      {
        "language": "json5",
        "code": "{\n    \"title\": \"Cheerio Crawler input\",\n    \"description\": \"To update crawler to another site, you need to change startUrls and pageFunction options!\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"startUrls\": {\n            \"title\": \"Start URLs\",\n            \"type\": \"array\",\n            \"description\": \"URLs to start with\",\n            \"prefill\": [\n                { \"url\": \"http://example.com\" },\n                { \"url\": \"http://example.com/some-path\" }\n            ],\n            \"editor\": \"requestListSources\"\n        },\n        \"pageFunction\": {\n            \"title\": \"Page function\",\n            \"type\": \"string\",\n            \"description\": \"Function executed for each request\",\n            \"prefill\": \"async () => { return $('title').text(); }\",\n            \"editor\": \"javascript\"\n        }\n    },\n    \"required\": [\"startUrls\", \"pageFunction\"]\n}"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Configuring Multiple Datasets Resource Input for Apify Actor in JSON",
    "codeDescription": "This snippet shows how to configure a multiple datasets resource input for an Apify Actor. It uses the 'array' type and 'resourceType' property to specify multiple dataset inputs.",
    "codeLanguage": "json",
    "codeTokens": 86,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_15",
    "pageTitle": "Actor Input Schema Specification",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"title\": \"Datasets\",\n    \"type\": \"array\",\n    \"description\": \"Select multiple datasets\",\n    \"resourceType\": \"dataset\"\n}"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Setting Up Proxy Configuration in Crawlee",
    "codeDescription": "This snippet shows how to set up a ProxyConfiguration with custom proxy URLs for use in a Crawlee scraper.",
    "codeLanguage": "javascript",
    "codeTokens": 110,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/using_proxies.md#2025-04-18_snippet_1",
    "pageTitle": "Using Proxies in Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { ProxyConfiguration } from 'crawlee';\n\nconst proxyConfiguration = new ProxyConfiguration({\n    proxyUrls: ['http://45.42.177.37:3128', 'http://43.128.166.24:59394', 'http://51.79.49.178:3128'],\n});"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Implementing Function Overloads for Scrape Function in TypeScript",
    "codeDescription": "This snippet demonstrates the use of function overloads to specify different return types based on the input parameter. It ensures type safety when accessing properties of the returned data.",
    "codeLanguage": "typescript",
    "codeTokens": 223,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_7",
    "pageTitle": "TypeScript Mini-Project Documentation",
    "codeList": [
      {
        "language": "typescript",
        "code": "// index.ts\n\n// ...\n// If \"removeImages\" is true, a ModifiedProduct array will be returned\nasync function scrape(input: UserInput<true>): Promise<ModifiedProduct[]>;\n// If false, a normal product array is returned\nasync function scrape(input: UserInput<false>): Promise<Product[]>;\n// The main function declaration, which accepts all types in the declarations above.\n// Notice that it has no explicit return type, since they are defined in the\n// overloads above.\nasync function scrape(input: UserInput) {\n    const data = await fetchData();\n\n    const sorted = sortData(data.products, input.sort as SortOrder);\n\n    if (input.removeImages) {\n        return sorted.map((item) => {\n            const { images, ...rest } = item;\n\n            return rest;\n        });\n    }\n\n    return sorted;\n}"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Example jQuery Scraping Logic for Browser Console Testing",
    "codeDescription": "A practical example of scraped data collection using jQuery selectors to extract information from a list of items. This can be tested directly in the browser console before adding to a Web Scraper configuration.",
    "codeLanguage": "javascript",
    "codeTokens": 101,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/debugging_web_scraper.md#2025-04-18_snippet_3",
    "pageTitle": "Debugging Web Scraper PageFunction in Browser Console",
    "codeList": [
      {
        "language": "javascript",
        "code": "results = [];\n$('.my-list-item').each((i, el) => {\n    results.push({\n        title: $(el).find('.title').text().trim(),\n        // other fields\n    });\n});"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Extracting Product Data with Cheerio in JavaScript",
    "codeDescription": "Code snippet that demonstrates how to extract product data from a webpage using Cheerio. It loads the page content, selects product elements using CSS selectors, and maps them to structured objects with brand and price information.",
    "codeLanguage": "javascript",
    "codeTokens": 174,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_9",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import * as cheerio from 'cheerio';\n\nconst $ = cheerio.load(await page.content());\n\n// Grab the newly loaded items\nconst items = [...$('a[data-testid*=\"productTile\"]')].slice(products.length);\n\nconst newItems = items.map((item) => {\n    const elem = $(item);\n\n    return {\n        brand: elem.find('p[data-testid=\"brandName\"]').text().trim(),\n        price: elem.find('span[data-testid=\"finalPrice\"]').text().trim(),\n    };\n});\n\nproducts.push(...newItems);"
      }
    ],
    "relevance": 0.942
  },
  {
    "codeTitle": "Using Named Datasets in Python",
    "codeDescription": "Demonstrates how to open and use a named dataset in Python, which can be shared between Actors or Actor runs. It shows opening a dataset and pushing data to it.",
    "codeLanguage": "python",
    "codeTokens": 112,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_13",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        # Save a named dataset to a variable\n        dataset = await Actor.open_dataset(name='some-name')\n\n        # Add data to the named dataset\n        await dataset.push_data({'foo': 'bar'})"
      }
    ],
    "relevance": 0.942
  },
  {
    "codeTitle": "Parsing JSON Objects from HTML in JavaScript",
    "codeDescription": "This snippet demonstrates how to extract a JSON object from an HTML string by splitting the content and parsing it. It targets a specific window object named '__sc_hydration'.",
    "codeLanguage": "javascript",
    "codeTokens": 94,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/js_in_html.md#2025-04-18_snippet_0",
    "pageTitle": "Scraping Hidden JavaScript Objects in HTML",
    "codeList": [
      {
        "language": "javascript",
        "code": "const html = $.html();\n\nconst string = html.split('window.__sc_hydration = ')[1].split(';</script>')[0];\n\nconst data = JSON.parse(string);\n\nconsole.log(data);"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Generating Browser Headers with browser-headers-generator (JavaScript)",
    "codeDescription": "This code snippet shows how to use the browser-headers-generator package to create randomized browser headers. It initializes the generator with specific operating system and browser options, then generates a set of random headers.",
    "codeLanguage": "javascript",
    "codeTokens": 112,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/generating_fingerprints.md#2025-04-18_snippet_3",
    "pageTitle": "Generating Fingerprints for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import BrowserHeadersGenerator from 'browser-headers-generator';\n\nconst browserHeadersGenerator = new BrowserHeadersGenerator({\n    operatingSystems: ['windows'],\n    browsers: ['chrome'],\n});\n\nawait browserHeadersGenerator.initialize();\n\nconst randomBrowserHeaders = await browserHeadersGenerator.getRandomizedHeaders();"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Configuring Dockerfile for Apify Actor Node.js Environment",
    "codeDescription": "This Dockerfile sets up a Node.js environment for an Apify Actor, installing dependencies, copying source code, and defining the start command.",
    "codeLanguage": "dockerfile",
    "codeTokens": 155,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/source_code.md#2025-04-18_snippet_0",
    "pageTitle": "Actor Source Code Structure and Placement",
    "codeList": [
      {
        "language": "dockerfile",
        "code": "FROM apify/actor-node:20\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version \\\n    && rm -r ~/.npm\n\nCOPY . ./\n\nCMD npm start --silent"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Error Handling in Web Crawling with Node.js and Cheerio",
    "codeDescription": "This code snippet extends the previous example by adding error handling. It uses try-catch blocks to prevent the crawler from crashing when encountering errors, allowing it to continue processing other URLs.",
    "codeLanguage": "JavaScript",
    "codeTokens": 378,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/first_crawl.md#2025-04-18_snippet_1",
    "pageTitle": "Web Crawling Tutorial with Node.js, Cheerio, and HTTP Client",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    productUrls.push(absoluteUrl);\n}\n\nfor (const url of productUrls) {\n    // Everything else is exactly the same.\n    // We only wrapped the code in try/catch blocks.\n    // The try block passes all errors into the catch block.\n    // So, instead of crashing the crawler, they can be handled.\n    try {\n        // The try block attempts to execute our code\n        const productResponse = await gotScraping(url);\n        const productHtml = productResponse.body;\n        const $productPage = cheerio.load(productHtml);\n        const productPageTitle = $productPage('h1').text().trim();\n        console.log(productPageTitle);\n    } catch (error) {\n        // In the catch block, we handle errors.\n        // This time, we will print\n        // the error message and the url.\n        console.error(error.message, url);\n    }\n}"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Extracting Timezone and Date Information from BBC Weather Page",
    "codeDescription": "This code snippet extracts the timezone information and determines the first displayed date for the weather forecast from the BBC Weather page using BeautifulSoup.",
    "codeLanguage": "python",
    "codeTokens": 412,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_5",
    "pageTitle": "Python Web Scraping with Beautiful Soup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "        # When parsing the first day, find out what day it represents,\n        # to know when do the results start\n        if day_offset == 0:\n            # Get the timezone offset written in the page footer and parse it\n            tz_description = soup.find_all(class_='wr-c-footer-timezone__item')[1].text\n            tz_offset_match = re.search(r'([+-]\\d\\d)(\\d\\d)', tz_description)\n            tz_offset_hours = int(tz_offset_match.group(1))\n            tz_offset_minutes = int(tz_offset_match.group(2))\n\n            # Get the current date and time at the scraped location\n            timezone_offset = timedelta(hours=tz_offset_hours, minutes=tz_offset_minutes)\n            location_timezone = timezone(timezone_offset)\n\n            location_current_datetime = datetime.now(tz=location_timezone)\n\n            # The times displayed for each day are from 6:00 AM that day to 5:00 AM the next day,\n            # so \"today\" on BBC Weather might actually mean \"yesterday\" in actual datetime.\n            # We have to parse the accessibility label containing the actual date on the header for the first day\n            # and compare it with the current date at the location, then adjust the date accordingly\n            day_carousel_item = soup.find(class_='wr-day--active')\n            day_carousel_title = day_carousel_item.find(class_='wr-day__title')['aria-label']\n            website_first_displayed_item_day = int(re.search(r'\\d{1,2}', day_carousel_title).group(0))\n\n            if location_current_datetime.day == website_first_displayed_item_day:\n                first_displayed_date = location_current_datetime.date()\n            else:\n                first_displayed_date = location_current_datetime.date() - timedelta(days=1)"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Advanced Crawler with Link Following",
    "codeDescription": "Complete crawler implementation with link following capabilities using enqueueLinks",
    "codeLanguage": "javascript",
    "codeTokens": 159,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_5",
    "pageTitle": "Professional Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request, enqueueLinks }) => {\n        console.log('URL:', request.url);\n        console.log('Title:', $('h1').text().trim());\n\n        if (request.label === 'start-url') {\n            await enqueueLinks({\n                selector: 'a.product-item__title',\n            });\n        }\n    },\n});\n\nawait crawler.addRequests([{\n    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n    label: 'start-url',\n}]);\n\nawait crawler.run();"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Logging into Yahoo with Puppeteer",
    "codeDescription": "This code snippet shows how to log into Yahoo using Puppeteer. It follows the same login process as the Playwright example, with slight differences in syntax and navigation handling.",
    "codeLanguage": "javascript",
    "codeTokens": 293,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_1",
    "pageTitle": "Logging into a Website with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\n// Launch a browser and open a page\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.yahoo.com/');\n\n// Agree to the cookies terms, then click on the \"Sign in\" button\nawait Promise.all([page.waitForSelector('a[data-ylk*=\"sign-in\"]'), page.click('button[name=\"agree\"]')]);\nawait Promise.all([page.waitForNavigation(), page.click('a[data-ylk*=\"sign-in\"]')]);\n\n// Type in the username and continue forward\nawait page.type('input[name=\"username\"]', 'YOUR-LOGIN-HERE');\nawait Promise.all([page.waitForNavigation(), page.click('input[name=\"signin\"]')]);\n\n// Type in the password and continue forward\nawait page.type('input[name=\"password\"]', 'YOUR-PASSWORD-HERE');\nawait Promise.all([page.waitForNavigation(), page.click('button[name=\"verifyPassword\"]')]);\n\n// Wait for 10 seconds so we can see that we have in fact\n// successfully logged in\nawait page.waitForTimeout(10000);"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Defining Custom GraphQL Query for Cheddar News API",
    "codeDescription": "Creates a custom GraphQL query to fetch media data from Cheddar's API, including title, publish date, and video URL.",
    "codeLanguage": "graphql",
    "codeTokens": 120,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/custom_queries.md#2025-04-18_snippet_2",
    "pageTitle": "Custom GraphQL Queries for API Scraping",
    "codeList": [
      {
        "language": "graphql",
        "code": "query SearchQuery($query: String!, $max_age: Int!) {\n  organization {\n    media(query: $query, max_age: $max_age , first: 1000) {\n      edges {\n        node {\n          title\n          public_at\n          hero_video {\n            video_urls {\n              url\n            }\n          }\n        }\n      }\n    }\n  }\n}"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Extracting Product Details from E-commerce Page in JavaScript",
    "codeDescription": "Demonstrates how to extract various product details such as title, vendor, price, review count, and description from an e-commerce product page using cheerio selectors.",
    "codeLanguage": "JavaScript",
    "codeTokens": 229,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_1",
    "pageTitle": "Web Scraping Tutorial: Crawling and Data Extraction",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst productUrl = 'https://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones';\nconst response = await gotScraping(productUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst title = $('h1').text().trim();\nconst vendor = $('a.product-meta__vendor').text().trim();\nconst price = $('span.price').contents()[2].nodeValue;\nconst reviewCount = parseInt($('span.rating__caption').text(), 10);\nconst description = $('div[class*=\"description\"] div.rte').text().trim();\n\nconst product = {\n    title,\n    vendor,\n    price,\n    reviewCount,\n    description,\n};\n\nconsole.log(product);"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Importing Required Packages for Weather Data Processing",
    "codeDescription": "Imports necessary Python packages including io, os, ApifyClient, ActorJobStatus, and pandas for data processing and API interaction.",
    "codeLanguage": "python",
    "codeTokens": 77,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_9",
    "pageTitle": "Python Web Scraping with Beautiful Soup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "from io import BytesIO\nimport os\n\nfrom apify_client import ApifyClient\nfrom apify_client.consts import ActorJobStatus\nimport pandas"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Identifying Nested iFrames with Puppeteer",
    "codeDescription": "This code snippet shows how to identify nested iFrames by searching for specific elements within child frames. It demonstrates a more advanced technique for locating the desired iFrame containing a tweet list.",
    "codeLanguage": "JavaScript",
    "codeTokens": 146,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/scraping_iframes.md#2025-04-18_snippet_1",
    "pageTitle": "Scraping iFrames with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "let twitterFrame;\n\nfor (const frame of page.mainFrame().childFrames()) {\n    if (frame.url().includes('twitter')) {\n        for (const nestedFrame of frame.childFrames()) {\n            const tweetList = await nestedFrame.$('.timeline-TweetList');\n            if (tweetList) {\n                console.log('We found the frame with tweet list');\n                twitterFrame = nestedFrame;\n            }\n        }\n    }\n}"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Optimizing Dockerfile for Apify Actor with Node.js",
    "codeDescription": "This Dockerfile demonstrates best practices for building an Apify Actor image. It leverages layer caching by first copying and installing dependencies, then copying the rest of the source code. This approach speeds up builds when only the source code changes.",
    "codeLanguage": "dockerfile",
    "codeTokens": 175,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/performance.md#2025-04-18_snippet_0",
    "pageTitle": "Performance Optimization Guide for Apify Actors",
    "codeList": [
      {
        "language": "dockerfile",
        "code": "FROM apify/actor-node:16\n\nCOPY package*.json ./\n\nRUN npm --quiet set progress=false \\\n    && npm install --omit=dev --omit=optional \\\n    && echo \"Installed NPM packages:\" \\\n    && (npm list --omit=dev --all || true) \\\n    && echo \"Node.js version:\" \\\n    && node --version \\\n    && echo \"NPM version:\" \\\n    && npm --version \\\n    && rm -r ~/.npm\n\nCOPY . ./\n\nCMD npm start --silent"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Environment Variable Configuration",
    "codeDescription": "Extracts required environment variables for container port, URL, and key-value store identification.",
    "codeLanguage": "javascript",
    "codeTokens": 64,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_1",
    "pageTitle": "Running Web Server in Apify Actor",
    "codeList": [
      {
        "language": "javascript",
        "code": "const {\n    APIFY_CONTAINER_PORT,\n    APIFY_CONTAINER_URL,\n    APIFY_DEFAULT_KEY_VALUE_STORE_ID,\n} = process.env;"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Extracting URLs from Sitemaps using Crawlee in JavaScript",
    "codeDescription": "This code snippet demonstrates how to use the Crawlee library to find a website's robots.txt file, parse it for sitemaps, and extract all URLs from those sitemaps. It simplifies the process of crawling sitemaps by handling nested sitemaps, compressed files, and URL extraction.",
    "codeLanguage": "javascript",
    "codeTokens": 123,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/crawling/crawling-sitemaps.md#2025-04-18_snippet_0",
    "pageTitle": "Crawling Sitemaps in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { RobotsFile } from 'crawlee';\n\nconst robots = await RobotsFile.find('https://www.mysite.com');\n\nconst allWebsiteUrls = await robots.parseUrlsFromSitemaps();"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Managing Page Handling and Error Recovery in PuppeteerCrawler",
    "codeDescription": "Implements the page handling logic with session management, including error handling and browser instance retirement on failures. Successfully processed sessions are stored while failed ones are removed.",
    "codeLanguage": "javascript",
    "codeTokens": 154,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/filter_blocked_requests_using_sessions.md#2025-04-18_snippet_6",
    "pageTitle": "Filtering Blocked Proxies with Sessions in Node.js",
    "codeList": [
      {
        "language": "javascript",
        "code": "const handlePageFunction = async ({ request, page, puppeteerPool }) => {\n    const { session } = request.userData;\n    console.log(`URL: ${request.url}, session: ${session.name}, userAgent: ${session.userAgent}`);\n\n    try {\n        // your main logic that is executed on each page\n        sessions[session.name] = session;\n    } catch (e) {\n        delete sessions[session.name];\n        await puppeteerPool.retire(page.browser());\n        throw e;\n    }\n};"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Emulating Geolocation with Playwright",
    "codeDescription": "Using Playwright's browserContext.setGeolocation() method to emulate geolocation settings. Should be combined with appropriate proxy settings matching the emulated location.",
    "codeLanguage": "javascript",
    "codeTokens": 54,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/geolocation.md#2025-04-18_snippet_1",
    "pageTitle": "Geolocation in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "browserContext.setGeolocation()"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Persisting State in ASINTracker with Apify SDK in JavaScript",
    "codeDescription": "Updates the ASINTracker class to persist state using the Apify SDK. It listens for the 'persistState' event and stores the state in the key-value store.",
    "codeLanguage": "JavaScript",
    "codeTokens": 215,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/handling_migrations.md#2025-04-18_snippet_2",
    "pageTitle": "Handling Migrations in Apify Actors",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "// asinTracker.js\nimport { Actor } from 'apify';\n// We've updated our constants.js file to include the name\n// of this new key in the key-value store\nconst { ASIN_TRACKER } = require('./constants');\n\nclass ASINTracker {\n    constructor() {\n        this.state = {};\n\n        Actor.on('persistState', async () => {\n            await Actor.setValue(ASIN_TRACKER, this.state);\n        });\n\n        setInterval(() => console.log(this.state), 10000);\n    }\n\n    incrementASIN(asin) {\n        if (this.state[asin] === undefined) {\n            this.state[asin] = 0;\n            return;\n        }\n\n        this.state[asin] += 1;\n    }\n}\n\nmodule.exports = new ASINTracker();"
      }
    ],
    "relevance": 0.938
  },
  {
    "codeTitle": "Scraping F1 News Dates from The Guardian",
    "codeDescription": "Script that scrapes Formula 1 news articles from The Guardian website using BeautifulSoup. Extracts article titles and publication dates from HTML time tags, parsing ISO 8601 datetime strings.",
    "codeLanguage": "python",
    "codeTokens": 68,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/07_extracting_data.md#2025-04-18_snippet_6",
    "pageTitle": "Data Extraction and Price Scraping with Python",
    "codeList": [
      {
        "language": "text",
        "code": "https://www.theguardian.com/sport/formulaone"
      },
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\n\nurl = \"https://www.theguardian.com/sport/formulaone\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\n\nfor article in soup.select(\"#maincontent ul li\"):\n    title = article.select_one(\"h3\").text.strip()\n\n    time_iso = article.select_one(\"time\")[\"datetime\"].strip()\n    published_at = datetime.fromisoformat(time_iso)\n    published_on = published_at.date()\n\n    print(title, published_on)"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Opening Datasets from Other Runs with Apify SDK in Python",
    "codeDescription": "This code shows how to open a dataset from another run using the Apify SDK in Python. The Actor.open_dataset() method allows accessing a dataset by name within an async context.",
    "codeLanguage": "Python",
    "codeTokens": 89,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_21",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "Python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        other_dataset = await Actor.open_dataset(name='old-dataset')\n        # ..."
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Frontend Pagination Implementation for Web and Puppeteer Scrapers",
    "codeDescription": "Code examples showing how to handle frontend pagination in both Web Scraper and Puppeteer Scraper. Demonstrates clicking the next pagination button to load more content dynamically.",
    "codeLanguage": "javascript",
    "codeTokens": 84,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_6",
    "pageTitle": "Choosing Between Web Scraper and Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Web Scraper\\\n$('li a span.pagination-next').click();\n\n// Puppeteer Scraper\\\nawait page.click('li a span.pagination-next');"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Implementing Wait Functions in JavaScript Scraper",
    "codeDescription": "Examples of using the waitFor() function to handle different waiting scenarios in web scraping, including time-based waits, selector-based waits, and condition-based waits.",
    "codeLanguage": "javascript",
    "codeTokens": 115,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_6",
    "pageTitle": "Scraping with Web Scraper Tutorial",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Waits for 2 seconds.\nawait waitFor(2000);\n// Waits until an element with id \"my-id\" appears\n// in the page.\nawait waitFor('#my-id');\n// Waits until a \"myObject\" variable appears\n// on the window object.\nawait waitFor(() => !!window.myObject);"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Emulating Devices with Multiple Browser Contexts in Playwright",
    "codeDescription": "Demonstrates how to create multiple browser contexts in Playwright, each emulating a different device (iPhone and Android) using playwright.devices.",
    "codeLanguage": "JavaScript",
    "codeTokens": 200,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_3",
    "pageTitle": "Creating Multiple Browser Contexts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { chromium, devices } from 'playwright';\n\n// Launch the browser\nconst browser = await chromium.launch({ headless: false });\n\nconst iPhone = devices['iPhone 11 Pro'];\n// Create a new context for our iPhone emulation\nconst iPhoneContext = await browser.newContext({ ...iPhone });\n// Open a page on the newly created iPhone context\nconst iPhonePage = await iPhoneContext.newPage();\n\nconst android = devices['Galaxy Note 3'];\n// Create a new context for our Android emulation\nconst androidContext = await browser.newContext({ ...android });\n// Open a page on the newly created Android context\nconst androidPage = await androidContext.newPage();\n\n// The code in the next step will go here\n\nawait browser.close();"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Python Web Scraper for F1 Articles",
    "codeDescription": "Python script that scrapes F1 news articles from The Guardian website, extracting author names and titles using httpx for HTTP requests and BeautifulSoup for HTML parsing. The script handles both individual authors and news agency contributors.",
    "codeLanguage": "python",
    "codeTokens": 264,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/10_crawling.md#2025-04-18_snippet_4",
    "pageTitle": "Building Web Crawlers with Python and HTTPX",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n    return BeautifulSoup(response.text, \"html.parser\")\n\ndef parse_author(article_soup):\n    link = article_soup.select_one('aside a[rel=\"author\"]')\n    if link:\n        return link.text.strip()\n    address = article_soup.select_one('aside address')\n    if address:\n        return address.text.strip()\n    return None\n\nlisting_url = \"https://www.theguardian.com/sport/formulaone\"\nlisting_soup = download(listing_url)\nfor item in listing_soup.select(\"#maincontent ul li\"):\n    link = item.select_one(\"a\")\n    article_url = urljoin(listing_url, link[\"href\"])\n    article_soup = download(article_url)\n    title = article_soup.select_one(\"h1\").text.strip()\n    author = parse_author(article_soup)\n    print(f\"{author}: {title}\")"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Complete Email Sending Automation with Playwright",
    "codeDescription": "This code snippet presents the complete implementation of email sending automation using Playwright. It includes logging into Yahoo, storing cookies, and sending multiple emails concurrently.",
    "codeLanguage": "JavaScript",
    "codeTokens": 483,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/logging_into_a_website.md#2025-04-18_snippet_9",
    "pageTitle": "Logging into a Website with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst emailsToSend = [\n    {\n        to: 'alice@example.com',\n        subject: 'Hello',\n        body: 'This is a message.',\n    },\n    {\n        to: 'bob@example.com',\n        subject: 'Testing',\n        body: 'I love the academy!',\n    },\n    {\n        to: 'carol@example.com',\n        subject: 'Apify is awesome!',\n        body: 'Some content.',\n    },\n];\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Login logic\nawait page.goto('https://www.yahoo.com/');\n\nawait page.click('button[name=\"agree\"]');\nawait page.waitForSelector('a:has-text(\"Sign in\")');\n\nawait page.click('a:has-text(\"Sign in\")');\nawait page.waitForLoadState('load');\n\nawait page.type('input[name=\"username\"]', 'YOUR-LOGIN-HERE');\nawait page.click('input[name=\"signin\"]');\n\nawait page.type('input[name=\"password\"]', 'YOUR-PASSWORD-HERE');\nawait page.click('button[name=\"verifyPassword\"]');\nawait page.waitForLoadState('load');\n\nconst cookies = await browser.contexts()[0].cookies();\n\nawait page.close();\n\n// Email sending logic\nconst promises = emailsToSend.map(({ to, subject, body }) => (async () => {\n    const sendEmailContext = await browser.newContext();\n    await sendEmailContext.addCookies(cookies);\n    const page2 = await sendEmailContext.newPage();\n\n    await page2.goto('https://mail.yahoo.com/');\n\n    await page2.click('a[aria-label=\"Compose\"]');\n\n    await page2.type('input#message-to-field', to);\n    await page2.type('input[data-test-id=\"compose-subject\"]', subject);\n    await page2.type('div[data-test-id=\"compose-editor-container\"] div[contenteditable=\"true\"]', body);\n\n    await page2.click('button[title=\"Send this email\"]');\n\n    await sendEmailContext.close();\n})(),\n);\n\nawait Promise.all(promises);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Implementing Metamorph in Python",
    "codeDescription": "Python implementation of a hotel review scraper using metamorph to transform into web-scraper Actor. Demonstrates async context management, input handling, and metamorph operation configuration.",
    "codeLanguage": "python",
    "codeTokens": 219,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/metamorph.md#2025-04-18_snippet_1",
    "pageTitle": "Actor Metamorph Operation Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        # Get input of your Actor\n        actor_input = await Actor.get_input() or {}\n\n        # Create input for apify/web-scraper\n        new_input = {\n            'startUrls': [{'url': actor_input['url']}],\n            'pageFunction': \"\"\"\n                # Here you pass the page function that\n                # scrapes all the reviews ...\n            \"\"\",\n            # ... and here would be all the additional input parameters\n        }\n\n        # Transform the Actor run to apify/web-scraper with the new input\n        await Actor.metamorph('apify/web-scraper', new_input)\n\n        # The line here will never be reached, because the Actor run will be interrupted\n        Actor.log.info('This should not be printed')"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Creating Result Objects for Zappos Scraper in JavaScript",
    "codeDescription": "JavaScript code showing how to construct the result objects that match the dataset schema. This example demonstrates scraping product data from Zappos and structuring it to work with the defined schema.",
    "codeLanguage": "js",
    "codeTokens": 176,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/output_schema.md#2025-04-18_snippet_2",
    "pageTitle": "Dataset Schema Implementation for Apify Actors",
    "codeList": [
      {
        "language": "js",
        "code": "const results = {\n    url: request.loadedUrl,\n    imgUrl: $('#stage button[data-media=\"image\"] img[itemprop=\"image\"]').attr('src'),\n    brand: $('span[itemprop=\"brand\"]').text().trim(),\n    name: $('meta[itemprop=\"name\"]').attr('content'),\n    SKU: $('*[itemprop~=\"sku\"]').text().trim(),\n    inStock: !request.url.includes('oosRedirected=true'),\n    onSale: !$('div[itemprop=\"offers\"]').text().includes('OFF'),\n    price: $('span[itemprop=\"price\"]').text(),\n};"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Blocking Resources with Puppeteer",
    "codeDescription": "Shows how to use Puppeteer to launch a browser, create a new page, enable request interception, and block requests for specific file types.",
    "codeLanguage": "javascript",
    "codeTokens": 219,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_6",
    "pageTitle": "Reading and Intercepting Network Requests with Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nconst blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];\n\n// Enable request interception (skipping this step will result in an error)\nawait page.setRequestInterception(true);\n\n// Listen for all requests\npage.on('request', async (req) => {\n    // If the request ends in a blocked extension, abort the request\n    if (blockedExtensions.some((str) => req.url().endsWith(str))) return req.abort();\n    // Otherwise, continue\n    await req.continue();\n});\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Defining Input Schema in JSON5 for Website Content Crawler Actor",
    "codeDescription": "This code snippet shows how to define an input schema for the Website Content Crawler Actor using JSON5. It includes various field types and configurations that the Apify platform supports for generating the input UI.",
    "codeLanguage": "json5",
    "codeTokens": 697,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/index.md#2025-04-18_snippet_0",
    "pageTitle": "Defining Input Schema for Apify Actors",
    "codeList": [
      {
        "language": "json5",
        "code": "{\n    \"title\": \"Input schema for Website Content Crawler\",\n    \"description\": \"Enter the start URL(s) of the website(s) to crawl, configure other optional settings, and run the Actor to crawl the pages and extract their text content.\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"startUrls\": {\n            \"title\": \"Start URLs\",\n            \"type\": \"array\",\n            \"description\": \"One or more URLs of the pages where the crawler will start. Note that the Actor will additionally only crawl sub-pages of these URLs. For example, for the start URL `https://www.example.com/blog`, it will crawl pages like `https://example.com/blog/article-1`, but will skip `https://example.com/docs/something-else`.\",\n            \"editor\": \"requestListSources\",\n            \"prefill\": [{ \"url\": \"https://docs.apify.com/\" }]\n        },\n        \"crawlerType\": {\n            \"sectionCaption\": \"Crawler settings\",\n            \"title\": \"Crawler type\",\n            \"type\": \"string\",\n            \"enum\": [\"playwright:chrome\", \"cheerio\", \"jsdom\"],\n            \"enumTitles\": [\"Headless web browser (Chrome+Playwright)\", \"Raw HTTP client (Cheerio)\", \"Raw HTTP client with JS execution (JSDOM) (experimental!)\"],\n            \"description\": \"Select the crawling engine:\\n- **Headless web browser** (default) - Useful for modern websites with anti-scraping protections and JavaScript rendering. It recognizes common blocking patterns like CAPTCHAs and automatically retries blocked requests through new sessions. However, running web browsers is more expensive as it requires more computing resources and is slower. It is recommended to use at least 8 GB of RAM.\\n- **Raw HTTP client** - High-performance crawling mode that uses raw HTTP requests to fetch the pages. It is faster and cheaper, but it might not work on all websites.\",\n            \"default\": \"playwright:chrome\"\n        },\n        \"maxCrawlDepth\": {\n            \"title\": \"Max crawling depth\",\n            \"type\": \"integer\",\n            \"description\": \"The maximum number of links starting from the start URL that the crawler will recursively descend. The start URLs have a depth of 0, the pages linked directly from the start URLs have a depth of 1, and so on.\\n\\nThis setting is useful to prevent accidental crawler runaway. By setting it to 0, the Actor will only crawl start URLs.\",\n            \"minimum\": 0,\n            \"default\": 20\n        },\n        \"maxCrawlPages\": {\n            \"title\": \"Max pages\",\n            \"type\": \"integer\",\n            \"description\": \"The maximum number pages to crawl. It includes the start URLs, pagination pages, pages with no content, etc. The crawler will automatically finish after reaching this number. This setting is useful to prevent accidental crawler runaway.\",\n            \"minimum\": 0,\n            \"default\": 9999999\n        }\n    }\n}"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "CSV Export Function",
    "codeDescription": "Function that exports data to a CSV file, automatically determining field names from the data structure.",
    "codeLanguage": "python",
    "codeTokens": 85,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_3",
    "pageTitle": "Python Web Scraping with BeautifulSoup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "def export_csv(file, data):\n    fieldnames = list(data[0].keys())\n    writer = csv.DictWriter(file, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Complete Filter Actor Implementation",
    "codeDescription": "Complete implementation of the filter Actor including initialization, data processing, and result storage.",
    "codeLanguage": "javascript",
    "codeTokens": 168,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/integrating_webhooks.md#2025-04-18_snippet_3",
    "pageTitle": "Integrating Webhooks with Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n\nconst { datasetId } = await Actor.getInput();\nconst dataset = await Actor.openDataset(datasetId);\n\nconst { items } = await dataset.getData();\n\nconst filtered = items.reduce((acc, curr) => {\n    const prevPrice = acc?.[curr.asin] ? +acc[curr.asin].offer.slice(1) : null;\n    const price = +curr.offer.slice(1);\n\n    if (!acc[curr.asin] || prevPrice > price) acc[curr.asin] = curr;\n\n    return acc;\n}, {});\n\nawait Actor.pushData(Object.values(filtered));\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Configuring MITM Proxy to Block or Allow Requests in JavaScript",
    "codeDescription": "This code demonstrates how to configure the proxy to handle intercepted requests. It sets up an onRequest handler that can block requests based on a condition variable, logging blocked requests and returning custom content.",
    "codeLanguage": "javascript",
    "codeTokens": 162,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/using_proxy_to_intercept_requests_puppeteer.md#2025-04-18_snippet_1",
    "pageTitle": "Using man-in-the-middle proxy to intercept requests in Puppeteer",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Setup blocking of requests in proxy\nconst proxyPort = 8000;\nconst proxy = setupProxy(proxyPort);\nproxy.onRequest((context, callback) => {\n    if (blockRequests) {\n        const request = context.clientToProxyRequest;\n        // Log out blocked requests\n        console.log('Blocked request:', request.headers.host, request.url);\n\n        // Close the connection with custom content\n        context.proxyToClientResponse.end('Blocked');\n        return;\n    }\n    return callback();\n});"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Configuring Markdown Linter for Exercise Section",
    "codeDescription": "This snippet disables a specific markdown linting rule for the next line, likely to allow a non-standard markdown structure for the exercise section.",
    "codeLanguage": "markdown",
    "codeTokens": 54,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/_exercises.mdx#2025-04-18_snippet_0",
    "pageTitle": "Exercises for Web Scraping Lesson",
    "codeList": [
      {
        "language": "markdown",
        "code": "<!-- markdownlint-disable-next-line MD041 -->"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Constructing a US-only Proxy URL",
    "codeDescription": "This snippet demonstrates how to construct a proxy URL that selects proxies only from the United States. It's used to restrict the geographical location of the proxy servers used in web scraping tasks.",
    "codeLanguage": "plaintext",
    "codeTokens": 73,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/bypassing_anti_scraping.md#2025-04-18_snippet_0",
    "pageTitle": "Bypassing Anti-Scraping Methods with Crawlee and Apify SDK",
    "codeList": [
      {
        "language": "plaintext",
        "code": "http://proxy.apify.com:8000"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Basic Playwright Setup for Web Scraping",
    "codeDescription": "Initial setup code for launching Playwright browser and navigating to target page. Creates a new browser instance and page object, then navigates to a demo webstore.",
    "codeLanguage": "javascript",
    "codeTokens": 116,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_0",
    "pageTitle": "Data Extraction Guide for Playwright and Puppeteer",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://demo-webstore.apify.org/search/on-sale');\n\n// code will go here\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Opening Request Queues in JavaScript SDK",
    "codeDescription": "Demonstrates how to open default and named request queues using the Apify JavaScript SDK. It shows the usage of Actor.openRequestQueue() method for managing request queues in a JavaScript Actor.",
    "codeLanguage": "javascript",
    "codeTokens": 135,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_4",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Import the JavaScript SDK into your project\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\n// Open the default request queue associated with\n// the Actor run\nconst queue = await Actor.openRequestQueue();\n\n// Open the 'my-queue' request queue\nconst queueWithName = await Actor.openRequestQueue('my-queue');\n\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Setting Session Rotation Parameters in Crawlee",
    "codeDescription": "Configures a SessionPool to discard sessions after 5 uses and to trash sessions that receive errors. This helps maintain clean sessions for scraping.",
    "codeLanguage": "javascript",
    "codeTokens": 120,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/rotating_proxies.md#2025-04-18_snippet_2",
    "pageTitle": "Rotating Proxies and Sessions for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "const crawler = new CheerioCrawler({\n    requestList,\n    requestQueue,\n    proxyConfiguration,\n    useSessionPool: true,\n    sessionPoolOptions: {\n        sessionOptions: {\n            maxUsageCount: 5,\n            maxErrorScore: 1,\n        },\n    },\n    maxConcurrency: 50,\n    // ...\n});"
      }
    ],
    "relevance": 0.932
  },
  {
    "codeTitle": "Clicking an Element with Playwright",
    "codeDescription": "Demonstrates how to click the 'Accept all' button on Google's cookie policy using Playwright's text-based selector.",
    "codeLanguage": "JavaScript",
    "codeTokens": 69,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/interacting_with_a_page.md#2025-04-18_snippet_0",
    "pageTitle": "Interacting with a Page using Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Click the \"Accept all\" button\nawait page.click('button:has-text(\"Accept all\")');"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Data Sorting Implementation",
    "codeDescription": "Implementation of product data sorting function with TypeScript type safety and handling of sort orders.",
    "codeLanguage": "typescript",
    "codeTokens": 115,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_4",
    "pageTitle": "TypeScript Mini-Project Documentation",
    "codeList": [
      {
        "language": "typescript",
        "code": "const sortData = (products: Product[], order: SortOrder) => {\n    switch (order) {\n        case SortOrder.ASC:\n            return [...products].sort((a, b) => a.price - b.price);\n        case SortOrder.DESC:\n            return [...products].sort((a, b) => b.price - a.price);\n        default:\n            return products;\n    }\n};"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Defining Proxy Configuration Structure in JSON5",
    "codeDescription": "This snippet illustrates the structure of the proxy configuration object. It includes properties for useApifyProxy, apifyProxyGroups, and proxyUrls.",
    "codeLanguage": "json5",
    "codeTokens": 140,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_9",
    "pageTitle": "Actor Input Schema Specification",
    "codeList": [
      {
        "language": "json5",
        "code": "{\n    // Indicates whether Apify Proxy was selected.\n    \"useApifyProxy\": Boolean,\n\n    // Array of Apify Proxy groups. Is missing or null if\n    // Apify Proxy's automatic mode was selected\n    // or if proxies are not used.\n    \"apifyProxyGroups\": String[],\n\n    // Array of custom proxy URLs.\n    // Is missing or null if custom proxies were not used.\n    \"proxyUrls\": String[],\n}"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Using PuppeteerCrawler with Apify Proxy in JavaScript",
    "codeDescription": "This snippet demonstrates how to use PuppeteerCrawler with Apify Proxy configuration. It initializes the Actor, creates a proxy configuration, and sets up a crawler to visit a URL and log the page content.",
    "codeLanguage": "javascript",
    "codeTokens": 145,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_0",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    async requestHandler({ page }) {\n        console.log(await page.content());\n    },\n});\n\nawait crawler.run(['https://proxy.apify.com/?format=json']);\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Extracting URLs from Sitemaps with Crawlee's RobotsFile",
    "codeDescription": "This snippet demonstrates how to automatically extract all URLs from a website's sitemaps using Crawlee's RobotsFile class.",
    "codeLanguage": "JavaScript",
    "codeTokens": 87,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_from_sitemaps.md#2025-04-18_snippet_0",
    "pageTitle": "Scraping Data from Sitemaps Using Crawlee",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { RobotsFile } from 'crawlee';\n\nconst robots = await RobotsFile.find('https://www.mysite.com');\n\nconst allWebsiteUrls = await robots.parseUrlsFromSitemaps();"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Running the LangGraph Agent with Apify Integration",
    "codeDescription": "Executes the agent with a query to search for an OpenAI TikTok profile and analyze it, streaming the results as they become available. The agent will use the configured tools to perform web searches and extract TikTok data.",
    "codeLanguage": "python",
    "codeTokens": 118,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_5",
    "pageTitle": "LangGraph Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "for state in agent_executor.stream(\n    stream_mode=\"values\",\n    input={\n        \"messages\": [\n            HumanMessage(content=\"Search the web for OpenAI TikTok profile and analyze their profile.\")\n        ]\n    }):\n    state[\"messages\"][-1].pretty_print()"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Parsing Product Data with URL Handling in Python",
    "codeDescription": "A function that extracts product information from HTML elements, including properly joining relative URLs with a base URL. It returns a dictionary with product title, price information, and full URL.",
    "codeLanguage": "python",
    "codeTokens": 122,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_6",
    "pageTitle": "Python Web Scraping with BeautifulSoup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "def parse_product(product, base_url):\n    title_element = product.select_one(\".product-item__title\")\n    title = title_element.text.strip()\n    url = urljoin(base_url, title_element[\"href\"])\n\n    ...\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price, \"url\": url}"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Initializing ApifyClient and Running Weather Scraper Actor",
    "codeDescription": "Sets up the ApifyClient, runs the weather scraper Actor, and checks for successful completion before accessing the dataset.",
    "codeLanguage": "python",
    "codeTokens": 175,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_10",
    "pageTitle": "Python Web Scraping with Beautiful Soup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "# Initialize the main ApifyClient instance\nclient = ApifyClient(os.environ['APIFY_TOKEN'], api_url=os.environ['APIFY_API_BASE_URL'])\n\n# Run the weather scraper and wait for it to finish\nprint('Downloading the weather data...')\nscraper_run = client.actor('~bbc-weather-scraper').call()\n\n# Check if the scraper finished successfully, otherwise raise an error\nif scraper_run['status'] != ActorJobStatus.SUCCEEDED:\n    raise RuntimeError('The weather scraper run has failed')\n\n# Get the resource sub-client for working with the dataset with the source data\ndataset_client = client.dataset(scraper_run['defaultDatasetId'])"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Handling Validation Errors in JavaScript",
    "codeDescription": "Shows how to catch and handle dataset validation errors in JavaScript using try-catch blocks with the Apify SDK.",
    "codeLanguage": "javascript",
    "codeTokens": 97,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/validation.md#2025-04-18_snippet_1",
    "pageTitle": "Dataset Schema Validation in Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "try {\n    const response = await Actor.pushData(items);\n} catch (error) {\n    if (!error.data?.invalidItems) throw error;\n    error.data.invalidItems.forEach((item) => {\n        const { itemPosition, validationErrors } = item;\n    });\n}"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Complete Dockerfile for Python Apify Actor",
    "codeDescription": "A complete Dockerfile for a Python Actor. It uses the Apify Python base image, installs dependencies from requirements.txt, and copies the source code. It includes a CMD instruction to specify how to run the Actor.",
    "codeLanguage": "Dockerfile",
    "codeTokens": 296,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/docker_file.md#2025-04-18_snippet_2",
    "pageTitle": "Writing Dockerfiles for Apify Actors",
    "codeList": [
      {
        "language": "Dockerfile",
        "code": "# First, specify the base Docker image.\n# You can also use any other image from Docker Hub.\nFROM apify/actor-python:3.9\n\n# Second, copy just requirements.txt into the Actor image,\n# since it should be the only file that affects \"pip install\" in the next step,\n# in order to speed up the build\nCOPY requirements.txt ./\n\n# Install the packages specified in requirements.txt,\n# Print the installed Python version, pip version\n# and all installed packages with their versions for debugging\nRUN echo \"Python version:\" \\\n && python --version \\\n && echo \"Pip version:\" \\\n && pip --version \\\n && echo \"Installing dependencies from requirements.txt:\" \\\n && pip install -r requirements.txt \\\n && echo \"All installed Python packages:\" \\\n && pip freeze\n\n# Next, copy the remaining files and directories with the source code.\n# Since we do this after installing the dependencies, quick build will be really fast\n# for most source file changes.\nCOPY . ./\n\n# Specify how to launch the source code of your Actor.\n# By default, the main.py file is run\nCMD python3 main.py"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Handling Dynamic Content and Pagination in Puppeteer for Apify Store Scraping",
    "codeDescription": "This code snippet demonstrates how to handle dynamic content and implement pagination when scraping the Apify Store. It uses Puppeteer's waitFor and click methods to interact with the 'Show more' button for loading additional actors.",
    "codeLanguage": "javascript",
    "codeTokens": 167,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_5",
    "pageTitle": "Scraping with Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Waits for 2 seconds.\nawait page.waitFor(2000);\n// Waits until an element with id \"my-id\" appears in the page.\nawait page.waitFor('#my-id');\n// Waits until a \"myObject\" variable appears\n// on the window object.\nawait page.waitFor(() => !!window.myObject);\n\n// Wait for the 'Show more' button\nawait page.waitFor('div.show-more > button');\n\n// Click the 'Show more' button\nawait page.click('div.show-more > button');"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Basic jQuery Injection in Page Function",
    "codeDescription": "Simple example of injecting jQuery into a page using Apify's utility function within a page function.",
    "codeLanguage": "javascript",
    "codeTokens": 75,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_9",
    "pageTitle": "Scraping with Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { Apify, page } = context;\n    await Apify.utils.puppeteer.injectJQuery(page);\n\n    // your code ...\n}"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Refactoring Page Function for Better Organization in JavaScript",
    "codeDescription": "This code snippet demonstrates how to refactor the page function for better organization and maintainability. It separates the logic for handling different page types into separate functions.",
    "codeLanguage": "JavaScript",
    "codeTokens": 399,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_8",
    "pageTitle": "Cheerio Scraper Tutorial Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    switch (context.request.userData.label) {\n        case 'START': return handleStart(context);\n        case 'DETAIL': return handleDetail(context);\n        default: throw new Error('Unknown request label.');\n    }\n\n    async function handleStart({ log, waitFor, $ }) {\n        log.info('Store opened!');\n\n        const dataJson = $('#__NEXT_DATA__').html();\n        // We requested HTML, but the data are actually JSON.\n        const data = JSON.parse(dataJson);\n\n        for (const item of data.props.pageProps.items) {\n            const { name, username } = item;\n            const actorDetailUrl = `https://apify.com/${username}/${name}`;\n            await context.enqueueRequest({\n                url: actorDetailUrl,\n                userData: {\n                    label: 'DETAIL',\n                },\n            });\n        }\n    }\n\n    async function handleDetail({ request, log, skipLinks, $ }) {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        return {\n            url,\n            uniqueIdentifier,\n            title: $('header h1').text(),\n            description: $('header span.actor-description').text(),\n            modifiedDate: new Date(\n                Number(\n                    $('ul.ActorHeader-stats time').attr('datetime'),\n                ),\n            ),\n            runCount: Number(\n                $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                    .text()\n                    .match(/[\\d,]+/)[0]\n                    .replace(/,/g, ''),\n            ),\n        };\n    }\n}"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Event-Driven Flow Examples in JavaScript",
    "codeDescription": "Demonstrates proper event-driven programming patterns using Puppeteer/Playwright, showing both good and bad practices for handling asynchronous operations.",
    "codeLanguage": "javascript",
    "codeTokens": 128,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/advanced_web_scraping/tips_and_tricks_robustness.md#2025-04-18_snippet_4",
    "pageTitle": "Web Scraping and Automation Robustness Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Avoid:\nawait page.waitForTimeout(timeout);\n\n// Good:\nawait page.waitForFunction(myFunction, options, args);\n\n// Good:\nawait page.waitForFunction(() => {\n    return window.location.href.includes('path');\n});\n\n// Good:\nawait page.waitForFunction(\n    (selector) => document.querySelector(selector).innerText,\n    { polling: 'mutation' },\n    '[data-qa=\"btnAppleSignUp\"]',\n);"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Emulating Devices with Multiple Browser Contexts in Puppeteer",
    "codeDescription": "Shows how to create multiple browser contexts in Puppeteer, each emulating a different device (iPhone and Android) using puppeteer.devices.",
    "codeLanguage": "JavaScript",
    "codeTokens": 230,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_4",
    "pageTitle": "Creating Multiple Browser Contexts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import puppeteer from 'puppeteer';\n\n// Launch the browser\nconst browser = await puppeteer.launch({ headless: false });\n\nconst iPhone = puppeteer.devices['iPhone 11 Pro'];\n// Create a new context for our iPhone emulation\nconst iPhoneContext = await browser.createIncognitoBrowserContext();\n// Open a page on the newly created iPhone context\nconst iPhonePage = await iPhoneContext.newPage();\n// Emulate the device\nawait iPhonePage.emulate(iPhone);\n\nconst android = puppeteer.devices['Galaxy Note 3'];\n// Create a new context for our Android emulation\nconst androidContext = await browser.createIncognitoBrowserContext();\n// Open a page on the newly created Android context\nconst androidPage = await androidContext.newPage();\n// Emulate the device\nawait androidPage.emulate(android);\n\n// The code in the next step will go here\n\nawait browser.close();"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Waiting for Download Completion in Puppeteer",
    "codeDescription": "This snippet demonstrates a simple wait operation to allow time for the file download to complete. In real scenarios, you would check the file system for the download status.",
    "codeLanguage": "javascript",
    "codeTokens": 60,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_2",
    "pageTitle": "Downloading Files with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "await page.waitFor(60000);"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Injecting Fingerprints into Playwright Browser (JavaScript)",
    "codeDescription": "This snippet demonstrates how to generate a fingerprint, create a Playwright browser context with the generated fingerprint, and inject the fingerprint into the context. It uses both the fingerprint-generator and fingerprint-injector packages.",
    "codeLanguage": "javascript",
    "codeTokens": 306,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/generating_fingerprints.md#2025-04-18_snippet_2",
    "pageTitle": "Generating Fingerprints for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import FingerprintGenerator from 'fingerprint-generator';\nimport { FingerprintInjector } from 'fingerprint-injector';\nimport { chromium } from 'playwright';\n\n// Instantiate a fingerprint injector\nconst fingerprintInjector = new FingerprintInjector();\n\n// Launch a browser in Playwright\nconst browser = await chromium.launch();\n\n// Instantiate the fingerprint generator with\n// configuration options\nconst fingerprintGenerator = new FingerprintGenerator({\n    browsers: [\n        { name: 'firefox', minVersion: 80 },\n    ],\n    devices: [\n        'desktop',\n    ],\n    operatingSystems: [\n        'windows',\n    ],\n});\n\n// Grab a fingerprint\nconst generated = fingerprintGenerator.getFingerprint({\n    locales: ['en-US', 'en'],\n});\n\n// Create a new browser context, plugging in\n// some values from the fingerprint\nconst context = await browser.newContext({\n    userAgent: generated.fingerprint.userAgent,\n    locale: generated.fingerprint.navigator.language,\n});\n\n// Attach the fingerprint to the newly created\n// browser context\nawait fingerprintInjector.attachFingerprintToPlaywright(context, generated);\n\n// Create a new page and go to Google\nconst page = await context.newPage();\nawait page.goto('https://google.com');"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Navigating to a Website with Multiple Browser Contexts",
    "codeDescription": "Demonstrates how to navigate to a website (deviceinfo.me) simultaneously using multiple browser contexts and wait for a specified timeout.",
    "codeLanguage": "JavaScript",
    "codeTokens": 122,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_5",
    "pageTitle": "Creating Multiple Browser Contexts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "// Go to deviceinfo.me on both at the same time\nawait Promise.all([iPhonePage.goto('https://www.deviceinfo.me/'), androidPage.goto('https://www.deviceinfo.me/')]);\n\n// Wait for 10 seconds on both before shutting down\nawait Promise.all([iPhonePage.waitForTimeout(10000), androidPage.waitForTimeout(10000)]);"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Exporting Dataset to CSV with Crawlee",
    "codeDescription": "Example showing how to export scraped data to a CSV file using Dataset.exportToCSV() after the crawler completes its run. The CSV file will be saved in the key-value store.",
    "codeLanguage": "javascript",
    "codeTokens": 77,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/exporting_data.md#2025-04-18_snippet_0",
    "pageTitle": "Exporting Data with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "// ...\nawait crawler.run();\n// Add this line to export to CSV.\nawait Dataset.exportToCSV('results');"
      }
    ],
    "relevance": 0.927
  },
  {
    "codeTitle": "Modifying GraphQL Query Variables",
    "codeDescription": "This snippet demonstrates how to modify the variables in a GraphQL query without changing the query structure. It shows how to change the search term from 'test' to 'cats'.",
    "codeLanguage": "json",
    "codeTokens": 79,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/modifying_variables.md#2025-04-18_snippet_2",
    "pageTitle": "Modifying Variables in GraphQL Queries",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"...\": \"...\",\n    \"variables\": { \"query\": \"cats\",\"count\": 10,\"cursor\": null }\n}"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Emulating Geolocation with Puppeteer",
    "codeDescription": "Using Puppeteer's page.setGeolocation() function to override browser geolocation settings. This should be used with a matching proxy location to avoid detection.",
    "codeLanguage": "javascript",
    "codeTokens": 53,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/geolocation.md#2025-04-18_snippet_0",
    "pageTitle": "Geolocation in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "page.setGeolocation()"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Creating a Weather Prediction Plot with Matplotlib",
    "codeDescription": "This snippet uses Matplotlib to create a plot of the processed weather data, setting titles, labels, and formatting options.",
    "codeLanguage": "python",
    "codeTokens": 119,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_4",
    "pageTitle": "Processing Scraped Data with Python",
    "codeList": [
      {
        "language": "python",
        "code": "print('Plotting the data...')\naxes = mean_daily_temperatures.plot(figsize=(10, 5))\naxes.set_title('Weather prediction for holiday destinations')\naxes.set_xlabel(None)\naxes.yaxis.set_major_formatter(lambda val, _: f'{int(val)} °C')\naxes.grid(which='both', linestyle='dotted')\naxes.legend(loc='best')\naxes.figure.tight_layout()"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Creating a Complex Function Type in TypeScript",
    "codeDescription": "This example shows how to create a complex function type alias for a function with multiple optional parameters and union return types. It demonstrates how to simplify verbose function declarations using type aliases.",
    "codeLanguage": "typescript",
    "codeTokens": 159,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/type_aliases.md#2025-04-18_snippet_1",
    "pageTitle": "TypeScript Type Aliases and Function Types",
    "codeList": [
      {
        "language": "typescript",
        "code": "type AddFunction = (numbers: number[], toString?: boolean, printResult?: boolean, printWithMessage?: string) => number | string | void;\n\nconst addAll: AddFunction = (nums, toString, printResult, printWithMessage) => {\n    const result = nums.reduce((prev, curr) => prev + curr, 0);\n\n    if (!printResult) return toString ? result.toString() : result;\n\n    console.log(printWithMessage || 'Result:', toString ? result.toString : result);\n};"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Implementing a Number Addition Actor in JavaScript",
    "codeDescription": "This code snippet demonstrates how to create an Actor that takes two numbers as input, adds them together, and stores the result in the default dataset. It uses the Apify SDK to handle input retrieval and data storage.",
    "codeLanguage": "javascript",
    "codeTokens": 132,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/inputs_outputs.md#2025-04-18_snippet_0",
    "pageTitle": "Creating Actors with Inputs and Outputs on Apify",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n\n// Grab our numbers which were inputted\nconst { num1, num2 } = await Actor.getInput();\n\n// Calculate the solution\nconst solution = num1 + num2;\n\n// Push the solution to the dataset\nawait Actor.pushData({ solution });\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Adding Items to a Dataset via API",
    "codeDescription": "API endpoint for adding new data items to an existing dataset. This requires a POST request with a JSON payload containing the data to be added.",
    "codeLanguage": "text",
    "codeTokens": 65,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_4",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "text",
        "code": "https://api.apify.com/v2/datasets/{DATASET_ID}/items"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Configuring and Running Qdrant Integration with Apify",
    "codeDescription": "Python code that configures and executes the Qdrant integration Actor to store crawled data in the Qdrant vector database with embeddings generation and chunking settings.",
    "codeLanguage": "python",
    "codeTokens": 237,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/qdrant.md#2025-04-18_snippet_3",
    "pageTitle": "Qdrant Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "qdrant_integration_inputs = {\n    \"qdrantUrl\": QDRANT_URL,\n    \"qdrantApiKey\": QDRANT_API_KEY,\n    \"qdrantCollectionName\": QDRANT_COLLECTION_NAME,\n    \"qdrantAutoCreateCollection\": True,\n    \"datasetId\": actor_call[\"defaultDatasetId\"],\n    \"datasetFields\": [\"text\"],\n    \"enableDeltaUpdates\": True,\n    \"deltaUpdatesPrimaryDatasetFields\": [\"url\"],\n    \"deleteExpiredObjects\": True,\n    \"expiredObjectDeletionPeriodDays\": 30,\n    \"embeddingsProvider\": \"OpenAI\",\n    \"embeddingsApiKey\": OPENAI_API_KEY,\n    \"performChunking\": True,\n    \"chunkSize\": 1000,\n    \"chunkOverlap\": 0,\n}\nactor_call = client.actor(\"apify/qdrant-integration\").call(run_input=qdrant_integration_inputs)\n"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Complete Page Function Implementation with Detail Scraping",
    "codeDescription": "Full implementation of the page function showing both pagination and detail page scraping logic. Includes parallel data extraction and type conversion for different fields.",
    "codeLanguage": "javascript",
    "codeTokens": 525,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_7",
    "pageTitle": "Scraping with Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { request, log, skipLinks, page } = context;\n    if (request.userData.label === 'START') {\n        log.info('Store opened!');\n        let timeout; // undefined\n        const buttonSelector = 'div.show-more > button';\n        for (;;) {\n            log.info('Waiting for the \"Show more\" button.');\n            try {\n                // Default timeout first time.\n                await page.waitFor(buttonSelector, { timeout });\n                // 2 sec timeout after the first.\n                timeout = 2000;\n            } catch (err) {\n                // Ignore the timeout error.\n                log.info('Could not find the \"Show more button\", '\n                    + 'we\\'ve reached the end.');\n                break;\n            }\n            log.info('Clicking the \"Show more\" button.');\n            await page.click(buttonSelector);\n        }\n    }\n\n    if (request.userData.label === 'DETAIL') {\n        const { url } = request;\n        log.info(`Scraping ${url}`);\n        await skipLinks();\n\n        // Do some scraping.\n        const uniqueIdentifier = url\n            .split('/')\n            .slice(-2)\n            .join('/');\n\n        // Get attributes in parallel to speed up the process.\n        const titleP = page.$eval(\n            'header h1',\n            (el) => el.textContent,\n        );\n        const descriptionP = page.$eval(\n            'header span.actor-description',\n            (el) => el.textContent,\n        );\n        const modifiedTimestampP = page.$eval(\n            'ul.ActorHeader-stats time',\n            (el) => el.getAttribute('datetime'),\n        );\n        const runCountTextP = page.$eval(\n            'ul.ActorHeader-stats > li:nth-of-type(3)',\n            (el) => el.textContent,\n        );\n\n        const [\n            title,\n            description,\n            modifiedTimestamp,\n            runCountText,\n        ] = await Promise.all([\n            titleP,\n            descriptionP,\n            modifiedTimestampP,\n            runCountTextP,\n        ]);\n\n        const modifiedDate = new Date(Number(modifiedTimestamp));\n        const runCount = Number(runCountText.match(/[\\d,]+/)[0].replace(',', ''));\n\n        return {\n            url,\n            uniqueIdentifier,\n            title,\n            description,\n            modifiedDate,\n            runCount,\n        };\n    }\n}"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Blocking Resources with CDP Session in Playwright",
    "codeDescription": "Demonstrates how to use a Chrome DevTools Protocol (CDP) session in Playwright to block resources while maintaining browser cache functionality.",
    "codeLanguage": "javascript",
    "codeTokens": 193,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_7",
    "pageTitle": "Reading and Intercepting Network Requests with Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Note, you can't use CDP session in other browsers!\n// Only in Chromium.\nimport { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\n// Define our blocked extensions\nconst blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];\n\n// Use CDP session to block resources\nconst client = await page.context().newCDPSession(page);\n\nawait client.send('Network.setBlockedURLs', { urls: blockedExtensions });\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Saving Scraped Data to Apify Dataset",
    "codeDescription": "Shows how to save the final merged result object to the Apify dataset using the pushData method. This is typically the final step in processing scraped data.",
    "codeLanguage": "js",
    "codeTokens": 59,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/request_labels_in_apify_actors.md#2025-04-18_snippet_4",
    "pageTitle": "Request Labels and Data Passing in Apify Actors",
    "codeList": [
      {
        "language": "js",
        "code": "await Apify.pushData(result);"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Extracting Multiple Data Points from Product Elements with JavaScript",
    "codeDescription": "This snippet demonstrates how to extract both title and price from each product element, create an object with this data, and push it into a results array.",
    "codeLanguage": "JavaScript",
    "codeTokens": 120,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/devtools_continued.md#2025-04-18_snippet_2",
    "pageTitle": "Extracting Data with DevTools in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "const results = [];\n\nfor (const product of products) {\n    const titleElement = product.querySelector('a.product-item__title');\n    const title = titleElement.textContent.trim();\n\n    const priceElement = product.querySelector('span.price');\n    const price = priceElement.childNodes[2].nodeValue.trim();\n\n    results.push({ title, price });\n}"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Performing Batch Operations with Request Queues in Python",
    "codeDescription": "This code demonstrates how to perform batch operations on request queues using the Python Apify client. It shows how to add multiple requests to a queue and how to delete multiple requests from a queue in a single API call.",
    "codeLanguage": "Python",
    "codeTokens": 213,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_10",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "Python",
        "code": "from apify_client import ApifyClient\n\napify_client = ApifyClient('MY-APIFY-TOKEN')\n\nrequest_queue_client = apify_client.request_queue('my-queue-id')\n\n# Add multiple requests to the queue\nrequest_queue_client.batch_add_requests([\n    {'url': 'http://example.com/foo', 'uniqueKey': 'http://example.com/foo', 'method': 'GET'},\n    {'url': 'http://example.com/bar', 'uniqueKey': 'http://example.com/bar', 'method': 'GET'},\n])\n\n# Remove multiple requests from the queue\nrequest_queue_client.batch_delete_requests([\n    {'uniqueKey': 'http://example.com/foo'},\n    {'uniqueKey': 'http://example.com/bar'},\n])"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "JavaScript Object for XML Conversion Example",
    "codeDescription": "Demonstrates a JavaScript object structure that will be converted to XML format, showcasing how object properties are transformed into XML tags and their values into children of these tags.",
    "codeLanguage": "json",
    "codeTokens": 113,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_16",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    name: 'Rashida Jones',\n    address: [\n        {\n            type: 'home',\n            street: '21st',\n            city: 'Chicago',\n        },\n        {\n            type: 'office',\n            street: null,\n            city: null,\n        },\n    ],\n}"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Using Node.js axios with Apify Proxy Authentication",
    "codeDescription": "Demonstrates how to configure axios in Node.js to use Apify's proxy with authentication. It specifies the proxy host, port, and authentication details, then makes a request to retrieve proxy information in JSON format.",
    "codeLanguage": "javascript",
    "codeTokens": 168,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_8",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import axios from 'axios';\n\nconst proxy = {\n    protocol: 'http',\n    host: 'proxy.apify.com',\n    port: 8000,\n    // Replace <YOUR_PROXY_PASSWORD> below with your password\n    // found at https://console.apify.com/proxy\n    auth: { username: 'auto', password: '<YOUR_PROXY_PASSWORD>' },\n};\n\nconst url = 'http://proxy.apify.com/?format=json';\n\nconst { data } = await axios.get(url, { proxy });\n\nconsole.log(data);"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Validating Runtime Statistics in JavaScript",
    "codeDescription": "This snippet checks the Actor's runtime statistics, including the number of request retries and overall runtime.",
    "codeLanguage": "javascript",
    "codeTokens": 147,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/automated_tests.md#2025-04-18_snippet_2",
    "pageTitle": "Automated Testing for Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "await expectAsync(runResult).withStatistics((stats) => {\n    // In most cases, you want it to be as close to zero as possible\n    expect(stats.requestsRetries)\n        .withContext(runResult.format('Request retries'))\n        .toBeLessThan(3);\n\n    // What is the expected run time for the number of items?\n    expect(stats.crawlerRuntimeMillis)\n        .withContext(runResult.format('Run time'))\n        .toBeWithinRange(1 * 60000, 10 * 60000);\n});"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Accessing and Modifying All Browser Contexts in Playwright",
    "codeDescription": "Shows how to loop through all browser contexts in Playwright and add an event listener to log when a specific site is visited.",
    "codeLanguage": "JavaScript",
    "codeTokens": 114,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_6",
    "pageTitle": "Creating Multiple Browser Contexts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "for (const context of browser.contexts()) {\n    // In Playwright, lots of events are supported in the \"on\" function of\n    // a BrowserContext instance\n    context.on('request', (req) => req.url() === 'https://www.deviceinfo.me/' && console.log('Site visited'));\n}"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Setting Clickable Elements Selector for JavaScript Links in Puppeteer Scraper",
    "codeDescription": "Shows how to set a selector for clickable elements that cause navigation, allowing Puppeteer Scraper to automatically click and enqueue these elements. This example is for a Turkish Remax website.",
    "codeLanguage": "javascript",
    "codeTokens": 66,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_4",
    "pageTitle": "Choosing Between Web Scraper and Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "'a[onclick ^= getPage]';"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Extracting Actor Data with Complex Selectors and Regex",
    "codeDescription": "Extracts title, description, modified date, and run count from an Actor page. Uses complex CSS selectors and regular expressions to parse formatted numbers, removing commas and converting to numeric values.",
    "codeLanguage": "javascript",
    "codeTokens": 180,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_3",
    "pageTitle": "Scraping with Web Scraper Tutorial",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { jQuery: $ } = context;\n\n    // ... rest of the code\n    return {\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n        runCount: Number(\n            $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                .text()\n                .match(/[\\d,]+/)[0]\n                .replace(/,/g, ''),\n        ),\n    };\n}"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Complete Apify-Haystack Integration Example",
    "codeDescription": "A complete Python script that combines all the previous steps into a single file for crawling a website, processing its content, and retrieving documents using both BM25 and vector search.",
    "codeLanguage": "python",
    "codeTokens": 668,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_7",
    "pageTitle": "Apify-Haystack Integration Guide",
    "codeList": [
      {
        "language": "python",
        "code": "import os\n\nfrom haystack import Document, Pipeline\nfrom haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\nfrom haystack.components.preprocessors import DocumentSplitter\nfrom haystack.components.retrievers import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\n\nfrom apify_haystack import ApifyDatasetFromActorCall\n\nos.environ[\"APIFY_API_TOKEN\"] = \"YOUR-APIFY-API-TOKEN\"\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR-OPENAI-API-KEY\"\n\ndocument_loader = ApifyDatasetFromActorCall(\n    actor_id=\"apify/website-content-crawler\",\n    run_input={\n        \"maxCrawlPages\": 3,  # limit the number of pages to crawl\n        \"startUrls\": [{\"url\": \"https://haystack.deepset.ai/\"}],\n    },\n    dataset_mapping_function=lambda item: Document(content=item[\"text\"] or \"\", meta={\"url\": item[\"url\"]}),\n)\n\ndocument_store = InMemoryDocumentStore()\nprint(f\"Initialized InMemoryDocumentStore with {document_store.count_documents()} documents\")\n\ndocument_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50)\ndocument_embedder = OpenAIDocumentEmbedder()\ndocument_writer = DocumentWriter(document_store)\n\npipe = Pipeline()\npipe.add_component(\"document_loader\", document_loader)\npipe.add_component(\"document_splitter\", document_splitter)\npipe.add_component(\"document_embedder\", document_embedder)\npipe.add_component(\"document_writer\", document_writer)\n\npipe.connect(\"document_loader\", \"document_splitter\")\npipe.connect(\"document_splitter\", \"document_embedder\")\npipe.connect(\"document_embedder\", \"document_writer\")\n\nprint(\"\\nCrawling will take some time ...\")\nprint(\"You can visit https://console.apify.com/actors/runs to monitor the progress\\n\")\n\npipe.run({})\nprint(f\"Added {document_store.count_documents()} to vector from Website Content Crawler\")\n\nprint(\"\\n ### Retrieving documents from the document store using BM25 ###\\n\")\nprint(\"query='Haystack'\\n\")\n\nbm25_retriever = InMemoryBM25Retriever(document_store)\n\nfor doc in bm25_retriever.run(\"Haystack\", top_k=1)[\"documents\"]:\n    print(doc.content)\n\nprint(\"\\n ### Retrieving documents from the document store using vector similarity ###\\n\")\nretrieval_pipe = Pipeline()\nretrieval_pipe.add_component(\"embedder\", OpenAITextEmbedder())\nretrieval_pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store, top_k=1))\n\nretrieval_pipe.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n\nresults = retrieval_pipe.run({\"embedder\": {\"text\": \"What is Haystack?\"}})\n\nfor doc in results[\"retriever\"][\"documents\"]:\n    print(doc.content)"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Running Actors with JavaScript Client",
    "codeDescription": "Example of running the Google Maps Scraper Actor using the JavaScript API client. Shows how to initialize the client, start an actor run, and fetch results from the dataset.",
    "codeLanguage": "javascript",
    "codeTokens": 152,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/index.md#2025-04-18_snippet_1",
    "pageTitle": "Running Apify Actors Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { ApifyClient } from 'apify-client';\n\nconst client = new ApifyClient({\n    token: 'MY-API-TOKEN',\n});\n\n// Start the Google Maps Scraper Actor and wait for it to finish.\nconst actorRun = await client.actor('compass/crawler-google-places').call({\n    queries: 'apify',\n});\n// Fetch scraped results from the Actor's dataset.\nconst { items } = await client.dataset(actorRun.defaultDatasetId).listItems();\nconsole.dir(items);"
      }
    ],
    "relevance": 0.923
  },
  {
    "codeTitle": "Using Apify Proxy with Python SDK and Requests",
    "codeDescription": "This Python script demonstrates how to use Apify Proxy with the requests library. It creates a proxy configuration, generates a new proxy URL, and makes multiple requests to a specified endpoint using the proxy.",
    "codeLanguage": "python",
    "codeTokens": 166,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_2",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\nimport requests, asyncio\n\nasync def main():\n    async with Actor:\n        proxy_configuration = await Actor.create_proxy_configuration()\n        proxy_url = await proxy_configuration.new_url()\n        proxies = {\n            'http': proxy_url,\n            'https': proxy_url,\n        }\n\n        for _ in range(10):\n            response = requests.get('https://api.apify.com/v2/browser-info', proxies=proxies)\n            print(response.text)\n\nif __name__ == '__main__':\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Authentication Request with Bearer Token",
    "codeDescription": "Example of how to authenticate a request to an Actor in Standby mode using a bearer token in the Authorization header.",
    "codeLanguage": "shell",
    "codeTokens": 69,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/actor_standby.md#2025-04-18_snippet_0",
    "pageTitle": "Actor Standby Mode Guide",
    "codeList": [
      {
        "language": "shell",
        "code": "curl -H \"Authorization: Bearer my_apify_token\" \\\n  https://rag-web-browser.apify.actor/search?query=apify"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Implementing State Persistence with Actor.on() - JavaScript",
    "codeDescription": "Example showing how to persist state during migration events using the Apify SDK in JavaScript. Uses Actor.on() to listen for migration events and setValue() to save state.",
    "codeLanguage": "javascript",
    "codeTokens": 100,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/state_persistence.md#2025-04-18_snippet_0",
    "pageTitle": "Actor State Persistence Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nActor.on('migrating', () => {\n    Actor.setValue('my-crawling-state', {\n        foo: 'bar',\n    });\n});\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Downloading HTML with Got-scraping in Node.js",
    "codeDescription": "This snippet demonstrates how to use the Got-scraping library to download HTML from a specific URL. It uses async/await syntax to handle the asynchronous request.",
    "codeLanguage": "javascript",
    "codeTokens": 107,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/node_js_scraper.md#2025-04-18_snippet_0",
    "pageTitle": "Scraping with Node.js",
    "codeList": [
      {
        "language": "javascript",
        "code": "// main.js\nimport { gotScraping } from 'got-scraping';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\nconsole.log(html);"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Getting Actor Input in JavaScript",
    "codeDescription": "Demonstrates how to access the Actor's input object stored in the default key-value store.",
    "codeLanguage": "javascript",
    "codeTokens": 81,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_2",
    "pageTitle": "Basic Apify SDK Commands Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n\nconst input = await Actor.getInput();\nconsole.log(input);\n// prints: {'option1': 'aaa', 'option2': 456}\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Building a Haystack Pipeline for Document Processing",
    "codeDescription": "Creates a Haystack pipeline that connects the document loader, splitter, embedder, and writer components to process and store the web content.",
    "codeLanguage": "python",
    "codeTokens": 177,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_4",
    "pageTitle": "Apify-Haystack Integration Guide",
    "codeList": [
      {
        "language": "python",
        "code": "document_store = InMemoryDocumentStore()\n\ndocument_splitter = DocumentSplitter(split_by=\"word\", split_length=150, split_overlap=50)\ndocument_embedder = OpenAIDocumentEmbedder()\ndocument_writer = DocumentWriter(document_store)\n\npipe = Pipeline()\npipe.add_component(\"document_loader\", document_loader)\npipe.add_component(\"document_splitter\", document_splitter)\npipe.add_component(\"document_embedder\", document_embedder)\npipe.add_component(\"document_writer\", document_writer)\n\npipe.connect(\"document_loader\", \"document_splitter\")\npipe.connect(\"document_splitter\", \"document_embedder\")\npipe.connect(\"document_embedder\", \"document_writer\")"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Error Handling Examples in JavaScript",
    "codeDescription": "These snippets demonstrate poor and good error logging practices. They highlight the importance of providing context and useful information in error messages for better debugging and user experience.",
    "codeLanguage": "JavaScript",
    "codeTokens": 57,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/best_practices.md#2025-04-18_snippet_2",
    "pageTitle": "Best Practices for Writing Web Scrapers",
    "codeList": [
      {
        "language": "text",
        "code": "Cannot read property \"0\" from undefined"
      },
      {
        "language": "text",
        "code": "Could not parse an address, skipping the page. Url: https://www.example-website.com/people/1234"
      },
      {
        "language": "text",
        "code": "We could not parse the price of product: Men's Trainers Orange, pushing anyways."
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Making API Request to Apify's Google SERP API",
    "codeDescription": "This snippet shows the API endpoint format for running a Google SERP task synchronously. The request requires your API token and should include the necessary input parameters as a JSON object in the request body.",
    "codeLanguage": "http",
    "codeTokens": 82,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/apify_free_google_serp_api.md#2025-04-18_snippet_0",
    "pageTitle": "Using Apify's Free Google SERP API",
    "codeList": [
      {
        "language": "http",
        "code": "https://api.apify.com/v2/acts/TASK_NAME_OR_ID/runs?token=YOUR_TOKEN"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Initializing Apify Client and Running Weather Scraper",
    "codeDescription": "This snippet initializes the ApifyClient, runs the weather scraper Actor, and checks if it completed successfully before accessing its dataset.",
    "codeLanguage": "python",
    "codeTokens": 125,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_2",
    "pageTitle": "Processing Scraped Data with Python",
    "codeList": [
      {
        "language": "python",
        "code": "client = ApifyClient(os.environ['APIFY_TOKEN'], api_url=os.environ['APIFY_API_BASE_URL'])\n\nprint('Downloading the weather data...')\nscraper_run = client.actor('~bbc-weather-scraper').call()\n\nif scraper_run['status'] != ActorJobStatus.SUCCEEDED:\n    raise RuntimeError('The weather scraper run has failed')\n\ndataset_client = client.dataset(scraper_run['defaultDatasetId'])"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Initializing Results Storage",
    "codeDescription": "Creating an empty list to store the scraped weather data results.",
    "codeLanguage": "python",
    "codeTokens": 42,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_3",
    "pageTitle": "Python Web Scraping with Beautiful Soup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "# List with scraped results\nweather_data = []"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Running an Apify Actor via cURL with JSON Input",
    "codeDescription": "cURL command to run an Actor synchronously through the Apify API. The command sends a POST request with JSON input and retrieves dataset results in CSV format.",
    "codeLanguage": "curl",
    "codeTokens": 111,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_api.md#2025-04-18_snippet_2",
    "pageTitle": "Apify API Integration Guide",
    "codeList": [
      {
        "language": "curl",
        "code": "curl -d '{\"num1\":1, \"num2\":8}' -H \"Content-Type: application/json\" -X POST \"https://api.apify.com/v2/acts/YOUR_USERNAME~adding-actor/run-sync-get-dataset-items?token=YOUR_TOKEN_HERE&format=csv\""
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "URL Processing and Screenshot Capture",
    "codeDescription": "Implements the POST endpoint for processing new URLs, capturing screenshots using Puppeteer, and storing them in the key-value store.",
    "codeLanguage": "javascript",
    "codeTokens": 216,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_4",
    "pageTitle": "Running Web Server in Apify Actor",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { launchPuppeteer } from 'crawlee';\n\napp.post('/add-url', async (req, res) => {\n    const { url } = req.body;\n    console.log(`Got new URL: ${url}`);\n\n    // Start chrome browser and open new page ...\n    const browser = await launchPuppeteer();\n    const page = await browser.newPage();\n\n    // ... go to our URL and grab a screenshot ...\n    await page.goto(url);\n    const screenshot = await page.screenshot({ type: 'jpeg' });\n\n    // ... close browser ...\n    await page.close();\n    await browser.close();\n\n    // ... save screenshot to key-value store and add URL to processedUrls.\n    await Actor.setValue(`${processedUrls.length}.jpg`, screenshot, { contentType: 'image/jpeg' });\n    processedUrls.push(url);\n\n    res.redirect('/');\n});"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Response Reading Implementation",
    "codeDescription": "Code demonstrating how to read and parse JSON responses from network requests, including error handling for non-JSON responses.",
    "codeLanguage": "javascript",
    "codeTokens": 133,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_3",
    "pageTitle": "Reading and Intercepting Network Requests with Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Notice that the callback function is now async\npage.on('response', async (res) => {\n    if (!res.request().url().includes('followings')) return;\n\n    // Grab the response body in JSON format\n    try {\n        const json = await res.json();\n        console.log(json);\n    } catch (err) {\n        console.error('Response wasn\\'t JSON or failed to parse response.');\n    }\n});"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Configuring Wait Timeout in JavaScript",
    "codeDescription": "Example of setting a custom timeout for the waitFor() function using options parameter.",
    "codeLanguage": "javascript",
    "codeTokens": 48,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_7",
    "pageTitle": "Scraping with Web Scraper Tutorial",
    "codeList": [
      {
        "language": "javascript",
        "code": "await waitFor('.bad-class', { timeoutMillis: 5000 });"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Intercepting Network Requests with Puppeteer Scraper in JavaScript",
    "codeDescription": "Demonstrates how to listen to all network requests being dispatched from the browser using Puppeteer Scraper. This example logs the URL of each request.",
    "codeLanguage": "javascript",
    "codeTokens": 66,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/when_to_use_puppeteer_scraper.md#2025-04-18_snippet_3",
    "pageTitle": "Choosing Between Web Scraper and Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "context.page.on('request', (req) => console.log(req.url()));"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Initializing Express.js Server with Apify Actor",
    "codeDescription": "Sets up the basic Express.js server configuration and initializes the Apify Actor with necessary middleware for handling form submissions.",
    "codeLanguage": "javascript",
    "codeTokens": 80,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/running_a_web_server.md#2025-04-18_snippet_0",
    "pageTitle": "Running Web Server in Apify Actor",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport express from 'express';\n\nawait Actor.init();\n\nconst app = express();\n\napp.use(express.json());\napp.use(express.urlencoded({ extended: true }));"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Running the Haystack Pipeline to Process Web Data",
    "codeDescription": "Executes the pipeline to crawl the website, process the content, and store it in the document store.",
    "codeLanguage": "python",
    "codeTokens": 43,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_5",
    "pageTitle": "Apify-Haystack Integration Guide",
    "codeList": [
      {
        "language": "python",
        "code": "pipe.run({})"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Using gotScraping with Apify Proxy in JavaScript",
    "codeDescription": "This snippet shows how to use the gotScraping library with Apify Proxy. It creates a proxy configuration, generates a new proxy URL, and makes two requests to demonstrate IP rotation.",
    "codeLanguage": "javascript",
    "codeTokens": 195,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_3",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport { gotScraping } from 'got-scraping';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\nconst proxyUrl = await proxyConfiguration.newUrl();\n\nconst url = 'https://api.apify.com/v2/browser-info';\n\nconst response1 = await gotScraping({\n    url,\n    proxyUrl,\n    responseType: 'json',\n});\n\nconst response2 = await gotScraping({\n    url,\n    proxyUrl,\n    responseType: 'json',\n});\n\nconsole.log(response1.body.clientIp);\nconsole.log('Should be different than');\nconsole.log(response2.body.clientIp);\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Importing Required Packages for Apify-Haystack Integration",
    "codeDescription": "Imports all necessary Python packages including Haystack components and the Apify integration module.",
    "codeLanguage": "python",
    "codeTokens": 140,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_1",
    "pageTitle": "Apify-Haystack Integration Guide",
    "codeList": [
      {
        "language": "python",
        "code": "from haystack import Document, Pipeline\nfrom haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\nfrom haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\nfrom haystack.components.retrievers import InMemoryBM25Retriever, InMemoryEmbeddingRetriever\nfrom haystack.components.writers import DocumentWriter\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom haystack.utils.auth import Secret\n\nfrom apify_haystack import ApifyDatasetFromActorCall"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Creating New Browser Context in Playwright and Puppeteer",
    "codeDescription": "Shows how to create a new browser context using Playwright's browser.newContext() and Puppeteer's browser.createIncognitoBrowserContext() methods.",
    "codeLanguage": "JavaScript",
    "codeTokens": 62,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_0",
    "pageTitle": "Creating Multiple Browser Contexts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "const myNewContext = await browser.newContext();"
      },
      {
        "language": "JavaScript",
        "code": "const myNewContext = await browser.createIncognitoBrowserContext();"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Using PHP cURL with Apify Proxy Authentication",
    "codeDescription": "Shows how to use PHP's cURL extension to make HTTP requests through Apify's proxy. It sets up a curl handler with proxy settings and authentication credentials, then executes the request and outputs the response.",
    "codeLanguage": "php",
    "codeTokens": 171,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_11",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "php",
        "code": "<?php\n$curl = curl_init(\"http://proxy.apify.com/?format=json\");\ncurl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);\ncurl_setopt($curl, CURLOPT_PROXY, \"http://proxy.apify.com:8000\");\n// Replace <YOUR_PROXY_PASSWORD> below with your password\n// found at https://console.apify.com/proxy\ncurl_setopt($curl, CURLOPT_PROXYUSERPWD, \"auto:<YOUR_PROXY_PASSWORD>\");\n$response = curl_exec($curl);\ncurl_close($curl);\nif ($response) echo $response;\n?>"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Initializing Dataset Client in JavaScript",
    "codeDescription": "Example showing how to initialize and save a reference to a specific dataset using the JavaScript API client. This allows easy access to dataset operations in Node.js applications.",
    "codeLanguage": "javascript",
    "codeTokens": 70,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_6",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "javascript",
        "code": "const myDatasetClient = apifyClient.dataset('jane-doe/my-dataset');"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Implementing RAG Web Browser Function for OpenAI Assistant",
    "codeDescription": "This function implements the actual call to the RAG Web Browser Actor on Apify, fetching search results based on the given query and maximum number of results.",
    "codeLanguage": "python",
    "codeTokens": 168,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_5",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "def call_rag_web_browser(query: str, max_results: int) -> list[dict]:\n    \"\"\"\n    Query Google search, scrape the top N pages from the results, and returns their cleaned content as markdown.\n    First start the Actor and wait for it to finish. Then fetch results from the Actor run's default dataset.\n    \"\"\"\n    actor_call = apify_client.actor(\"apify/rag-web-browser\").call(run_input={\"query\": query, \"maxResults\": max_results})\n    return apify_client.dataset(actor_call[\"defaultDatasetId\"]).list_items().items"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Complete Scraping Function with Cheerio",
    "codeDescription": "Final version that extracts title, description, modified date, and run count using complex selectors and data transformations.",
    "codeLanguage": "javascript",
    "codeTokens": 164,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_3",
    "pageTitle": "Cheerio Scraper Tutorial Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { $ } = context;\n    // ... rest of your code can come here\n    return {\n        title: $('header h1').text(),\n        description: $('header span.actor-description').text(),\n        modifiedDate: new Date(\n            Number(\n                $('ul.ActorHeader-stats time').attr('datetime'),\n            ),\n        ),\n        runCount: Number(\n            $('ul.ActorHeader-stats > li:nth-of-type(3)')\n                .text()\n                .match(/[\\d,]+/)[0]\n                .replace(/,/g, ''),\n        ),\n    };\n}"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Setting Up Router Configuration",
    "codeDescription": "Router configuration script that creates a Cheerio router and adds a default request handler.",
    "codeLanguage": "javascript",
    "codeTokens": 80,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/initializing_and_setting_up.md#2025-04-18_snippet_2",
    "pageTitle": "Crawlee Web Scraper Setup Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "// routes.js\nimport { createCheerioRouter } from 'crawlee';\n\nexport const router = createCheerioRouter();\n\nrouter.addDefaultHandler(({ log }) => {\n    log.info('Route reached.');\n});"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Initializing OpenAI and Apify Clients",
    "codeDescription": "This snippet initializes the OpenAI and Apify clients with their respective API keys. Users need to replace placeholder values with their actual API keys.",
    "codeLanguage": "python",
    "codeTokens": 75,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_2",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "client = OpenAI(api_key=\"YOUR OPENAI API KEY\")\napify_client = ApifyClient(\"YOUR APIFY API TOKEN\")"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Opening a Web Page with Puppeteer in JavaScript",
    "codeDescription": "This snippet shows how to launch a Puppeteer browser instance, create a new page, and navigate to a specific URL. It's the initial setup for interacting with a web form.",
    "codeLanguage": "javascript",
    "codeTokens": 90,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_4",
    "pageTitle": "Submitting a Form with File Attachment in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "const browser = await puppeteer.launch();\nconst page = await browser.newPage();\nawait page.goto('https://some-site.com/file-upload.php');"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "HTML Download Function with BeautifulSoup",
    "codeDescription": "Function that downloads HTML content from a given URL and returns a BeautifulSoup object for parsing.",
    "codeLanguage": "python",
    "codeTokens": 71,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_1",
    "pageTitle": "Python Web Scraping with BeautifulSoup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "def download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n\n    html_code = response.text\n    return BeautifulSoup(html_code, \"html.parser\")"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Creating an Ad-hoc Webhook in JavaScript SDK",
    "codeDescription": "Code snippet showing how to dynamically create a webhook from within an Actor's JavaScript code using the Apify SDK. This registers a webhook that triggers when the Actor run fails.",
    "codeLanguage": "javascript",
    "codeTokens": 108,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/programming/webhooks/ad_hoc_webhooks.md#2025-04-18_snippet_2",
    "pageTitle": "Ad-hoc Webhooks Documentation for Apify",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nawait Actor.addWebhook({\n    eventTypes: ['ACTOR.RUN.FAILED'],\n    requestUrl: 'https://example.com/run-failed',\n});\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Retrieving Documents Using BM25 Search",
    "codeDescription": "Demonstrates how to retrieve documents from the document store using BM25 text retrieval, which is based on keyword matching.",
    "codeLanguage": "python",
    "codeTokens": 121,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/haystack.md#2025-04-18_snippet_6",
    "pageTitle": "Apify-Haystack Integration Guide",
    "codeList": [
      {
        "language": "python",
        "code": "print(f\"Added {document_store.count_documents()} to vector from Website Content Crawler\")\n\nprint(\"Retrieving documents from the document store using BM25\")\nprint(\"query='Haystack'\")\nbm25_retriever = InMemoryBM25Retriever(document_store)\nfor doc in bm25_retriever.run(\"Haystack\", top_k=1)[\"documents\"]:\n  print(doc.content)"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Configuring Actor Output UI in JSON",
    "codeDescription": "This JSON configuration defines the structure and presentation of the Actor's output tab UI. It specifies the dataset schema, including field transformations and display properties for various data types such as images, links, text, boolean, arrays, objects, dates, and numbers.",
    "codeLanguage": "json",
    "codeTokens": 401,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/index.md#2025-04-18_snippet_1",
    "pageTitle": "Dataset Schema Specification for Apify Actors",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"actorSpecification\": 1,\n    \"name\": \"Actor Name\",\n    \"title\": \"Actor Title\",\n    \"version\": \"1.0.0\",\n    \"storages\": {\n        \"dataset\": {\n            \"actorSpecification\": 1,\n            \"views\": {\n                \"overview\": {\n                    \"title\": \"Overview\",\n                    \"transformation\": {\n                        \"fields\": [\n                            \"pictureUrl\",\n                            \"linkUrl\",\n                            \"textField\",\n                            \"booleanField\",\n                            \"arrayField\",\n                            \"objectField\",\n                            \"dateField\",\n                            \"numericField\"\n                        ]\n                    },\n                    \"display\": {\n                        \"component\": \"table\",\n                        \"properties\": {\n                            \"pictureUrl\": {\n                                \"label\": \"Image\",\n                                \"format\": \"image\"\n                            },\n                            \"linkUrl\": {\n                                \"label\": \"Link\",\n                                \"format\": \"link\"\n                            },\n                            \"textField\": {\n                                \"label\": \"Text\",\n                                \"format\": \"text\"\n                            },\n                            \"booleanField\": {\n                                \"label\": \"Boolean\",\n                                \"format\": \"boolean\"\n                            },\n                            \"arrayField\": {\n                                \"label\": \"Array\",\n                                \"format\": \"array\"\n                            },\n                            \"objectField\": {\n                                \"label\": \"Object\",\n                                \"format\": \"object\"\n                            },\n                            \"dateField\": {\n                                \"label\": \"Date\",\n                                \"format\": \"date\"\n                            },\n                            \"numericField\": {\n                                \"label\": \"Number\",\n                                \"format\": \"number\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Implementing Proxy Rotation in BasicCrawler",
    "codeDescription": "Example of setting up proxy rotation for BasicCrawler using request-promise package. Each request uses a fresh proxy from Apify's proxy pool.",
    "codeLanguage": "javascript",
    "codeTokens": 152,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/handle_blocked_requests_puppeteer.md#2025-04-18_snippet_0",
    "pageTitle": "Handling Blocked Requests in Web Crawlers",
    "codeList": [
      {
        "language": "javascript",
        "code": "const Apify = require('apify');\nconst requestPromise = require('request-promise');\n\nconst PROXY_PASSWORD = process.env.APIFY_PROXY_PASSWORD;\nconst proxyUrl = `http://auto:${PROXY_PASSWORD}@proxy.apify.com`;\n\nconst crawler = new Apify.BasicCrawler({\n    requestList: someInitializedRequestList,\n    handleRequestFunction: async ({ request }) => {\n        const response = await requestPromise({\n            url: request.url,\n            proxy: proxyUrl,\n        });\n    },\n});"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Accessing and Modifying All Browser Contexts in Puppeteer",
    "codeDescription": "Demonstrates how to loop through all browser contexts in Puppeteer and add an event listener to log when any target URL is changed.",
    "codeLanguage": "JavaScript",
    "codeTokens": 102,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_7",
    "pageTitle": "Creating Multiple Browser Contexts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "for (const context of browser.browserContexts()) {\n    // In Puppeteer, only three events are supported in the \"on\" function\n    // of a BrowserContext instance\n    context.on('targetchanged', () => console.log('Site visited'));\n}"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Launching Persistent Browser Context in Playwright",
    "codeDescription": "Demonstrates how to launch a persistent browser context in Playwright using chromium.launchPersistentContext(). This context stores cache, cookies, and storage on disk.",
    "codeLanguage": "JavaScript",
    "codeTokens": 115,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/browser_contexts.md#2025-04-18_snippet_1",
    "pageTitle": "Creating Multiple Browser Contexts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { chromium } from 'playwright';\n\n// Here, we launch a persistent browser context. The first\n// argument is the location to store the data.\nconst browser = await chromium.launchPersistentContext('./persistent-context', { headless: false });\n\nconst page = await browser.newPage();\n\nawait browser.close();"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Storing Global Run Statistics in JSON",
    "codeDescription": "This snippet demonstrates the structure for storing global run statistics in the Key-Value store. It includes an errors object with error messages for each request path and the total number of saved items.",
    "codeLanguage": "json",
    "codeTokens": 116,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/saving_useful_stats.md#2025-04-18_snippet_1",
    "pageTitle": "Saving Useful Run Statistics in Apify Actors",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"errors\": { // all of the errors for every request path\n        \"some-site.com/products/123\": [\n            \"error1\",\n            \"error2\"\n        ]\n    },\n    \"totalSaved\": 43 // total number of saved items throughout the entire run\n}"
      }
    ],
    "relevance": 0.912
  },
  {
    "codeTitle": "Counting Product Items on a Web Page with Python and Beautiful Soup",
    "codeDescription": "This code downloads a product listing page, parses the HTML, and counts the number of product items using a CSS selector.",
    "codeLanguage": "python",
    "codeTokens": 111,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_3",
    "pageTitle": "Parsing HTML with Python for Web Scraping",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\nproducts = soup.select(\".product-item\")\nprint(len(products))"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Defining Custom Object Types in TypeScript",
    "codeDescription": "Demonstrates how to create a custom object type with string properties for a course object.",
    "codeLanguage": "typescript",
    "codeTokens": 80,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types_continued.md#2025-04-18_snippet_0",
    "pageTitle": "TypeScript Core Types Tutorial - Objects, Arrays, and Tuples",
    "codeList": [
      {
        "language": "typescript",
        "code": "const course: {\n    name: string;\n    currentLesson: string;\n} = {\n    name: 'Switching to TypeScript',\n    currentLesson: 'Using types - II',\n};"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "TypeScript Type Definitions",
    "codeDescription": "Complete type definitions including Product interface, ResponseData interface, ModifiedProduct type, SortOrder enum, and UserInput interface with generics.",
    "codeLanguage": "typescript",
    "codeTokens": 178,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_2",
    "pageTitle": "TypeScript Mini-Project Documentation",
    "codeList": [
      {
        "language": "typescript",
        "code": "export interface Product {\n    id: number;\n    title: string;\n    description: string;\n    price: number;\n    discountPercentage: number;\n    rating: number;\n    stock: number;\n    brand: string;\n    category: string;\n    thumbnail: string;\n    images: string[];\n}\n\nexport interface ResponseData {\n    products: Product[];\n}\n\nexport type ModifiedProduct = Omit<Product, 'images'>;\n\nexport enum SortOrder {\n    ASC = 'ascending',\n    DESC = 'descending',\n}\n\nexport interface UserInput<RemoveImages extends boolean = boolean> {\n    sort: 'ascending' | 'descending';\n    removeImages: RemoveImages;\n}"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Opening New Page with Playwright",
    "codeDescription": "Demonstrates how to launch a Chrome browser instance and create a new page using Playwright. The browser is launched in non-headless mode for visibility.",
    "codeLanguage": "javascript",
    "codeTokens": 92,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/index.md#2025-04-18_snippet_0",
    "pageTitle": "Opening and Controlling Pages in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\n\n// Open a new page\nconst page = await browser.newPage();\n\nawait browser.close();"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Compiling TypeScript to JavaScript",
    "codeDescription": "Use the TypeScript compiler (tsc) to compile a .ts file into JavaScript. This command generates a .js file that can be run with Node.js.",
    "codeLanguage": "shell",
    "codeTokens": 56,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/installation.md#2025-04-18_snippet_5",
    "pageTitle": "Installing TypeScript and Writing First TS Code",
    "codeList": [
      {
        "language": "shell",
        "code": "tsc first-lines.ts"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Managing Request Queues in Python SDK",
    "codeDescription": "Demonstrates various operations on request queues using the Apify Python SDK. It includes adding requests, fetching requests, handling specific requests, and dropping a queue.",
    "codeLanguage": "python",
    "codeTokens": 235,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_7",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\nfrom apify.storages import RequestQueue\n\nasync def main():\n    async with Actor:\n        queue: RequestQueue = await Actor.open_request_queue()\n\n        # Enqueue requests\n        await queue.add_request(request={'url': 'http:#example.com/aaa'})\n        await queue.add_request(request={'url': 'http:#example.com/foo'})\n        await queue.add_request(request={'url': 'http:#example.com/bar'}, forefront=True)\n\n        # Get the next requests from queue\n        request1 = await queue.fetch_next_request()\n        request2 = await queue.fetch_next_request()\n\n        # Get a specific request\n        specific_request = await queue.get_request('shi6Nh3bfs3')\n\n        # Reclaim a failed request back to the queue and process it again\n        await queue.reclaim_request(request2)\n\n        # Remove a queue\n        await queue.drop()"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Complete Python Actor with Input/Output Handling",
    "codeDescription": "Full implementation of a Python Actor with custom input and output handling functions.",
    "codeLanguage": "python",
    "codeTokens": 292,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/inputs_outputs.md#2025-04-18_snippet_6",
    "pageTitle": "Actor Input and Output Handling Guide",
    "codeList": [
      {
        "language": "python",
        "code": "# index.py\nfrom apify_client import ApifyClient\nfrom os import environ\nimport json\n\nclient = ApifyClient(token='YOUR_TOKEN')\n\ndef is_on_apify ():\n    return 'APIFY_IS_AT_HOME' in environ\n\ndef get_input ():\n    if not is_on_apify():\n        with open('./apify_storage/key_value_stores/default/INPUT.json') as actor_input:\n            return json.load(actor_input)\n\n    kv_store = client.key_value_store(environ.get('APIFY_DEFAULT_KEY_VALUE_STORE_ID'))\n    return kv_store.get_record('INPUT')['value']\n\n# Push the solution to the dataset\ndef set_output (data):\n    if not is_on_apify():\n        with open('./apify_storage/datasets/default/solution.json', 'w') as output:\n            return output.write(json.dumps(data, indent=2))\n\n    dataset = client.dataset(environ.get('APIFY_DEFAULT_DATASET_ID'))\n    dataset.push_items('OUTPUT', value=[json.dumps(data, indent=4)])\n\ndef add_all_numbers (nums):\n    total = 0\n\n    for num in nums:\n        total += num\n\n    return total\n\nactor_input = get_input()['numbers']\n\nsolution = add_all_numbers(actor_input)\n\nset_output({ 'solution': solution })"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Blocking Resources with Playwright",
    "codeDescription": "Demonstrates how to use Playwright to launch a browser, create a new page, and block requests for specific file types.",
    "codeLanguage": "javascript",
    "codeTokens": 167,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_5",
    "pageTitle": "Reading and Intercepting Network Requests with Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nconst blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];\n\n// Only listen for requests with one of our blocked extensions\n// Abort all matching requests\npage.route(`**/*{${blockedExtensions.join(',')}}`, async (route) => route.abort());\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Configuring SessionPool with Crawlee",
    "codeDescription": "Sets up a SessionPool in a Crawlee crawler configuration. This demonstrates the basic structure of where session pool configuration options should be placed.",
    "codeLanguage": "javascript",
    "codeTokens": 139,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/rotating_proxies.md#2025-04-18_snippet_1",
    "pageTitle": "Rotating Proxies and Sessions for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "const crawler = new CheerioCrawler({\n    requestList,\n    requestQueue,\n    proxyConfiguration,\n    useSessionPool: true,\n    // This is where our session pool\n    // configuration lives\n    sessionPoolOptions: {\n        // We can add options for each\n        // session created by the session\n        // pool here\n        sessionOptions: {\n\n        },\n    },\n    maxConcurrency: 50,\n    // ...\n});"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Initializing OpenAI and Apify Clients with API Keys",
    "codeDescription": "Sets up the OpenAI and Apify client instances using API keys for authentication. These clients are required for all subsequent API operations.",
    "codeLanguage": "python",
    "codeTokens": 89,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_11",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "from apify_client import ApifyClient\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"YOUR OPENAI API KEY\")\napify_client = ApifyClient(\"YOUR APIFY API TOKEN\")"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Exporting Product Data to CSV in Python",
    "codeDescription": "Uses the csv module from Python's standard library to write the collected product data to a CSV file.",
    "codeLanguage": "python",
    "codeTokens": 92,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/08_saving_data.md#2025-04-18_snippet_1",
    "pageTitle": "Saving Data with Python in CSV and JSON Formats",
    "codeList": [
      {
        "language": "python",
        "code": "import csv\n\nwith open(\"products.csv\", \"w\") as file:\n    writer = csv.DictWriter(file, fieldnames=[\"title\", \"min_price\", \"price\"])\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Configuring Actor Input via JSON in Apify API",
    "codeDescription": "This JSON object demonstrates how to configure input parameters for an Actor when running it through the Apify API. It includes settings for maximum requests per crawl, proxy configuration, and the starting URL.",
    "codeLanguage": "json",
    "codeTokens": 100,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/running/input_and_output.md#2025-04-18_snippet_0",
    "pageTitle": "Configuring Input and Output for Apify Actors",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"maxRequestsPerCrawl\": 10,\n    \"proxy\": {\n        \"useApifyProxy\": true\n    },\n    \"startUrl\": \"https://apify.com\"\n}"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Retrieving Specific Fields from Dataset in JavaScript",
    "codeDescription": "Illustrates how to use the 'fields' option in the getData() method to retrieve specific data fields from a dataset in JavaScript.",
    "codeLanguage": "javascript",
    "codeTokens": 112,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_11",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\nconst dataset = await Actor.openDataset();\n\n// Only get the 'hotel' and 'cafe' fields\nconst hotelAndCafeData = await dataset.getData({\n    fields: ['hotel', 'cafe'],\n});\n\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Implementing Country Selection Dropdown in Apify Actor Input",
    "codeDescription": "This example shows how to create a select input for country selection in an Apify Actor. It defines enum values and titles for USA, Germany, and France, with a default selection of 'us'.",
    "codeLanguage": "json",
    "codeTokens": 122,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_4",
    "pageTitle": "Actor Input Schema Specification",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"title\": \"Country\",\n    \"type\": \"string\",\n    \"description\": \"Select your country\",\n    \"editor\": \"select\",\n    \"default\": \"us\",\n    \"enum\": [\"us\", \"de\", \"fr\"],\n    \"enumTitles\": [\"USA\", \"Germany\", \"France\"]\n}"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Visualizing Weather Data with Matplotlib",
    "codeDescription": "Creates a plot of the processed weather data using matplotlib, setting appropriate titles, labels, and formatting.",
    "codeLanguage": "python",
    "codeTokens": 125,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_12",
    "pageTitle": "Python Web Scraping with Beautiful Soup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "# Create a plot of the data\nprint('Plotting the data...')\naxes = mean_daily_temperatures.plot(figsize=(10, 5))\naxes.set_title('Weather prediction for holiday destinations')\naxes.set_xlabel(None)\naxes.yaxis.set_major_formatter(lambda val, _: f'{int(val)} °C')\naxes.grid(which='both', linestyle='dotted')\naxes.legend(loc='best')\naxes.figure.tight_layout()"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Exposing Functions with Puppeteer",
    "codeDescription": "Shows how to expose a function to the page context using Puppeteer's page.exposeFunction(). The example demonstrates exposing and executing a simple message function.",
    "codeLanguage": "javascript",
    "codeTokens": 141,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/injecting_code.md#2025-04-18_snippet_4",
    "pageTitle": "Injecting Code in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\nconst returnMessage = () => 'Apify academy!';\n\nawait page.exposeFunction(returnMessage.name, returnMessage);\n\nconst msg = await page.evaluate(() => returnMessage());\n\nconsole.log(msg);\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Using PHP Guzzle with Apify Proxy Authentication",
    "codeDescription": "Demonstrates how to configure the Guzzle HTTP client in PHP to use Apify's proxy. It creates a new Guzzle client with a proxy URL that includes authentication credentials, then makes an HTTP request and outputs the response.",
    "codeLanguage": "php",
    "codeTokens": 152,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_12",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "php",
        "code": "<?php\nrequire 'vendor/autoload.php';\n\n\n$client = new \\GuzzleHttp\\Client([\n    // Replace <YOUR_PROXY_PASSWORD> below with your password\n    // found at https://console.apify.com/proxy\n    'proxy' => 'http://auto:<YOUR_PROXY_PASSWORD>@proxy.apify.com:8000'\n]);\n\n$response = $client->get(\"http://proxy.apify.com/?format=json\");\necho $response->getBody();"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Setting up Basic Actor.json Configuration with Dataset Schema",
    "codeDescription": "Template code for creating an actor.json file that defines the dataset schema with example fields. This configuration enables the Overview table interface for displaying Actor results.",
    "codeLanguage": "json",
    "codeTokens": 286,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/output_schema.md#2025-04-18_snippet_0",
    "pageTitle": "Dataset Schema Implementation for Apify Actors",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"actorSpecification\": 1,\n    \"name\": \"___ENTER_ACTOR_NAME____\",\n    \"title\": \"___ENTER_ACTOR_TITLE____\",\n    \"version\": \"1.0.0\",\n    \"storages\": {\n        \"dataset\": {\n            \"actorSpecification\": 1,\n            \"views\": {\n                \"overview\": {\n                    \"title\": \"Overview\",\n                    \"transformation\": {\n                        \"fields\": [\n                            \"___EXAMPLE_NUMERIC_FIELD___\",\n                            \"___EXAMPLE_PICTURE_URL_FIELD___\",\n                            \"___EXAMPLE_LINK_URL_FIELD___\",\n                            \"___EXAMPLE_TEXT_FIELD___\",\n                            \"___EXAMPLE_BOOLEAN_FIELD___\"\n                        ]\n                    },\n                    \"display\": {\n                        \"component\": \"table\",\n                        \"properties\": {\n                            \"___EXAMPLE_NUMERIC_FIELD___\": {\n                                \"label\": \"ID\",\n                                \"format\": \"number\"\n                            },\n                            \"___EXAMPLE_PICTURE_URL_FIELD___\": {\n                                \"format\": \"image\"\n                            },\n                            \"___EXAMPLE_LINK_URL_FIELD___\": {\n                                \"label\": \"Clickable link\",\n                                \"format\": \"link\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}"
      }
    ],
    "relevance": 0.907
  },
  {
    "codeTitle": "Parsing HTML and Extracting Heading with Python and Beautiful Soup",
    "codeDescription": "This code demonstrates how to download HTML, parse it with Beautiful Soup, and extract the main heading (h1 tag) from the page.",
    "codeLanguage": "python",
    "codeTokens": 111,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_1",
    "pageTitle": "Parsing HTML with Python for Web Scraping",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\n\nurl = \"https://warehouse-theme-metal.myshopify.com/collections/sales\"\nresponse = httpx.get(url)\nresponse.raise_for_status()\n\nhtml_code = response.text\nsoup = BeautifulSoup(html_code, \"html.parser\")\nprint(soup.select(\"h1\"))"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Common Validation Types Examples",
    "codeDescription": "Collection of JSON Schema examples showing common validation patterns like optional fields, multiple types, nullable fields, and array validation.",
    "codeLanguage": "json",
    "codeTokens": 110,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/validation.md#2025-04-18_snippet_3",
    "pageTitle": "Dataset Schema Validation in Apify Actors",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"type\": \"string\"\n        },\n        \"price\": {\n            \"type\": \"number\"\n        }\n    },\n    \"required\": [\"name\"]\n}"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Using Named Datasets in JavaScript",
    "codeDescription": "Shows how to open and use a named dataset in JavaScript, which can be shared between Actors or Actor runs. It demonstrates opening a dataset and pushing data to it.",
    "codeLanguage": "javascript",
    "codeTokens": 114,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_10",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\n// Save a named dataset to a variable\nconst dataset = await Actor.openDataset('some-name');\n\n// Add data to the named dataset\nawait dataset.pushData({ foo: 'bar' });\n\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Implementing State Persistence with Actor.on() - Python",
    "codeDescription": "Example showing how to persist state during migration events using the Apify SDK in Python. Uses Actor.on() to listen for migration events and set_value() to save state.",
    "codeLanguage": "python",
    "codeTokens": 113,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/builds_and_runs/state_persistence.md#2025-04-18_snippet_1",
    "pageTitle": "Actor State Persistence Guide",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor, Event\n\nasync def actor_migrate(_event_data):\n    await Actor.set_value('my-crawling-state', {'foo': 'bar'})\n\nasync def main():\n    async with Actor:\n        # ...\n        Actor.on(Event.MIGRATING, actor_migrate)\n        # ..."
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Implementing GraphQL Query in JavaScript",
    "codeDescription": "Imports the GraphQL query, sets up variables, and makes an API request using got-scraping library in JavaScript.",
    "codeLanguage": "javascript",
    "codeTokens": 274,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/custom_queries.md#2025-04-18_snippet_3",
    "pageTitle": "Custom GraphQL Queries for API Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { gql } from 'graphql-tag';\nimport scrapeAppToken from './scrapeAppToken.mjs';\n\nconst token = await scrapeAppToken();\n\nconst GET_LATEST = gql`\n    query SearchQuery($query: String!, $max_age: Int!) {\n        organization {\n            media(query: $query, max_age: $max_age, first: 1000) {\n                edges {\n                    node {\n                        title\n                        public_at\n                        hero_video {\n                            video_urls {\n                                url\n                            }\n                        }\n                        thumbnail_url\n                    }\n                }\n            }\n        }\n    }\n`;\n\nconst testInput = { hours: 48, query: 'stocks' };\n\nconst variables = { query: testInput.query, max_age: Math.round(testInput.hours) * 60 ** 2 };\n\nconst data = await gotScraping('https://api.cheddar.com/graphql', {\n    responseType: 'json',\n    method: 'POST',\n    headers: { 'X-App-Token': token, 'Content-Type': 'application/json' },\n    body: JSON.stringify({ query: GET_LATEST.loc.source.body, variables }),\n});"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Filtering Products from JSON File in Python",
    "codeDescription": "This Python script reads a JSON file containing product data, filters products with a minimum price greater than $500, and prints the results using the pretty print function. It uses the json module for parsing, decimal for precise comparisons, and pprint for formatted output.",
    "codeLanguage": "python",
    "codeTokens": 125,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/08_saving_data.md#2025-04-18_snippet_3",
    "pageTitle": "Saving Data with Python in CSV and JSON Formats",
    "codeList": [
      {
        "language": "python",
        "code": "import json\nfrom pprint import pp\nfrom decimal import Decimal\n\nwith open(\"products.json\", \"r\") as file:\n    products = json.load(file)\n\nfor product in products:\n    if Decimal(product[\"min_price\"]) > 500:\n        pp(product)"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Creating an Apify Actor Integration Webhook",
    "codeDescription": "This JSON configuration demonstrates how to create a webhook for Actor integration in Apify. It specifies the requestUrl pointing to the integration Actor, event type to trigger on, condition for filtering events, and payload template for data passing. The isApifyIntegration flag ensures proper display in Apify Console.",
    "codeLanguage": "json5",
    "codeTokens": 172,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/actors/integrating_actors_via_api.md#2025-04-18_snippet_0",
    "pageTitle": "Integrating Actors via API with Apify",
    "codeList": [
      {
        "language": "json5",
        "code": "{\n    \"requestUrl\": \"https://api.apify.com/v2/acts/<integration-actor-id>/runs\",\n    \"eventTypes\": [\"ACTOR.RUN.SUCCEEDED\"],\n    \"condition\": {\n        \"actorId\": \"<actor-id>\",\n    },\n    \"shouldInterpolateStrings\": true,\n    \"isApifyIntegration\": true,\n    \"payloadTemplate\": \"{\\\"field\\\":\\\"value\\\",\\\"payload\\\":{\\\"resource\\\":\\\"{{resource}}\\\"}}\",\n}"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Scraping Title with Cheerio in JavaScript",
    "codeDescription": "Uses Cheerio to extract the title from a webpage by selecting the h1 element within a header tag.",
    "codeLanguage": "javascript",
    "codeTokens": 81,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/cheerio_scraper.md#2025-04-18_snippet_0",
    "pageTitle": "Cheerio Scraper Tutorial Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const { $ } = context;\n    // ... rest of your code can come here\n    return {\n        title: $('header h1').text(),\n    };\n}"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Finding All Product Elements with JavaScript in DevTools",
    "codeDescription": "This snippet demonstrates how to use querySelectorAll() to find all product elements on a webpage and count them using the length property.",
    "codeLanguage": "JavaScript",
    "codeTokens": 60,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/devtools_continued.md#2025-04-18_snippet_0",
    "pageTitle": "Extracting Data with DevTools in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "const products = document.querySelectorAll('.product-item');\nproducts.length;"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Implementing Show More Button Pagination in Puppeteer",
    "codeDescription": "Demonstrates how to handle pagination by clicking a Show More button repeatedly until it's no longer available. Includes timeout handling with different initial and subsequent wait times.",
    "codeLanguage": "javascript",
    "codeTokens": 202,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_6",
    "pageTitle": "Scraping with Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    // ...\n    let timeout; // undefined\n    const buttonSelector = 'div.show-more > button';\n    for (;;) {\n        log.info('Waiting for the \"Show more\" button.');\n        try {\n        // Default timeout first time.\n            await page.waitFor(buttonSelector, { timeout });\n            // 2 sec timeout after the first.\n            timeout = 2000;\n        } catch (err) {\n        // Ignore the timeout error.\n            log.info('Could not find the \"Show more button\", '\n            + 'we\\'ve reached the end.');\n            break;\n        }\n        log.info('Clicking the \"Show more\" button.');\n        await page.click(buttonSelector);\n    }\n    // ...\n}"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Adding Custom Proxy URLs in Python SDK",
    "codeDescription": "Utilize the proxy_configuration.new_url(session_id) method to incorporate custom proxy URLs into the proxy configuration when using the Python SDK. This enables the use of personal proxies in Apify projects.",
    "codeLanguage": "python",
    "codeTokens": 65,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/your_own_proxies.md#2025-04-18_snippet_1",
    "pageTitle": "Using Custom Proxies with Apify Platform",
    "codeList": [
      {
        "language": "python",
        "code": "proxy_configuration.new_url(session_id)"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Extracting Product URLs from Shopify Sales Page using Cheerio in JavaScript",
    "codeDescription": "This snippet uses Cheerio to parse the HTML and extract product URLs from the sales page. It converts relative URLs to absolute URLs.",
    "codeLanguage": "JavaScript",
    "codeTokens": 116,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/scraping_the_data.md#2025-04-18_snippet_5",
    "pageTitle": "Web Scraping Tutorial: Crawling and Data Extraction",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "const $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    productUrls.push(absoluteUrl);\n}"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Using ChargingManager in Python for Pay-per-event (PPE) Model",
    "codeDescription": "This code demonstrates how to use the ChargingManager from the Apify Python SDK to handle charging for events in the Pay-per-event (PPE) pricing model.",
    "codeLanguage": "python",
    "codeTokens": 189,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/publishing/monetize.mdx#2025-04-18_snippet_2",
    "pageTitle": "Monetizing Actors in Apify Store",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        charging_manager = Actor.charging_manager()\n\n        charge_result = await charging_manager.charge(\n            'ACTOR_START',\n            amount=1,  # charge $1 for starting the Actor\n            idempotency_key='unique-key-for-this-run'\n        )\n\n        if charge_result.error:\n            print(f'Charging failed: {charge_result.error}')\n            return\n\n        if charge_result.max_reached:\n            print('Maximum cost per run reached. Stopping the Actor.')\n            return\n\n        # Continue with Actor logic\n\nif __name__ == '__main__':\n    Actor.main(main)"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Dataset Operations in Python",
    "codeDescription": "Demonstrates pushing data to an Apify dataset in Python.",
    "codeLanguage": "python",
    "codeTokens": 71,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_7",
    "pageTitle": "Basic Apify SDK Commands Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        # Append result object to the default dataset associated with the run\n        await Actor.push_data({'some_result': 123})"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Exposing Functions with Playwright",
    "codeDescription": "Demonstrates how to expose a function to the page context using Playwright's page.exposeFunction(). The example shows exposing and executing a simple message function.",
    "codeLanguage": "javascript",
    "codeTokens": 141,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/injecting_code.md#2025-04-18_snippet_3",
    "pageTitle": "Injecting Code in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\nawait page.goto('https://google.com');\n\nconst returnMessage = () => 'Apify academy!';\n\nawait page.exposeFunction(returnMessage.name, returnMessage);\n\nconst msg = await page.evaluate(() => returnMessage());\n\nconsole.log(msg);\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Implementing a Type Guard in TypeScript",
    "codeDescription": "This snippet shows how to use a type guard to safely assign an 'unknown' type to a specific type after runtime type checking.",
    "codeLanguage": "typescript",
    "codeTokens": 85,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/unknown_and_type_assertions.md#2025-04-18_snippet_2",
    "pageTitle": "Understanding Unknown, Any, Type Guards, and Type Assertions in TypeScript",
    "codeList": [
      {
        "language": "typescript",
        "code": "let userInput: unknown;\nlet savedInput: string;\n\nuserInput = 5;\n\nif (typeof userInput === 'string') {\n    savedInput = userInput;\n}"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Loading and Processing Weather Data with Pandas",
    "codeDescription": "This code loads the scraped weather data into a Pandas DataFrame, pivots it, and calculates rolling averages for temperature trends.",
    "codeLanguage": "python",
    "codeTokens": 131,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_3",
    "pageTitle": "Processing Scraped Data with Python",
    "codeList": [
      {
        "language": "python",
        "code": "print('Parsing weather data...')\ndataset_items_stream = dataset_client.stream_items(item_format='csv')\nweather_data = pandas.read_csv(dataset_items_stream, parse_dates=['datetime'], date_parser=lambda val: pandas.to_datetime(val, utc=True))\n\npivot = weather_data.pivot(index='datetime', columns='location', values='temperature')\nmean_daily_temperatures = pivot.rolling(window='24h', min_periods=24, center=True).mean()"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Handling Optional Dataset ID in JavaScript",
    "codeDescription": "This code snippet demonstrates a flexible approach to handle the dataset ID, allowing it to be provided either explicitly in the input or retrieved from the payload.",
    "codeLanguage": "javascript",
    "codeTokens": 78,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/actors/integration_ready_actors.md#2025-04-18_snippet_4",
    "pageTitle": "Creating Integration-Ready Actors for Apify",
    "codeList": [
      {
        "language": "javascript",
        "code": "const { payload, datasetId } = await Actor.getInput();\nconst datasetIdToProcess = datasetId || payload?.resource?.defaultDatasetId;"
      }
    ],
    "relevance": 0.903
  },
  {
    "codeTitle": "Managing Request Queues in JavaScript SDK",
    "codeDescription": "Illustrates various operations on request queues using the Apify JavaScript SDK. It includes adding requests, fetching requests, handling specific requests, and dropping a queue.",
    "codeLanguage": "javascript",
    "codeTokens": 220,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_5",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Import the JavaScript SDK into your project\nimport { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n\nconst queue = await Actor.openRequestQueue();\n\n// Enqueue requests\nawait queue.addRequests([{ url: 'http://example.com/aaa' }]);\nawait queue.addRequests(['http://example.com/foo', 'http://example.com/bar'], {\n    forefront: true,\n});\n\n// Get the next request from queue\nconst request1 = await queue.fetchNextRequest();\nconst request2 = await queue.fetchNextRequest();\n\n// Get a specific request\nconst specificRequest = await queue.getRequest('shi6Nh3bfs3');\n\n// Reclaim a failed request back to the queue\n// and process it again\nawait queue.reclaimRequest(request2);\n\n// Remove a queue\nawait queue.drop();\n\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Running Website Content Crawler to Extract Data from URLs",
    "codeDescription": "Uses Apify's Website Content Crawler to scrape content from specified URLs with a page limit. The scraped data is stored in an Apify dataset for later use.",
    "codeLanguage": "python",
    "codeTokens": 130,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_14",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "run_input = {\"startUrls\": [{\"url\": \"https://docs.apify.com/platform\"}], \"maxCrawlPages\": 10, \"crawlerType\": \"cheerio\"}\nactor_call_website_crawler = apify_client.actor(\"apify/website-content-crawler\").call(run_input=run_input)\n\ndataset_id = actor_call_website_crawler[\"defaultDatasetId\"]"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Configuring Country-Specific Residential Proxy in Apify SDK (Python)",
    "codeDescription": "Shows how to set up a residential proxy with a specific country (France) in the Apify SDK using Python.",
    "codeLanguage": "python",
    "codeTokens": 87,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/residential_proxy.md#2025-04-18_snippet_5",
    "pageTitle": "Residential Proxy Configuration and Usage Guide",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        # ...\n        proxy_configuration = await Actor.create_proxy_configuration(\n            groups=['RESIDENTIAL'],\n            country_code='FR',\n        )\n        # ..."
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Final Implementation with File Writing",
    "codeDescription": "Complete implementation including file system operations to save the CSV data to a file using Node.js fs module.",
    "codeLanguage": "javascript",
    "codeTokens": 243,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/save_to_csv.md#2025-04-18_snippet_4",
    "pageTitle": "CSV Data Export Guide for Web Scrapers",
    "codeList": [
      {
        "language": "javascript",
        "code": "// main.js\nimport { writeFileSync } from 'fs'; // <---- added a new import\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\nimport { parse } from 'json2csv';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst products = $('.product-item');\n\nconst results = [];\nfor (const product of products) {\n    const titleElement = $(product).find('a.product-item__title');\n    const title = titleElement.text().trim();\n\n    const priceElement = $(product).find('span.price');\n    const price = priceElement.contents()[2].nodeValue.trim();\n\n    results.push({ title, price });\n}\n\nconst csv = parse(results);\nwriteFileSync('products.csv', csv); // <---- added writing of CSV to file"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Exporting Product Data to JSON in Python",
    "codeDescription": "Uses the json module from Python's standard library to write the collected product data to a JSON file, handling Decimal serialization.",
    "codeLanguage": "python",
    "codeTokens": 102,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/08_saving_data.md#2025-04-18_snippet_2",
    "pageTitle": "Saving Data with Python in CSV and JSON Formats",
    "codeList": [
      {
        "language": "python",
        "code": "import json\nfrom decimal import Decimal\n\ndef serialize(obj):\n    if isinstance(obj, Decimal):\n        return str(obj)\n    raise TypeError(\"Object not JSON serializable\")\n\nwith open(\"products.json\", \"w\") as file:\n    json.dump(data, file, default=serialize)"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Loading Data from Apify Dataset into LlamaIndex Documents",
    "codeDescription": "Python code showing how to use the ApifyDataset class to load data from an existing Apify dataset and convert it into LlamaIndex Document objects. The code requires an Apify API token and a dataset ID.",
    "codeLanguage": "python",
    "codeTokens": 140,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/llama.md#2025-04-18_snippet_2",
    "pageTitle": "LlamaIndex Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "from llama_index.core import Document\nfrom llama_index.readers.apify import ApifyDataset\n\nreader = ApifyDataset(\"<My Apify API token>\")\ndocuments = reader.load_data(\n    dataset_id=\"my_dataset_id\",\n    dataset_mapping_function=lambda item: Document(\n        text=item.get(\"text\"),\n        metadata={\n            \"url\": item.get(\"url\"),\n        },\n    ),\n)"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Example Zappos Scraper Actor.json Implementation",
    "codeDescription": "A complete example of an actor.json file from the Zappos Scraper that demonstrates how to properly configure fields for display in the Overview table. It includes various field formats like image, text, boolean, and link.",
    "codeLanguage": "json",
    "codeTokens": 390,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/output_schema.md#2025-04-18_snippet_1",
    "pageTitle": "Dataset Schema Implementation for Apify Actors",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"actorSpecification\": 1,\n    \"name\": \"zappos-scraper\",\n    \"title\": \"Zappos Scraper\",\n    \"description\": \"\",\n    \"version\": \"1.0.0\",\n    \"storages\": {\n        \"dataset\": {\n            \"actorSpecification\": 1,\n            \"title\": \"Zappos.com Dataset\",\n            \"description\": \"\",\n            \"views\": {\n                \"products\": {\n                    \"title\": \"Overview\",\n                    \"description\": \"It can take about one minute until the first results are available.\",\n                    \"transformation\": {\n                        \"fields\": [\n                            \"imgUrl\",\n                            \"brand\",\n                            \"name\",\n                            \"SKU\",\n                            \"inStock\",\n                            \"onSale\",\n                            \"price\",\n                            \"url\"\n                        ]\n                    },\n                    \"display\": {\n                        \"component\": \"table\",\n                        \"properties\": {\n                            \"imgUrl\": {\n                                \"label\": \"Product image\",\n                                \"format\": \"image\"\n                            },\n                            \"url\": {\n                                \"label\": \"Link\",\n                                \"format\": \"link\"\n                            },\n                            \"brand\": {\n                                \"format\": \"text\"\n                            },\n                            \"name\": {\n                                \"format\": \"text\"\n                            },\n                            \"SKU\": {\n                                \"format\": \"text\"\n                            },\n                            \"inStock\": {\n                                \"format\": \"boolean\"\n                            },\n                            \"onSale\": {\n                                \"format\": \"boolean\"\n                            },\n                            \"price\": {\n                                \"format\": \"text\"\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Creating Residential Proxy Configuration in Apify SDK (JavaScript)",
    "codeDescription": "Demonstrates how to configure the Apify SDK in JavaScript to use residential proxies by setting the 'RESIDENTIAL' group in the proxy configuration.",
    "codeLanguage": "javascript",
    "codeTokens": 87,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/residential_proxy.md#2025-04-18_snippet_1",
    "pageTitle": "Residential Proxy Configuration and Usage Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n});\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Passing Variables to Browser Context in Puppeteer",
    "codeDescription": "This snippet demonstrates how to pass variables from the Node.js context to the browser context in Puppeteer, changing the page title to a random string.",
    "codeLanguage": "JavaScript",
    "codeTokens": 145,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/index.md#2025-04-18_snippet_4",
    "pageTitle": "Executing Scripts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\nconst params = { randomString: Math.random().toString(36).slice(2) };\n\nawait page.evaluate(({ randomString }) => {\n    document.querySelector('title').textContent = randomString;\n}, params);\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Implementing Request Queue Locking in Actor 1",
    "codeDescription": "This code demonstrates how to implement request queue locking in Actor 1 using the JavaScript Apify client. It creates a request queue, adds multiple requests, locks the first two requests, checks the lock expiration time, and shows how to prolong or delete a lock.",
    "codeLanguage": "JavaScript",
    "codeTokens": 481,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_11",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { Actor, ApifyClient } from 'apify';\n\nawait Actor.init();\n\nconst client = new ApifyClient({\n    token: 'MY-APIFY-TOKEN',\n});\n\n// Creates a new request queue.\nconst requestQueue = await client.requestQueues().getOrCreate('example-queue');\n\n// Creates two clients with different keys for the same request queue.\nconst requestQueueClient = client.requestQueue(requestQueue.id, {\n    clientKey: 'requestqueueone',\n});\n\n// Adds multiple requests to the queue.\nawait requestQueueClient.batchAddRequests([\n    {\n        url: 'http://example.com/foo',\n        uniqueKey: 'http://example.com/foo',\n        method: 'GET',\n    },\n    {\n        url: 'http://example.com/bar',\n        uniqueKey: 'http://example.com/bar',\n        method: 'GET',\n    },\n    {\n        url: 'http://example.com/baz',\n        uniqueKey: 'http://example.com/baz',\n        method: 'GET',\n    },\n    {\n        url: 'http://example.com/qux',\n        uniqueKey: 'http://example.com/qux',\n        method: 'GET',\n    },\n]);\n\n// Locks the first two requests at the head of the queue.\nconst processingRequestsClientOne = await requestQueueClient.listAndLockHead(\n    {\n        limit: 2,\n        lockSecs: 120,\n    },\n);\n\n// Checks when the lock will expire. The locked request will have a lockExpiresAt attribute.\nconst lockedRequest = processingRequestsClientOne.items[0];\nconst lockedRequestDetail = await requestQueueClient.getRequest(\n    lockedRequest.id,\n);\nconsole.log(`Request locked until ${lockedRequestDetail?.lockExpiresAt}`);\n\n// Prolongs the lock of the first request or unlocks it.\nawait requestQueueClient.prolongRequestLock(\n    lockedRequest.id,\n    { lockSecs: 120 },\n);\nawait requestQueueClient.deleteRequestLock(\n    lockedRequest.id,\n);\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Creating a ReAct Agent with Apify Tools",
    "codeDescription": "Configures a ReAct agent with the LLM and Apify Actors tools, preparing it to process requests that require web browsing and TikTok data extraction.",
    "codeLanguage": "python",
    "codeTokens": 69,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_4",
    "pageTitle": "LangGraph Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "tools = [browser, tiktok]\nagent_executor = create_react_agent(llm, tools)"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Configuring Country-Specific Residential Proxy in Apify SDK (JavaScript)",
    "codeDescription": "Demonstrates how to set up a residential proxy with a specific country (France) in the Apify SDK using JavaScript.",
    "codeLanguage": "javascript",
    "codeTokens": 88,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/residential_proxy.md#2025-04-18_snippet_4",
    "pageTitle": "Residential Proxy Configuration and Usage Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\nconst proxyConfiguration = await Actor.createProxyConfiguration({\n    groups: ['RESIDENTIAL'],\n    countryCode: 'FR',\n});\n// ...\nawait Actor.exit();"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Complete Actor Workflow Implementation",
    "codeDescription": "Full implementation showing how to run an actor and process its dataset items using the Apify client.",
    "codeLanguage": "multiple",
    "codeTokens": 123,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_4",
    "pageTitle": "Apify Client Implementation Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "// client.js\nimport { ApifyClient } from 'apify-client';\n\nconst client = new ApifyClient({\n    token: 'YOUR_TOKEN',\n});\n\nconst run = await client.actor('YOUR_USERNAME/adding-actor').call({\n    num1: 4,\n    num2: 2,\n});\n\nconst dataset = client.dataset(run.defaultDatasetId);\n\nconst { items } = await dataset.listItems();\n\nconsole.log(items);"
      },
      {
        "language": "python",
        "code": "# client.py\nfrom apify_client import ApifyClient\n\nclient = ApifyClient(token='YOUR_TOKEN')\n\nactor = client.actor('YOUR_USERNAME/adding-actor').call(run_input={\n    'num1': 4,\n    'num2': 2\n})\n\ndataset = client.dataset(run['defaultDatasetId'])\n\nitems = dataset.list_items().items\n\nprint(items)"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Extracting Text from iFrame Elements with Puppeteer",
    "codeDescription": "This snippet demonstrates how to extract text content from elements within an identified iFrame. It uses the $$eval method to select multiple elements and map their text content to an array.",
    "codeLanguage": "JavaScript",
    "codeTokens": 109,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/scraping_iframes.md#2025-04-18_snippet_2",
    "pageTitle": "Scraping iFrames with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "const textFeed = await twitterFrame.$$eval('.timeline-Tweet-text', (pElements) => pElements.map((elem) => elem.textContent));\n\nfor (const text of textFeed) {\n    console.log(text);\n    console.log('**********');\n}"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Triggering File Download by Clicking Button in Puppeteer",
    "codeDescription": "This code snippet shows how to trigger a file download by clicking a button on the page using Puppeteer.",
    "codeLanguage": "javascript",
    "codeTokens": 49,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_1",
    "pageTitle": "Downloading Files with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "await page.click('.export-button');"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Blocking Tracking Scripts in Puppeteer",
    "codeDescription": "Implementation example showing how to block specific tracking and analytics scripts. The code filters requests by URL patterns and aborts those containing specific tracking-related keywords.",
    "codeLanguage": "javascript",
    "codeTokens": 136,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/block_requests_puppeteer.md#2025-04-18_snippet_1",
    "pageTitle": "Block Requests in Puppeteer",
    "codeList": [
      {
        "language": "javascript",
        "code": "await page.setRequestInterception(true);\npage.on('request', (request) => {\n    const url = request.url();\n    const filters = [\n        'livefyre',\n        'moatad',\n        'analytics',\n        'controltag',\n        'chartbeat',\n    ];\n    const shouldAbort = filters.some((urlPart) => url.includes(urlPart));\n    if (shouldAbort) request.abort();\n    else request.continue();\n});"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Using Python 3 urllib with Apify Proxy Authentication",
    "codeDescription": "Shows how to use Python 3's urllib module with Apify's proxy authentication. It creates a ProxyHandler with the proxy URL containing authentication credentials and disables SSL certificate verification for HTTPS requests.",
    "codeLanguage": "python",
    "codeTokens": 200,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_9",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "python",
        "code": "import urllib.request as request\nimport ssl\n\n# Replace <YOUR_PROXY_PASSWORD> below with your password\n# found at https://console.apify.com/proxy\npassword = \"<YOUR_PROXY_PASSWORD>\"\nproxy_url = f\"http://auto:{password}@proxy.apify.com:8000\"\nproxy_handler = request.ProxyHandler({\n    \"http\": proxy_url,\n    \"https\": proxy_url,\n})\n\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\nhttpHandler = request.HTTPSHandler(context=ctx)\n\nopener = request.build_opener(httpHandler,proxy_handler)\nprint(opener.open(\"http://proxy.apify.com/?format=json\").read())"
      }
    ],
    "relevance": 0.897
  },
  {
    "codeTitle": "Reading Downloaded File from File System in Node.js",
    "codeDescription": "This code shows how to read a downloaded file from the file system using Node.js fs module. It lists files in the download directory and reads the first one.",
    "codeLanguage": "javascript",
    "codeTokens": 109,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_3",
    "pageTitle": "Downloading Files with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import fs from 'fs';\n\nconst fileNames = fs.readdirSync('./my-downloads');\n\n// Let's pick the first one\nconst fileData = fs.readFileSync(`./my-downloads/${fileNames[0]}`);\n\n// ...Now we can do whatever we want with the data"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Enabling Strict Type Checking in TypeScript",
    "codeDescription": "This snippet shows how to enable strict type checking in TypeScript by setting the 'strict' option to true in tsconfig.json.",
    "codeLanguage": "json",
    "codeTokens": 155,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_12",
    "pageTitle": "TypeScript Configuration Guide - Watch Mode and tsconfig.json",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"compilerOptions\": {\n        \"target\": \"esnext\",\n        \"lib\": [\"ES2015\", \"ES2016\", \"ES2018\", \"ES2019.Object\", \"ES2018.AsyncIterable\", \"ES2020.String\", \"ES2019.Array\"],\n        \"outDir\": \"dist/\",\n        \"removeComments\": true,\n        \"noEmitOnError\": true,\n        \"strict\": true\n    },\n    \"exclude\": [\"node_modules\"],\n    \"include\": [\"src/\"]\n}"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Adding Custom Proxy URLs in JavaScript SDK",
    "codeDescription": "Use the proxyConfiguration.newUrl(sessionId) method to add custom proxy URLs to the proxy configuration in the JavaScript SDK. This allows for the integration of personal proxies into Apify workflows.",
    "codeLanguage": "javascript",
    "codeTokens": 64,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/your_own_proxies.md#2025-04-18_snippet_0",
    "pageTitle": "Using Custom Proxies with Apify Platform",
    "codeList": [
      {
        "language": "javascript",
        "code": "proxyConfiguration.newUrl(sessionId)"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Implementing Main Function for Running Scraper in TypeScript",
    "codeDescription": "This snippet defines the main function that initializes user input, calls the scrape function, and logs the result.",
    "codeLanguage": "typescript",
    "codeTokens": 87,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/mini_project.md#2025-04-18_snippet_6",
    "pageTitle": "TypeScript Mini-Project Documentation",
    "codeList": [
      {
        "language": "typescript",
        "code": "// index.ts\n\n// ...\nconst main = async () => {\n    const INPUT: UserInput<true> = { sort: 'ascending', removeImages: true };\n\n    const result = await scrape(INPUT);\n\n    console.log(result);\n};"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Adding Properties to Input Schema in JSON",
    "codeDescription": "Extends the basic schema by adding a 'properties' object that defines the input fields the Actor expects. Each property includes a title and description to help users understand what input is required.",
    "codeLanguage": "json",
    "codeTokens": 133,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/input_schema.md#2025-04-18_snippet_1",
    "pageTitle": "Input Schema Documentation for Actor Configuration",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"title\": \"Adding Actor input\",\n    \"description\": \"Add all values in list of numbers with an arbitrary length.\",\n    \"type\": \"object\",\n    \"schemaVersion\": 1,\n    \"properties\": {\n        \"numbers\": {\n            \"title\": \"Number list\",\n            \"description\": \"The list of numbers to add up.\"\n        }\n    }\n}"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Creating Requests for Amazon Product Offers",
    "codeDescription": "This code snippet shows how to create requests for Amazon product offer pages. It extracts the product description and adds a new request to the crawler's queue for offer data.",
    "codeLanguage": "JavaScript",
    "codeTokens": 167,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/scraping_amazon.md#2025-04-18_snippet_1",
    "pageTitle": "Building an Amazon Web Scraper with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "router.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {\n    const { data } = request.userData;\n\n    const element = $('div#productDescription');\n\n    // Add to the request queue\n    await crawler.addRequests([{\n        url: `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${data.asin}&pc=dp`,\n        label: labels.OFFERS,\n        userData: {\n            data: {\n                ...data,\n                description: element.text().trim(),\n            },\n        },\n    }]);\n});"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Product Data Parser Function",
    "codeDescription": "Function that parses product information from a BeautifulSoup element, extracting title and price data.",
    "codeLanguage": "python",
    "codeTokens": 155,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_2",
    "pageTitle": "Python Web Scraping with BeautifulSoup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "def parse_product(product):\n    title = product.select_one(\".product-item__title\").text.strip()\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price}"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Implementing ACTOR_MAX_PAID_DATASET_ITEMS Check in JavaScript",
    "codeDescription": "This code snippet demonstrates how to implement a check for the maximum number of paid dataset items in an Actor to prevent excess result generation in Pay-per-result (PPR) pricing model.",
    "codeLanguage": "javascript",
    "codeTokens": 183,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/publishing/monetize.mdx#2025-04-18_snippet_0",
    "pageTitle": "Monetizing Actors in Apify Store",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Example code for implementing ACTOR_MAX_PAID_DATASET_ITEMS check\n// This is a simplified version, actual implementation may vary\nconst Apify = require('apify');\n\nApify.main(async () => {\n    const maxItems = process.env.ACTOR_MAX_PAID_DATASET_ITEMS;\n    const dataset = await Apify.openDataset();\n    const itemCount = await dataset.getInfo().then(info => info.itemCount);\n\n    if (maxItems && itemCount >= maxItems) {\n        console.log(`Reached maximum of ${maxItems} items. Stopping the Actor.`);\n        return;\n    }\n\n    // Your Actor logic here\n});"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Initializing Apify Client",
    "codeDescription": "Create a new instance of ApifyClient with an API token for authentication.",
    "codeLanguage": "multiple",
    "codeTokens": 48,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_2",
    "pageTitle": "Apify Client Implementation Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "const client = new ApifyClient({\n    token: 'YOUR_TOKEN',\n});"
      },
      {
        "language": "python",
        "code": "client = ApifyClient(token='YOUR_TOKEN')"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "jQuery Injection in Pre-goto Function",
    "codeDescription": "Alternative approach to inject jQuery using the pre-goto function hook before page navigation.",
    "codeLanguage": "javascript",
    "codeTokens": 62,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/puppeteer_scraper.md#2025-04-18_snippet_10",
    "pageTitle": "Scraping with Puppeteer Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function preGotoFunction({ page, Apify }) {\n    await Apify.utils.puppeteer.injectJQuery(page);\n}"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Defining Full Actor Configuration in JSON",
    "codeDescription": "A comprehensive example of an `actor.json` file, showcasing all available configuration options for an Apify Actor including name, version, memory requirements, environment variables, and file paths.",
    "codeLanguage": "json",
    "codeTokens": 196,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/actor_json.md#2025-04-18_snippet_0",
    "pageTitle": "Configuring Actor Settings with actor.json",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"actorSpecification\": 1,\n    \"name\": \"name-of-my-scraper\",\n    \"version\": \"0.0\",\n    \"buildTag\": \"latest\",\n    \"minMemoryMbytes\": 256,\n    \"maxMemoryMbytes\": 4096,\n    \"environmentVariables\": {\n        \"MYSQL_USER\": \"my_username\",\n        \"MYSQL_PASSWORD\": \"@mySecretPassword\"\n    },\n    \"usesStandbyMode\": false,\n    \"dockerfile\": \"./Dockerfile\",\n    \"readme\": \"./ACTOR.md\",\n    \"input\": \"./input_schema.json\",\n    \"storages\": {\n        \"dataset\": \"./dataset_schema.json\"\n    }\n}"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Initializing LLM and Apify Actors Tools",
    "codeDescription": "Creates instances of the OpenAI chat model and Apify Actors tools for web browsing and TikTok data extraction, which will be used by the ReAct agent.",
    "codeLanguage": "python",
    "codeTokens": 99,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_3",
    "pageTitle": "LangGraph Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nbrowser = ApifyActorsTool(\"apify/rag-web-browser\")\ntiktok = ApifyActorsTool(\"clockworks/free-tiktok-scraper\")"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Passing Variables to Browser Context in Playwright",
    "codeDescription": "This snippet shows how to pass variables from the Node.js context to the browser context in Playwright, changing the page title to a random string.",
    "codeLanguage": "JavaScript",
    "codeTokens": 144,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/index.md#2025-04-18_snippet_3",
    "pageTitle": "Executing Scripts in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { chromium } from 'playwright';\n\nconst browser = await chromium.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://www.google.com/');\n\nconst params = { randomString: Math.random().toString(36).slice(2) };\n\nawait page.evaluate(({ randomString }) => {\n    document.querySelector('title').textContent = randomString;\n}, params);\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Implementing Request Queue Locking in Actor 2",
    "codeDescription": "This code demonstrates how Actor 2 interacts with the same request queue that Actor 1 is using. It shows how the locking mechanism prevents concurrent processing of the same request by multiple Actor runs, and how trying to modify a lock owned by a different client throws an error.",
    "codeLanguage": "JavaScript",
    "codeTokens": 484,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_12",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "import { Actor, ApifyClient } from 'apify';\n\nawait Actor.init();\n\nconst client = new ApifyClient({\n    token: 'MY-APIFY-TOKEN',\n});\n\n// Waits for the first Actor to lock the requests.\nawait new Promise((resolve) => setTimeout(resolve, 5000));\n\n// Get the same request queue in different Actor run and with a different client key.\nconst requestQueue = await client.requestQueues().getOrCreate('example-queue');\n\nconst requestQueueClient = client.requestQueue(requestQueue.id, {\n    clientKey: 'requestqueuetwo',\n});\n\n// Get all requests from the queue and check one locked by the first Actor.\nconst requests = await requestQueueClient.listRequests();\nconst requestsLockedByAnotherRun = requests.items.filter((request) => request.lockByClient === 'requestqueueone');\nconst requestLockedByAnotherRunDetail = await requestQueueClient.getRequest(\n    requestsLockedByAnotherRun[0].id,\n);\n\n// Other clients cannot list and lock these requests; the listAndLockHead call returns other requests from the queue.\nconst processingRequestsClientTwo = await requestQueueClient.listAndLockHead(\n    {\n        limit: 10,\n        lockSecs: 60,\n    },\n);\nconst wasBothRunsLockedSameRequest = !!processingRequestsClientTwo.items.find(\n    (request) => request.id === requestLockedByAnotherRunDetail.id,\n);\n\nconsole.log(`Was the request locked by the first run locked by the second run? ${wasBothRunsLockedSameRequest}`);\nconsole.log(`Request locked until ${requestLockedByAnotherRunDetail?.lockExpiresAt}`);\n\n// Other clients cannot modify the lock; attempting to do so will throw an error.\ntry {\n    await requestQueueClient.prolongRequestLock(\n        requestLockedByAnotherRunDetail.id,\n        { lockSecs: 60 },\n    );\n} catch (err) {\n    // This will throw an error.\n}\n\n// Cleans up the queue.\nawait requestQueueClient.delete();\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Displaying Cookie Information Table in Markdown",
    "codeDescription": "This snippet presents a markdown table listing various cookies used on the Apify docs website. It includes cookie names, descriptions, types (strictly necessary or performance), and expiration periods in days.",
    "codeLanguage": "markdown",
    "codeTokens": 816,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/legal/latest/policies/cookie-policy.md#2025-04-18_snippet_0",
    "pageTitle": "Apify Cookie Policy Documentation",
    "codeList": [
      {
        "language": "markdown",
        "code": "| Cookie name                              | Cookie description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Type               | Expiration (in days) |\n|------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|----------------------|\n| AWSALB                                   | AWS ELB application load balancer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Strictly necessary | 6                    |\n| OptanonConsent                           | This cookie is set by the cookie compliance solution from OneTrust. It stores information about the categories of cookies the site uses and whether visitors have given or withdrawn consent for the use of each category. This enables site owners to prevent cookies in each category from being set in the user's browser, when consent is not given. The cookie has a normal lifespan of one year, so that returning visitors to the site will have their preferences remembered. It contains no information that can identify the site visitor. | Strictly necessary | 364                  |\n| AWSALBCORS                               | This cookie is managed by AWS and is used for load balancing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Strictly necessary | 6                    |\n| ApifyProdUserId                          | This cookie is created by Apify after a user signs into their account and is used across Apify domains to identify if the user is signed in.                                                                                                                                                                                                                                                                                                                                                                                                         | Strictly necessary | 0                    |\n| ApifyProdUser                            | This cookie is created by Apify after a user signs into their account and is used across Apify domains to identify if the user is signed in.                                                                                                                                                                                                                                                                                                                                                                                                         | Strictly necessary | 0                    |\n| intercom-id-kod1r788                     | This cookie is used by Intercom service to identify user sessions for customer support chat.                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Strictly necessary | 270                  |\n| intercom-session-kod1r788                | This cookie is used by Intercom service to identify user sessions for customer support chat.                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Strictly necessary | 6                    |\n| \\_gaexp\\_rc                              | \\_ga                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Performance        | 0                    |\n| \\_hjTLDTest                              | When the Hotjar script executes we try to determine the most generic cookie path we should use, instead of the page hostname. This is done so that cookies can be shared across subdomains (where applicable). To determine this, we try to store the \\_hjTLDTest cookie for different URL substring alternatives until it fails. After this check, the cookie is removed.                                                                                                                                                                           | Performance        | 0                    |\n| \\_hjSessionUser\\_1441872                 | Hotjar cookie that is set when a user first lands on a page with the Hotjar script. It is used to persist the Hotjar User ID, unique to that site on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID.                                                                                                                                                                                                                                                                           | Performance        | 364                  |\n| \\_hjIncludedInPageviewSample             | This cookie is set to let Hotjar know whether that visitor is included in the data sampling defined by your site's pageview limit.                                                                                                                                                                                                                                                                                                                                                                                                                   | Performance        | 0                    |\n| \\_ga                                     | This cookie name is associated with Google Universal Analytics - which is a significant update to Google's more commonly used analytics service. This cookie is used to distinguish unique users by assigning a randomly generated number as a client identifier. It is included in each page request in a site and used to calculate visitor, session and campaign data for the sites analytics reports. By default it is set to expire after 2 years, although this is customisable by website owners. \\_ga                                        | Performance        | 729                  |"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Core Product Scraping Functions",
    "codeDescription": "Main Python script containing functions for downloading pages, parsing products, and exporting data to CSV and JSON formats. Includes functionality for downloading product listings and extracting basic product information.",
    "codeLanguage": "python",
    "codeTokens": 361,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/10_crawling.md#2025-04-18_snippet_0",
    "pageTitle": "Building Web Crawlers with Python and HTTPX",
    "codeList": [
      {
        "language": "python",
        "code": "import httpx\nfrom bs4 import BeautifulSoup\nfrom decimal import Decimal\nimport csv\nimport json\nfrom urllib.parse import urljoin\n\ndef download(url):\n    response = httpx.get(url)\n    response.raise_for_status()\n\n    html_code = response.text\n    return BeautifulSoup(html_code, \"html.parser\")\n\ndef parse_product(product, base_url):\n    title_element = product.select_one(\".product-item__title\")\n    title = title_element.text.strip()\n    url = urljoin(base_url, title_element[\"href\"])\n\n    price_text = (\n        product\n        .select_one(\".price\")\n        .contents[-1]\n        .strip()\n        .replace(\"$\", \"\")\n        .replace(\",\", \"\")\n    )\n    if price_text.startswith(\"From \"):\n        min_price = Decimal(price_text.removeprefix(\"From \"))\n        price = None\n    else:\n        min_price = Decimal(price_text)\n        price = min_price\n\n    return {\"title\": title, \"min_price\": min_price, \"price\": price, \"url\": url}\n\ndef export_csv(file, data):\n    fieldnames = list(data[0].keys())\n    writer = csv.DictWriter(file, fieldnames=fieldnames)\n    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\ndef export_json(file, data):\n    def serialize(obj):\n        if isinstance(obj, Decimal):\n            return str(obj)\n        raise TypeError(\"Object not JSON serializable\")\n\n    json.dump(data, file, default=serialize, indent=2)"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Taking Screenshot with Puppeteer/Playwright",
    "codeDescription": "This code snippet shows how to take a screenshot of a web page using the page.screenshot() method in Puppeteer and Playwright. It saves the screenshot as a PNG file in the project folder.",
    "codeLanguage": "javascript",
    "codeTokens": 84,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/page_methods.md#2025-04-18_snippet_1",
    "pageTitle": "Page Methods in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Take the screenshot and write it to the filesystem\nawait page.screenshot({ path: 'screenshot.png' });"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Replacing Shadow DOM Content with HTML",
    "codeDescription": "This snippet iterates through all elements in the main DOM and replaces any shadow DOM content with its HTML. This allows access to the content using standard DOM manipulation methods.",
    "codeLanguage": "javascript",
    "codeTokens": 111,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/scraping_shadow_doms.md#2025-04-18_snippet_1",
    "pageTitle": "How to Scrape Sites with a Shadow DOM",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Iterate over all elements in the main DOM.\nfor (const el of document.getElementsByTagName('*')) {\n    // If element contains shadow root then replace its\n    // content with the HTML of shadow DOM.\n    if (el.shadowRoot) el.innerHTML = el.shadowRoot.innerHTML;\n}"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Example of Dataset Record with Hidden Fields",
    "codeDescription": "Illustrates a dataset record with hidden fields (prefixed with '#') including HTTP response and error details. These fields can be excluded when downloading data using query parameters.",
    "codeLanguage": "json",
    "codeTokens": 108,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_15",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"url\": \"https://example.com\",\n    \"title\": \"Example page\",\n    \"data\": {\n        \"foo\": \"bar\"\n    },\n    \"#error\": null,\n    \"#response\": {\n        \"statusCode\": 201\n    }\n}"
      }
    ],
    "relevance": 0.892
  },
  {
    "codeTitle": "Specific Product Link Selection",
    "codeDescription": "Selecting product links using specific CSS class selectors to target product titles.",
    "codeLanguage": "javascript",
    "codeTokens": 41,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/filtering_links.md#2025-04-18_snippet_2",
    "pageTitle": "Link Filtering in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "document.querySelectorAll('a.product-item__title');"
      },
      {
        "language": "javascript",
        "code": "$('a.product-item__title');"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Setting Status Message in JavaScript with Apify SDK",
    "codeDescription": "Demonstrates how to update an Actor's status message using the JavaScript SDK. The code initializes an Actor, sets a custom status message, and then exits. The SDK optimizes API calls by only invoking them when the status actually changes.",
    "codeLanguage": "javascript",
    "codeTokens": 100,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/status_messages.md#2025-04-18_snippet_0",
    "pageTitle": "Setting Status Messages in Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n\n// ...\nawait Actor.setStatusMessage('Crawled 45 of 100 pages');\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Defining RAG Web Browser Function for OpenAI Assistant",
    "codeDescription": "This snippet defines the function description for the RAG Web Browser, which will be used by the OpenAI Assistant to retrieve search results.",
    "codeLanguage": "python",
    "codeTokens": 189,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_4",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "rag_web_browser_function = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"call_rag_web_browser\",\n        \"description\": \"Query Google search, scrape the top N pages from the results, and returns their cleaned content as markdown\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": { \"type\": \"string\", \"description\": \"Use regular search words or enter Google Search URLs. \"},\n                \"maxResults\": {\"type\": \"integer\", \"description\": \"The number of top organic search results to return and scrape text from\"}\n            },\n            \"required\": [\"query\"]\n        }\n    }\n}"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Using RAG Web Browser Actor",
    "codeDescription": "Example of using Apify's RAG Web Browser Actor to crawl and scrape content from Google Search results for use with LangChain.",
    "codeLanguage": "python",
    "codeTokens": 119,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langchain.md#2025-04-18_snippet_6",
    "pageTitle": "LangChain Integration with Apify Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "loader = apify.call_actor(\n    actor_id=\"apify/rag-web-browser\",\n    run_input={\"query\": \"apify langchain web browser\", \"maxResults\": 3},\n    dataset_mapping_function=lambda item: Document(page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"metadata\"][\"url\"]}),\n)\nprint(\"Documents:\", loader.load())"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Initiating Conversation with OpenAI Assistant",
    "codeDescription": "This snippet demonstrates how to start a conversation with the OpenAI Assistant by creating a thread, adding a message, and running the assistant.",
    "codeLanguage": "python",
    "codeTokens": 110,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_7",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "thread = client.beta.threads.create()\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id, role=\"user\", content=\"What are the latest LLM news?\"\n)\n\nrun = client.beta.threads.runs.create_and_poll(thread_id=thread.id, assistant_id=my_assistant.id)"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Scraper Actor Input Schema",
    "codeDescription": "JSON schema defining the input structure for the Scraper Actor, which includes the IDs for the shared request queue and dataset.",
    "codeLanguage": "json",
    "codeTokens": 52,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_8",
    "pageTitle": "Parallel Website Scraping with Multiple Actor Runs",
    "codeList": [
      {
        "language": "json",
        "code": "// ScraperActorInputSchemaJson reference"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Complete Link Scraping with URL Resolution",
    "codeDescription": "Complete script showing how to scrape product links and properly resolve relative URLs to absolute URLs using Node.js and Cheerio.",
    "codeLanguage": "javascript",
    "codeTokens": 177,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/relative_urls.md#2025-04-18_snippet_3",
    "pageTitle": "Working with Relative URLs in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst productLinks = $('a.product-item__title');\n\nfor (const link of productLinks) {\n    const relativeUrl = $(link).attr('href');\n    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n    console.log(absoluteUrl.href);\n}"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Running an Actor",
    "codeDescription": "Example of running an Apify actor with input parameters using the client.",
    "codeLanguage": "multiple",
    "codeTokens": 62,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_3",
    "pageTitle": "Apify Client Implementation Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "const run = await client.actor('YOUR_USERNAME/adding-actor').call({\n    num1: 4,\n    num2: 2,\n});"
      },
      {
        "language": "python",
        "code": "run = client.actor('YOUR_USERNAME/adding-actor').call(run_input={\n    'num1': 4,\n    'num2': 2\n})"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Complete Crawler with URL Queue",
    "codeDescription": "Extended crawler implementation that includes URL queue management and execution",
    "codeLanguage": "javascript",
    "codeTokens": 115,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_4",
    "pageTitle": "Professional Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request }) => {\n        console.log('URL:', request.url);\n        console.log('Title:', $('h1').text().trim());\n    },\n});\n\nawait crawler.addRequests([\n    'https://warehouse-theme-metal.myshopify.com/collections/sales',\n]);\n\nawait crawler.run();"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Using Configuration Class in JavaScript",
    "codeDescription": "Illustrates how to use the Configuration class in JavaScript for more convenient access to Actor configuration.",
    "codeLanguage": "JavaScript",
    "codeTokens": 91,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/environment_variables.md#2025-04-18_snippet_2",
    "pageTitle": "Actor Environment Variables in Apify",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n\n// get current token\nconst token = Actor.config.get('token');\n// use different token\nActor.config.set('token', 's0m3n3wt0k3n');\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Scraper Actor Input Handling",
    "codeDescription": "Code for the Scraper Actor to retrieve and use the request queue and dataset IDs provided by the Orchestrator Actor, establishing the connection between the two Actors.",
    "codeLanguage": "typescript",
    "codeTokens": 125,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_7",
    "pageTitle": "Parallel Website Scraping with Multiple Actor Runs",
    "codeList": [
      {
        "language": "typescript",
        "code": "import { Actor } from 'apify';\n\ninterface Input {\n    requestQueueId: string;\n    datasetId: string;\n}\n\nconst {\n    requestQueueId,\n    datasetId,\n} = await Actor.getInput<Input>() ?? {} as Input;\n\nconst requestQueue = await Actor.openRequestQueue(requestQueueId);\nconst dataset = await Actor.openDataset(datasetId);"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Adding Request-Specific Statistics to Dataset Items",
    "codeDescription": "Enhances the request handler to include additional statistics in each dataset item, such as the date handled, number of retries, and current pending requests.",
    "codeLanguage": "javascript",
    "codeTokens": 258,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/saving_stats.md#2025-04-18_snippet_4",
    "pageTitle": "Saving Run Statistics for Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "router.addHandler(labels.OFFERS, async ({ $, request }) => {\n    const { data } = request.userData;\n\n    const { asin } = data;\n\n    for (const offer of $('#aod-offer')) {\n        tracker.incrementASIN(asin);\n        // Add 1 to totalSaved for every offer\n        Stats.success();\n\n        const element = $(offer);\n\n        await dataset.pushData({\n            ...data,\n            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\n            offer: element.find('.a-price .a-offscreen').text().trim(),\n            // Store the handledAt date or current date if that is undefined\n            dateHandled: request.handledAt || new Date().toISOString(),\n            // Access the number of retries on the request object\n            numberOfRetries: request.retryCount,\n            // Grab the number of pending requests from the requestQueue\n            currentPendingRequests: (await requestQueue.getInfo()).pendingRequestCount,\n        });\n    }\n});"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Initializing LLM and Apify Tools",
    "codeDescription": "Creating instances of the LLM and Apify Actor tools for web browsing and TikTok scraping",
    "codeLanguage": "python",
    "codeTokens": 92,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_3",
    "pageTitle": "CrewAI Integration with Apify Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n\nbrowser_tool = ApifyActorsTool(actor_name=\"apify/rag-web-browser\")\ntiktok_tool = ApifyActorsTool(actor_name=\"clockworks/free-tiktok-scraper\")"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Implementing Element Wait in Puppeteer",
    "codeDescription": "Example showing how to wait for elements to be present on a Google search page before interaction using Puppeteer's waitForSelector method.",
    "codeLanguage": "javascript",
    "codeTokens": 156,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/waiting.md#2025-04-18_snippet_0",
    "pageTitle": "Browser Automation: Waiting for Content and Events",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\nconst page = await browser.newPage();\nawait page.goto('https://www.google.com/');\n\nawait page.click('button + button');\n\nawait page.type('textarea[title]', 'hello world');\nawait page.keyboard.press('Enter');\n\n// Wait for the element to be present on the page prior to clicking it\nawait page.waitForSelector('.g a');\nawait page.click('.g a');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Intercepting File Download Request in Puppeteer",
    "codeDescription": "This code demonstrates how to intercept a file download request in Puppeteer. It waits for a request to be sent and then aborts it, capturing the request details.",
    "codeLanguage": "javascript",
    "codeTokens": 101,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/downloading_files.md#2025-04-18_snippet_5",
    "pageTitle": "Downloading Files with Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "const xRequest = await new Promise((resolve) => {\n    page.on('request', (interceptedRequest) => {\n        interceptedRequest.abort(); // stop intercepting requests\n        resolve(interceptedRequest);\n    });\n});"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Initializing Base Variables for GitHub Repository Scraping",
    "codeDescription": "Sets up initial variables for storing scraped data and defining base URLs for GitHub repository scraping.",
    "codeLanguage": "javascript",
    "codeTokens": 69,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_0",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "const repositories = [];\nconst BASE_URL = 'https://github.com';\nconst REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Setting Up Request Interception in Puppeteer",
    "codeDescription": "Basic code to set up request interception in Puppeteer. This snippet shows how to enable request interception and define conditional logic to either abort or continue requests.",
    "codeLanguage": "javascript",
    "codeTokens": 80,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/block_requests_puppeteer.md#2025-04-18_snippet_0",
    "pageTitle": "Block Requests in Puppeteer",
    "codeList": [
      {
        "language": "javascript",
        "code": "await page.setRequestInterception(true);\npage.on('request', (request) => {\n    if (someCondition) request.abort();\n    else request.continue();\n});"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Implementing E-commerce Product Scraper in Node.js",
    "codeDescription": "A complete Node.js script that scrapes product information from a Shopify store. It uses got-scraping for HTTP requests, Cheerio for HTML parsing, and json2csv for data conversion. The script extracts product titles and prices from the sales collection page and saves them to a CSV file.",
    "codeLanguage": "javascript",
    "codeTokens": 512,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/recap_extraction_basics.md#2025-04-18_snippet_0",
    "pageTitle": "Data Extraction Guide for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "// First, we imported all the libraries we needed to\n// download, extract, and convert the data we wanted\nimport { writeFileSync } from 'fs';\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\nimport { parse } from 'json2csv';\n\n// Here, we fetched the website's HTML and saved it to a new variable.\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// We used Cheerio, a popular library, to parse (process)\n// the downloaded HTML so that we could manipulate it.\nconst $ = cheerio.load(html);\n\n// Using the .product-item CSS selector, we collected all the HTML\n// elements which contained data about individual products.\nconst products = $('.product-item');\n\n// Then, we prepared a new array to store the results.\nconst results = [];\n\n// And looped over all the elements to extract\n// information about the individual products.\nfor (const product of products) {\n    // The product's title was in an <a> element\n    // with the CSS class: product-item__title\n    const titleElement = $(product).find('a.product-item__title');\n    const title = titleElement.text().trim();\n    // The product's price was in a <span> element\n    // with the CSS class: price\n    const priceElement = $(product).find('span.price');\n    // Because the <span> also included some useless data,\n    // we had to extract the price from a specific HTML node.\n    const price = priceElement.contents()[2].nodeValue.trim();\n\n    // We added the data to the results array\n    // in the form of an object with keys and values.\n    results.push({ title, price });\n}\n\n// Finally, we formatted the results\n// as a CSV file instead of a JS object\nconst csv = parse(results);\n\n// Then, we saved the CSV to the disk\nwriteFileSync('products.csv', csv);"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Pre-injecting Scripts with Puppeteer",
    "codeDescription": "Shows how to inject custom code before page load using Puppeteer's page.evaluateOnNewDocument(). The example demonstrates overriding the native addEventListener function.",
    "codeLanguage": "javascript",
    "codeTokens": 124,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/injecting_code.md#2025-04-18_snippet_1",
    "pageTitle": "Injecting Code in Puppeteer and Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.evaluateOnNewDocument(() => {\n    // Override the prototype\n    Node.prototype.addEventListener = null;\n});\n\nawait page.goto('https://google.com');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.887
  },
  {
    "codeTitle": "Setting Status Message in Python with Apify SDK",
    "codeDescription": "Shows how to update an Actor's status message using the Python SDK. The code uses an async context manager to handle Actor initialization and sets a custom status message within the Actor's execution context.",
    "codeLanguage": "python",
    "codeTokens": 105,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/status_messages.md#2025-04-18_snippet_1",
    "pageTitle": "Setting Status Messages in Apify Actors",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        await Actor.set_status_message('Crawled 45 of 100 pages')\n        # INFO  [Status message]: Crawled 45 of 100 pages"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Using Non-null Assertion in TypeScript",
    "codeDescription": "This snippet shows how to use the non-null assertion operator to remove null and undefined from a value's type, asserting its presence.",
    "codeLanguage": "typescript",
    "codeTokens": 67,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/unknown_and_type_assertions.md#2025-04-18_snippet_4",
    "pageTitle": "Understanding Unknown, Any, Type Guards, and Type Assertions in TypeScript",
    "codeList": [
      {
        "language": "typescript",
        "code": "let job: undefined | string;\n\nconst chars = job!.split('');"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Opening New Page with Puppeteer",
    "codeDescription": "Shows how to launch a browser instance and create a new page using Puppeteer. The browser is launched in non-headless mode for visibility.",
    "codeLanguage": "javascript",
    "codeTokens": 91,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/page/index.md#2025-04-18_snippet_1",
    "pageTitle": "Opening and Controlling Pages in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\n\n// Open a new page\nconst page = await browser.newPage();\n\nawait browser.close();"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Initializing ASINTracker in main.js with Apify SDK in JavaScript",
    "codeDescription": "Updates the main.js file to initialize the ASINTracker at the beginning of the Actor's run. It ensures that the Actor.init() function is executed before the tracker's initialization.",
    "codeLanguage": "JavaScript",
    "codeTokens": 95,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/handling_migrations.md#2025-04-18_snippet_4",
    "pageTitle": "Handling Migrations in Apify Actors",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "// main.js\n\n// ...\nimport tracker from './asinTracker';\n\n// The Actor.init() function should be executed before\n// the tracker's initialization\nawait Actor.init();\n\nawait tracker.initialize();\n// ..."
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Querying Single Element with JavaScript",
    "codeDescription": "Demonstrates how to use document.querySelector() to find the first element matching a CSS selector on a webpage.",
    "codeLanguage": "javascript",
    "codeTokens": 47,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/using_devtools.md#2025-04-18_snippet_0",
    "pageTitle": "Using Browser DevTools for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "document.querySelector('.product-item');"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Running Compiled JavaScript with Node.js",
    "codeDescription": "Execute the compiled JavaScript file using Node.js to see the output of the TypeScript code.",
    "codeLanguage": "shell",
    "codeTokens": 40,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/installation.md#2025-04-18_snippet_6",
    "pageTitle": "Installing TypeScript and Writing First TS Code",
    "codeList": [
      {
        "language": "shell",
        "code": "node first-lines.js"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Interacting with OpenAI Assistant to Query Stored Data",
    "codeDescription": "Creates a thread, sends a user message, runs the assistant with vector search, and displays the assistant's response. This demonstrates how the assistant uses the vector store data to answer questions.",
    "codeLanguage": "python",
    "codeTokens": 166,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_16",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "thread = client.beta.threads.create()\nmessage = client.beta.threads.messages.create(\n    thread_id=thread.id, role=\"user\", content=\"How can I scrape a website using Apify?\"\n)\n\nrun = client.beta.threads.runs.create_and_poll(\n    thread_id=thread.id,\n    assistant_id=assistant.id,\n    tool_choice={\"type\": \"file_search\"}\n)\n\nprint(\"Assistant response:\")\nfor m in client.beta.threads.messages.list(thread_id=run.thread_id):\n    print(m.content[0].text.value)"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Actor Exit Operations in JavaScript",
    "codeDescription": "Shows various ways to terminate an Actor in JavaScript, including successful exits, immediate exits, and failure scenarios.",
    "codeLanguage": "javascript",
    "codeTokens": 77,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_8",
    "pageTitle": "Basic Apify SDK Commands Documentation",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n// Actor will finish with 'SUCCEEDED' status\nawait Actor.exit('Succeeded, crawled 50 pages');"
      },
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n// Exit right away without calling `exit` handlers at all\nawait Actor.exit('Done right now', { timeoutSecs: 0 });"
      },
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n// Actor will finish with 'FAILED' status\nawait Actor.exit('Could not finish the crawl, try increasing memory', { exitCode: 1 });"
      },
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\n\nawait Actor.init();\n// ...\n// Or nicer way using this syntactic sugar:\nawait Actor.fail('Could not finish the crawl, try increasing memory');"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Defining Prefilled Input for Integration in JSON",
    "codeDescription": "This JSON snippet shows how to define a prefilled input for an Actor when it's being used as an integration, automatically using the default dataset ID of the triggering run.",
    "codeLanguage": "json",
    "codeTokens": 67,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/actors/integration_ready_actors.md#2025-04-18_snippet_2",
    "pageTitle": "Creating Integration-Ready Actors for Apify",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"datasetId\": \"{{resource.defaultDatasetId}}\"\n}"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Loading Scraped Data into OpenAI Vector Store using Integration Actor",
    "codeDescription": "Processes the scraped website data from the Apify dataset and loads it into the OpenAI Vector Store using the Vector Store Integration actor, making it accessible to the assistant.",
    "codeLanguage": "python",
    "codeTokens": 138,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_15",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "run_input_vs = {\n    \"datasetId\": dataset_id,\n    \"assistantId\": my_assistant.id,\n    \"datasetFields\": [\"text\", \"url\"],\n    \"openaiApiKey\": \"YOUR-OPENAI-API-KEY\",\n    \"vectorStoreId\": vector_store.id,\n}\n\napify_client.actor(\"jiri.spilka/openai-vector-store-integration\").call(run_input=run_input_vs)"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Implementing Error Handling in Cheerio Crawler",
    "codeDescription": "Configures the Cheerio Crawler to use an error handler that tracks errors using the Stats utility class. This handler is called for every failed request.",
    "codeLanguage": "javascript",
    "codeTokens": 171,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/saving_stats.md#2025-04-18_snippet_2",
    "pageTitle": "Saving Run Statistics for Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "const crawler = new CheerioCrawler({\n    proxyConfiguration,\n    useSessionPool: true,\n    sessionPoolOptions: {\n        persistStateKey: 'AMAZON-SESSIONS',\n        sessionOptions: {\n            maxUsageCount: 5,\n            maxErrorScore: 1,\n        },\n    },\n    maxConcurrency: 50,\n    requestHandler: router,\n    // Handle all failed requests\n    errorHandler: async ({ error, request }) => {\n        // Add an error for this url to our error tracker\n        Stats.addError(request.url, error?.message);\n    },\n});"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Inspecting Scraped Results in Browser Console",
    "codeDescription": "Shows how to inspect the collected results in browser console by simply entering the variable name, which displays a formatted representation of the scraped data.",
    "codeLanguage": "javascript",
    "codeTokens": 78,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/debugging_web_scraper.md#2025-04-18_snippet_4",
    "pageTitle": "Debugging Web Scraper PageFunction in Browser Console",
    "codeList": [
      {
        "language": "javascript",
        "code": "results;\n// Will log a nicely formatted [{ title: 'my-article-1'}, { title: 'my-article-2'}] etc."
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Applying Type Assertion in TypeScript",
    "codeDescription": "This example demonstrates how to use type assertion to tell TypeScript to treat a value as a specific type, bypassing compiler checks.",
    "codeLanguage": "typescript",
    "codeTokens": 77,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/unknown_and_type_assertions.md#2025-04-18_snippet_3",
    "pageTitle": "Understanding Unknown, Any, Type Guards, and Type Assertions in TypeScript",
    "codeList": [
      {
        "language": "typescript",
        "code": "let userInput: unknown;\nlet savedInput: string;\n\nuserInput = 'hello world!';\n\nsavedInput = userInput as string;"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Logging Extracted Product Data to Console with JavaScript",
    "codeDescription": "This simple command logs the entire results array, containing extracted product data, to the console for review.",
    "codeLanguage": "JavaScript",
    "codeTokens": 46,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/devtools_continued.md#2025-04-18_snippet_3",
    "pageTitle": "Extracting Data with DevTools in Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "console.log(results);"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Structuring Output JSON for Amazon Product Offers",
    "codeDescription": "Illustrates the expected output format for scraped Amazon product offers, including product details and offer-specific information.",
    "codeLanguage": "json",
    "codeTokens": 413,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/challenge/index.md#2025-04-18_snippet_2",
    "pageTitle": "Building an Amazon Scraper with Crawlee",
    "codeList": [
      {
        "language": "json",
        "code": "[\n    {\n        \"title\": \"Apple iPhone 6 a1549 16GB Space Gray Unlocked (Certified Refurbished)\",\n        \"asin\": \"B07P6Y7954\",\n        \"itemUrl\": \"https://www.amazon.com/Apple-iPhone-Unlocked-Certified-Refurbished/dp/B00YD547Q6/ref=sr_1_2?s=wireless&ie=UTF8&qid=1539772626&sr=1-2&keywords=iphone\",\n        \"description\": \"What's in the box: Certified Refurbished iPhone 6 Space Gray 16GB Unlocked , USB Cable/Adapter. Comes in a Generic Box with a 1 Year Limited Warranty.\",\n        \"keyword\": \"iphone\",\n        \"seller name\": \"Blutek Intl\",\n        \"offer\": \"$162.97\"\n    },\n    {\n        \"title\": \"Apple iPhone 6 a1549 16GB Space Gray Unlocked (Certified Refurbished)\",\n        \"asin\": \"B07P6Y7954\",\n        \"itemUrl\": \"https://www.amazon.com/Apple-iPhone-Unlocked-Certified-Refurbished/dp/B00YD547Q6/ref=sr_1_2?s=wireless&ie=UTF8&qid=1539772626&sr=1-2&keywords=iphone\",\n        \"description\": \"What's in the box: Certified Refurbished iPhone 6 Space Gray 16GB Unlocked , USB Cable/Adapter. Comes in a Generic Box with a 1 Year Limited Warranty.\",\n        \"keyword\": \"iphone\",\n        \"sellerName\": \"PLATINUM DEALS\",\n        \"offer\": \"$169.98\"\n    },\n    {\n        \"...\": \"...\"\n    }\n]"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Loading and Processing Weather Data with Pandas",
    "codeDescription": "Loads the scraped weather data into a pandas DataFrame, transforms it into a pivot table, and calculates rolling averages for temperature trends.",
    "codeLanguage": "python",
    "codeTokens": 154,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/scrape_data_python.md#2025-04-18_snippet_11",
    "pageTitle": "Python Web Scraping with Beautiful Soup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "# Load the dataset items into a pandas dataframe\nprint('Parsing weather data...')\ndataset_items_stream = dataset_client.stream_items(item_format='csv')\nweather_data = pandas.read_csv(dataset_items_stream, parse_dates=['datetime'], date_parser=lambda val: pandas.to_datetime(val, utc=True))\n\n# Transform data to a pivot table for easier plotting\npivot = weather_data.pivot(index='datetime', columns='location', values='temperature')\nmean_daily_temperatures = pivot.rolling(window='24h', min_periods=24, center=True).mean()"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Sharing Request Queues Between Runs in Python",
    "codeDescription": "This code shows how to access a request queue from another Actor or task run in Python. You can open a request queue using its name or ID and then use it like any other request queue.",
    "codeLanguage": "Python",
    "codeTokens": 92,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_14",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "Python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        other_queue = await Actor.open_request_queue(name='old-queue')\n        # ..."
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Starting a Simple Flask Web Server in an Apify Actor",
    "codeDescription": "This code snippet shows how to create a basic web server using Flask within an Apify Actor. It sets up a single route that responds with 'Hello world' and runs the server on the port specified by the ACTOR_WEB_SERVER_PORT environment variable.",
    "codeLanguage": "Python",
    "codeTokens": 232,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/container_web_server.md#2025-04-18_snippet_1",
    "pageTitle": "Running a Container Web Server in Apify Actors",
    "codeList": [
      {
        "language": "python",
        "code": "# pip install flask\nimport asyncio\nimport os\nfrom apify import Actor\nfrom apify_shared.consts import ActorEnvVars\nfrom flask import Flask\n\nasync def main():\n    async with Actor:\n        # Create a Flask app\n        app = Flask(__name__)\n\n        # Define a route\n        @app.route('/')\n        def hello_world():\n            return 'Hello world from Flask app!'\n\n        # Log the public URL\n        url = os.environ.get(ActorEnvVars.WEB_SERVER_URL)\n        Actor.log.info(f'Web server is listening and can be accessed at {url}')\n\n        # Start the web server\n        port = os.environ.get(ActorEnvVars.WEB_SERVER_PORT)\n        app.run(host='0.0.0.0', port=port)"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "GraphQL Query Example",
    "codeDescription": "Example of a GraphQL query structure with variables for fetching repository data.",
    "codeLanguage": "graphql",
    "codeTokens": 69,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/index.md#2025-04-18_snippet_1",
    "pageTitle": "API Scraping Guide",
    "codeList": [
      {
        "language": "graphql",
        "code": "query($number_of_repos: Int!) {\n  viewer {\n    name\n     repositories(last: $number_of_repos) {\n       nodes {\n         name\n       }\n     }\n   }\n}"
      }
    ],
    "relevance": 0.882
  },
  {
    "codeTitle": "Implementing Tool Output Submission for OpenAI Assistant",
    "codeDescription": "This function handles the submission of tool outputs to the OpenAI Assistant, specifically for the RAG Web Browser function calls.",
    "codeLanguage": "python",
    "codeTokens": 188,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_8",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "def submit_tool_outputs(run_):\n    \"\"\" Submit tool outputs to continue the run \"\"\"\n    tool_output = []\n    for tool in run_.required_action.submit_tool_outputs.tool_calls:\n        if tool.function.name == \"call_rag_web_browser\":\n            d = json.loads(tool.function.arguments)\n            output = call_rag_web_browser(query=d[\"query\"], max_results=d[\"maxResults\"])\n            tool_output.append(ToolOutput(tool_call_id=tool.id, output=json.dumps(output)))\n            print(\"RAG Web Browser added as a tool output.\")\n\n    return client.beta.threads.runs.submit_tool_outputs_and_poll(thread_id=run_.thread_id, run_id=run_.id, tool_outputs=tool_output)"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "JSON Export Function with Decimal Handling",
    "codeDescription": "Function that exports data to a JSON file with proper handling of Decimal types and formatted indentation.",
    "codeLanguage": "python",
    "codeTokens": 86,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/09_getting_links.md#2025-04-18_snippet_4",
    "pageTitle": "Python Web Scraping with BeautifulSoup Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "def export_json(file, data):\n    def serialize(obj):\n        if isinstance(obj, Decimal):\n            return str(obj)\n        raise TypeError(\"Object not JSON serializable\")\n\n    json.dump(data, file, default=serialize, indent=2)"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Scraping App Token using Puppeteer in JavaScript",
    "codeDescription": "Function to scrape the 'X-App-Token' from Cheddar's website using Puppeteer for authentication in API requests.",
    "codeLanguage": "javascript",
    "codeTokens": 224,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/api_scraping/graphql_scraping/custom_queries.md#2025-04-18_snippet_1",
    "pageTitle": "Custom GraphQL Queries for API Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst scrapeAppToken = async () => {\n    const browser = await puppeteer.launch();\n    const page = await browser.newPage();\n\n    let appToken = null;\n\n    page.on('response', async (res) => {\n        // grab the token from the request headers\n        const token = res.request().headers()?.['x-app-token'];\n\n        // if there is a token, grab it and close the browser\n        if (token) {\n            appToken = token;\n            await browser.close();\n        }\n    });\n\n    await page.goto('https://www.cheddar.com/');\n\n    await page.waitForNetworkIdle();\n\n    // otherwise, close the browser after networkidle\n    // has been fired\n    await browser.close();\n\n    // return the apptoken (or null)\n    return appToken;\n};\n\nexport default scrapeAppToken;"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Accessing Environment Variables in JavaScript",
    "codeDescription": "Demonstrates how to access environment variables in Node.js using the process.env object.",
    "codeLanguage": "JavaScript",
    "codeTokens": 43,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/environment_variables.md#2025-04-18_snippet_0",
    "pageTitle": "Actor Environment Variables in Apify",
    "codeList": [
      {
        "language": "javascript",
        "code": "console.log(process.env.APIFY_USER_ID);"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Defining CrewAI Agents",
    "codeDescription": "Creating agent definitions with specific roles, goals, and tools for web search and TikTok analysis",
    "codeLanguage": "python",
    "codeTokens": 151,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/crewai.md#2025-04-18_snippet_4",
    "pageTitle": "CrewAI Integration with Apify Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "search_agent = Agent(\n    role=\"Web Search Specialist\",\n    goal=\"Find the TikTok profile URL on the web\",\n    backstory=\"Expert in web searching and data retrieval\",\n    tools=[browser_tool],\n    llm=llm,\n    verbose=True\n)\n\nanalysis_agent = Agent(\n    role=\"TikTok Profile Analyst\",\n    goal=\"Extract and analyze data from the TikTok profile\",\n    backstory=\"Skilled in social media data extraction and analysis\",\n    tools=[tiktok_tool],\n    llm=llm,\n    verbose=True\n)"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Running TypeScript Compiler in Watch Mode",
    "codeDescription": "This shell command demonstrates how to run the TypeScript compiler in watch mode, which automatically recompiles files when changes are detected.",
    "codeLanguage": "shell",
    "codeTokens": 51,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/watch_mode_and_tsconfig.md#2025-04-18_snippet_14",
    "pageTitle": "TypeScript Configuration Guide - Watch Mode and tsconfig.json",
    "codeList": [
      {
        "language": "shell",
        "code": "tsc -w"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Basic Puppeteer Setup for Web Scraping",
    "codeDescription": "Initial setup code for launching Puppeteer browser and navigating to target page. Creates a new browser instance and page object, then navigates to a demo webstore.",
    "codeLanguage": "javascript",
    "codeTokens": 117,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/executing_scripts/extracting_data.md#2025-04-18_snippet_1",
    "pageTitle": "Data Extraction Guide for Playwright and Puppeteer",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.goto('https://demo-webstore.apify.org/search/on-sale');\n\n// code will go here\n\nawait page.waitForTimeout(10000);\n\nawait browser.close();"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Key-Value Store Operations in Python",
    "codeDescription": "Shows basic key-value store operations in Python including saving and retrieving objects.",
    "codeLanguage": "python",
    "codeTokens": 129,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/basic_commands.md#2025-04-18_snippet_5",
    "pageTitle": "Basic Apify SDK Commands Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        # Save object to store (stringified to JSON)\n        await Actor.set_value('my_state', {'something': 123})\n\n        # Get a record from the store (automatically parsed from JSON)\n        value = await Actor.get_value('my_state')\n\n        # Log the obtained value\n        Actor.log.info(f'value = {value}')\n        # prints: value = {'something': 123}"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Importing Required Packages for OpenAI Assistant and Apify Integration",
    "codeDescription": "This code imports necessary Python modules for working with OpenAI's API, Apify's client, and handling JSON data.",
    "codeLanguage": "python",
    "codeTokens": 83,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_1",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "import json\nimport time\n\nfrom apify_client import ApifyClient\nfrom openai import OpenAI, Stream\nfrom openai.types.beta.threads.run_submit_tool_outputs_params import ToolOutput"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Importing Required Packages for LangGraph-Apify Integration",
    "codeDescription": "Imports necessary Python packages for creating a LangGraph agent with Apify Actors tools, including modules for API tools, messages, LLM access, and ReAct agent creation.",
    "codeLanguage": "python",
    "codeTokens": 96,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_1",
    "pageTitle": "LangGraph Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "import os\n\nfrom langchain_apify import ApifyActorsTool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Paginated Request Processing",
    "codeDescription": "Implementation of promise-based pagination handling for scraping multiple pages of results.",
    "codeLanguage": "javascript",
    "codeTokens": 151,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/paginating_through_results.md#2025-04-18_snippet_4",
    "pageTitle": "Web Scraping Pagination Guide - GitHub Repository Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "const pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);\nconst promises = pageNumbers.map((pageNumber) => (async () => {\n    const paginatedPage = await browser.newPage();\n\n    const url = new URL(REPOSITORIES_URL);\n    url.searchParams.set('page', pageNumber);\n\n    await paginatedPage.goto(url.href);\n    const results = await scrapeRepos(paginatedPage);\n\n    repositories.push(...results);\n\n    await paginatedPage.close();\n})(),\n);\nawait Promise.all(promises);\n\nconsole.log(repositories.length);"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "JSON Output Example for Scraped Product Data",
    "codeDescription": "An example of the JSON output structure for the scraped product data, showing how variant information, prices, and other details are formatted.",
    "codeLanguage": "json",
    "codeTokens": 229,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/11_scraping_variants.md#2025-04-18_snippet_3",
    "pageTitle": "Scraping Product Variants with Python",
    "codeList": [
      {
        "language": "json",
        "code": "[\n  {\n    \"variant_name\": \"Red\",\n    \"title\": \"Sony XB-950B1 Extra Bass Wireless Headphones with App Control\",\n    \"min_price\": \"128.00\",\n    \"price\": \"178.00\",\n    \"url\": \"https://warehouse-theme-metal.myshopify.com/products/sony-xb950-extra-bass-wireless-headphones-with-app-control\",\n    \"vendor\": \"Sony\"\n  },\n  {\n    \"variant_name\": \"Black\",\n    \"title\": \"Sony XB-950B1 Extra Bass Wireless Headphones with App Control\",\n    \"min_price\": \"128.00\",\n    \"price\": \"178.00\",\n    \"url\": \"https://warehouse-theme-metal.myshopify.com/products/sony-xb950-extra-bass-wireless-headphones-with-app-control\",\n    \"vendor\": \"Sony\"\n  }\n]"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Demonstrating Type Safety Issues in JavaScript",
    "codeDescription": "This example shows common JavaScript type-related problems where undefined values and incorrect type conversions can lead to runtime errors that TypeScript would catch during development.",
    "codeLanguage": "javascript",
    "codeTokens": 118,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/index.md#2025-04-18_snippet_0",
    "pageTitle": "TypeScript Introduction Guide for JavaScript Developers",
    "codeList": [
      {
        "language": "javascript",
        "code": "const john = {\n    name: 'john',\n    job: 'web developer',\n};\n\nconst bob = {\n    name: 'bob',\n    job: 'data analyst',\n    age: '27',\n};\n\nconst addAges = (num1, num2) => num1 + num2;\n\nconsole.log(addAges(bob.age, john.age));"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Installing Crawlee Dependencies",
    "codeDescription": "Shell commands to initialize a new project and install the necessary Crawlee dependencies.",
    "codeLanguage": "shell",
    "codeTokens": 68,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/dealing_with_dynamic_pages.md#2025-04-18_snippet_1",
    "pageTitle": "Scraping Dynamic Pages with Crawlee",
    "codeList": [
      {
        "language": "shell",
        "code": "# this command will initialize your project\n# and install the \"crawlee\" and \"cheerio\" packages\nnpm init -y && npm i crawlee"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Setting Environment Variables for API Authentication",
    "codeDescription": "Sets environment variables for the OpenAI API key and Apify API token, which are required for authentication with these services.",
    "codeLanguage": "python",
    "codeTokens": 74,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/langgraph.md#2025-04-18_snippet_2",
    "pageTitle": "LangGraph Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API key\"\nos.environ[\"APIFY_API_TOKEN\"] = \"Your Apify API token\""
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Importing Python Libraries for Data Processing",
    "codeDescription": "This code imports the required Python libraries for working with Apify, processing data with Pandas, and handling I/O operations.",
    "codeLanguage": "python",
    "codeTokens": 74,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/python/process_data_using_python.md#2025-04-18_snippet_1",
    "pageTitle": "Processing Scraped Data with Python",
    "codeList": [
      {
        "language": "python",
        "code": "from io import BytesIO\nimport os\n\nfrom apify_client import ApifyClient\nfrom apify_client.consts import ActorJobStatus\nimport pandas"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Simple Number Addition in JavaScript",
    "codeDescription": "A JavaScript function that takes multiple numbers as arguments and returns their sum using the reduce method. The function demonstrates basic arithmetic operations that will be converted into an Apify Actor.",
    "codeLanguage": "javascript",
    "codeTokens": 103,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/index.md#2025-04-18_snippet_0",
    "pageTitle": "Deploying Code to Apify Platform Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "// index.js\nconst addAllNumbers = (...nums) => nums.reduce((total, curr) => total + curr, 0);\n\nconsole.log(addAllNumbers(1, 2, 3, 4)); // -> 10"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Performing Batch Operations with Request Queues in JavaScript",
    "codeDescription": "This code demonstrates how to perform batch operations on request queues using the JavaScript Apify client. It shows how to add multiple requests to a queue and how to delete multiple requests from a queue in a single API call.",
    "codeLanguage": "JavaScript",
    "codeTokens": 231,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_9",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "const { ApifyClient } = require('apify-client');\n\nconst client = new ApifyClient({\n    token: 'MY-APIFY-TOKEN',\n});\n\nconst requestQueueClient = client.requestQueue('my-queue-id');\n\n// Add multiple requests to the queue\nawait requestQueueClient.batchAddRequests([\n    {\n        url: 'http://example.com/foo',\n        uniqueKey: 'http://example.com/foo',\n        method: 'GET',\n    },\n    {\n        url: 'http://example.com/bar',\n        uniqueKey: 'http://example.com/bar',\n        method: 'GET',\n    },\n]);\n\n// Remove multiple requests from the queue\nawait requestQueueClient.batchDeleteRequests([\n    { uniqueKey: 'http://example.com/foo' },\n    { uniqueKey: 'http://example.com/bar' },\n]);"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Importing Apify Client",
    "codeDescription": "Import statements for the Apify client in both Node.js and Python environments.",
    "codeLanguage": "multiple",
    "codeTokens": 48,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/getting_started/apify_client.md#2025-04-18_snippet_1",
    "pageTitle": "Apify Client Implementation Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "// client.js\nimport { ApifyClient } from 'apify-client';"
      },
      {
        "language": "python",
        "code": "# client.py\nfrom apify_client import ApifyClient"
      }
    ],
    "relevance": 0.877
  },
  {
    "codeTitle": "Creating an OpenAI Assistant with File Search Tool",
    "codeDescription": "Creates an OpenAI Assistant configured as a customer support agent with the file_search tool enabled. The assistant uses the gpt-4o-mini model.",
    "codeLanguage": "python",
    "codeTokens": 108,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/integrations/ai/openai_assistants.md#2025-04-18_snippet_12",
    "pageTitle": "OpenAI Assistants Integration with Apify",
    "codeList": [
      {
        "language": "python",
        "code": "my_assistant = client.beta.assistants.create(\n    instructions=\"As a customer support agent at Apify, your role is to assist customers\",\n    name=\"Support assistant\",\n    tools=[{\"type\": \"file_search\"}],\n    model=\"gpt-4o-mini\",\n)"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Configuring Start URLs Array Input for Apify Actor in JSON",
    "codeDescription": "This snippet shows how to configure a start URLs array input for an Apify Actor. It uses the 'requestListSources' editor and includes a prefill URL.",
    "codeLanguage": "json",
    "codeTokens": 103,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/input_schema/specification.md#2025-04-18_snippet_11",
    "pageTitle": "Actor Input Schema Specification",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"title\": \"Start URLs\",\n    \"type\": \"array\",\n    \"description\": \"URLs to start with\",\n    \"prefill\": [{ \"url\": \"https://apify.com\" }],\n    \"editor\": \"requestListSources\"\n}"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Debugging Proxy Usage in Crawlee",
    "codeDescription": "This snippet shows how to access and log proxy information for debugging purposes in a Crawlee scraper.",
    "codeLanguage": "javascript",
    "codeTokens": 107,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/mitigation/using_proxies.md#2025-04-18_snippet_3",
    "pageTitle": "Using Proxies in Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "const crawler = new CheerioCrawler({\n    proxyConfiguration,\n    // Destructure \"proxyInfo\" from the \"context\" object\n    handlePageFunction: async ({ $, request, proxyInfo }) => {\n        // Log its value\n        console.log(proxyInfo);\n        // ...\n        // ...\n    },\n});"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Rust Actor Dockerfile Example",
    "codeDescription": "An example Dockerfile for a Rust Actor. It uses a multi-stage build approach to cache dependencies, copies only what's needed for dependency resolution first, then builds the actual application. The CMD instruction specifies how to run the compiled executable.",
    "codeLanguage": "Dockerfile",
    "codeTokens": 236,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/deploying_your_code/docker_file.md#2025-04-18_snippet_4",
    "pageTitle": "Writing Dockerfiles for Apify Actors",
    "codeList": [
      {
        "language": "Dockerfile",
        "code": "# Image with prebuilt Rust. We use the newest 1.* version\n# https://hub.docker.com/_/rust\nFROM rust:1\n\n# We copy only package setup so we cache building all dependencies\nCOPY Cargo* ./\n\n# We need to have dummy main.rs file to be able to build\nRUN mkdir src && echo \"fn main() {}\" > src/main.rs\n\n# Build dependencies only\n# Since we do this before copying  the rest of the files,\n# the dependencies will be cached by Docker, allowing fast\n# build times for new code changes\nRUN cargo build --release\n\n# Delete dummy main.rs\nRUN rm -rf src\n\n# Copy rest of the files\nCOPY . ./\n\n# Build the source files\nRUN cargo build --release\n\nCMD [\"./target/release/actor-example\"]"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Using PuppeteerCrawler with Proxy Configuration and Session Management",
    "codeDescription": "Demonstrates how to use PuppeteerCrawler with proxy configuration and session pooling. The crawler makes requests to proxy.apify.com endpoints while using a single session to maintain the same IP address across requests.",
    "codeLanguage": "javascript",
    "codeTokens": 169,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/proxy/datacenter_proxy.md#2025-04-18_snippet_4",
    "pageTitle": "Datacenter Proxy for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { Actor } from 'apify';\nimport { PuppeteerCrawler } from 'crawlee';\n\nawait Actor.init();\n\nconst proxyConfiguration = await Actor.createProxyConfiguration();\n\nconst crawler = new PuppeteerCrawler({\n    proxyConfiguration,\n    sessionPoolOptions: { maxPoolSize: 1 },\n    async requestHandler({ page }) {\n        console.log(await page.content());\n    },\n});\n\nawait crawler.run([\n    'https://proxy.apify.com/?format=json',\n    'https://proxy.apify.com',\n]);\n\nawait Actor.exit();"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Array Type Definition in TypeScript",
    "codeDescription": "Demonstrates how to define array types in TypeScript objects using the string[] syntax.",
    "codeLanguage": "typescript",
    "codeTokens": 110,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/using_types_continued.md#2025-04-18_snippet_2",
    "pageTitle": "TypeScript Core Types Tutorial - Objects, Arrays, and Tuples",
    "codeList": [
      {
        "language": "typescript",
        "code": "const course: {\n    name: string;\n    currentLesson: string;\n    typesLearned: string[];\n    learningBasicTypes?: boolean;\n} = {\n    name: 'Switching to TypeScript',\n    currentLesson: 'Using types - II',\n    typesLearned: ['number', 'boolean', 'string', 'object'],\n};"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Extracting Text from HTML Heading with Python and Beautiful Soup",
    "codeDescription": "This snippet shows how to extract and print only the text content of the first h1 tag found on the page.",
    "codeLanguage": "python",
    "codeTokens": 63,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_python/05_parsing_html.md#2025-04-18_snippet_2",
    "pageTitle": "Parsing HTML with Python for Web Scraping",
    "codeList": [
      {
        "language": "python",
        "code": "headings = soup.select(\"h1\")\nfirst_heading = headings[0]\nprint(first_heading.text)"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Using Configuration Class in Python",
    "codeDescription": "Demonstrates how to use the Configuration class in Python for more convenient access to Actor configuration.",
    "codeLanguage": "Python",
    "codeTokens": 117,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/environment_variables.md#2025-04-18_snippet_3",
    "pageTitle": "Actor Environment Variables in Apify",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        old_token = Actor.config.token\n        Actor.log.info(f'old_token = {old_token}')\n\n        # use different token\n        Actor.config.token = 's0m3n3wt0k3n'\n\n        new_token = Actor.config.token\n        Actor.log.info(f'new_token = {new_token}')"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Defining and Using a Union Type Alias in TypeScript",
    "codeDescription": "This snippet demonstrates how to create a type alias for a union type and use it in function parameters and variable declarations. It improves code readability by avoiding repetition of complex type annotations.",
    "codeLanguage": "typescript",
    "codeTokens": 107,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/type_aliases.md#2025-04-18_snippet_0",
    "pageTitle": "TypeScript Type Aliases and Function Types",
    "codeList": [
      {
        "language": "typescript",
        "code": "type MyUnionType = string | number | boolean;\n\nconst returnValueAsString = (value: MyUnionType) => {\n    return `${value}`;\n};\n\nlet myValue: MyUnionType;\n\nmyValue = 55;\n\nconsole.log(returnValueAsString(myValue));"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Adding Data to Default Dataset in Python",
    "codeDescription": "Shows how to use the Apify Python SDK to add single and multiple items to the default dataset in an Actor.",
    "codeLanguage": "python",
    "codeTokens": 110,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_12",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "python",
        "code": "from apify import Actor\n\nasync def main():\n    async with Actor:\n        # Add one item to the default dataset\n        await Actor.push_data({'foo': 'bar'})\n\n        # Add multiple items to the default dataset\n        await Actor.push_data([{'foo': 'hotel'}, {'foo': 'cafe'}])"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Filling Form Inputs with Puppeteer in JavaScript",
    "codeDescription": "This code demonstrates how to fill in form inputs using Puppeteer's page.type() method. It shows how to enter text into multiple input fields identified by their name attributes.",
    "codeLanguage": "javascript",
    "codeTokens": 99,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_5",
    "pageTitle": "Submitting a Form with File Attachment in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "await page.type('input[name=firstName]', 'John');\nawait page.type('input[name=surname]', 'Doe');\nawait page.type('input[name=email]', 'john.doe@example.com');"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Extending Interfaces in TypeScript",
    "codeDescription": "This example illustrates how to use the 'extends' keyword to inherit properties from one interface to another. It creates an 'Employee' interface that extends the 'Person' interface, adding an 'occupation' property.",
    "codeLanguage": "typescript",
    "codeTokens": 87,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/typescript/interfaces.md#2025-04-18_snippet_2",
    "pageTitle": "TypeScript Interfaces",
    "codeList": [
      {
        "language": "typescript",
        "code": "interface Person {\n    name: string;\n    age: number;\n    hobbies: string[];\n}\n\ninterface Employee extends Person {\n    occupation: string;\n}"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Defining Minimal Actor Configuration in JSON",
    "codeDescription": "A minimal example of an `actor.json` file, showing only the required fields for configuring an Apify Actor: the specification version, name, and version number.",
    "codeLanguage": "json",
    "codeTokens": 82,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/actor_json.md#2025-04-18_snippet_1",
    "pageTitle": "Configuring Actor Settings with actor.json",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"actorSpecification\": 1,\n    \"name\": \"name-of-my-scraper\",\n    \"version\": \"0.0\"\n}"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Extracting Title with jQuery in Web Scraper",
    "codeDescription": "Uses jQuery to select and extract the title text from a webpage header. This function captures the title from an h1 element inside a header element.",
    "codeLanguage": "javascript",
    "codeTokens": 93,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/apify_scrapers/web_scraper.md#2025-04-18_snippet_0",
    "pageTitle": "Scraping with Web Scraper Tutorial",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Using jQuery.\nasync function pageFunction(context) {\n    const { jQuery: $ } = context;\n\n    // ... rest of the code\n    return {\n        title: $('header h1').text(),\n    };\n}"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Creating ASINTracker Utility Class in JavaScript",
    "codeDescription": "Defines an ASINTracker class to store, modify, and log tracked ASIN data. It includes methods for incrementing ASIN counts and exports an instance of the class.",
    "codeLanguage": "JavaScript",
    "codeTokens": 224,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/handling_migrations.md#2025-04-18_snippet_0",
    "pageTitle": "Handling Migrations in Apify Actors",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "// asinTracker.js\nclass ASINTracker {\n    constructor() {\n        this.state = {};\n\n        // Log the state to the console every ten\n        // seconds\n        setInterval(() => console.log(this.state), 10000);\n    }\n\n    // Add an offer to the ASIN's offer count\n    // If ASIN doesn't exist yet, set it to 0\n    incrementASIN(asin) {\n        if (this.state[asin] === undefined) {\n            this.state[asin] = 0;\n            return;\n        }\n\n        this.state[asin] += 1;\n    }\n}\n\n// It is only a utility class, so we will immediately\n// create an instance of it and export that. We only\n// need one instance for our use case.\nmodule.exports = new ASINTracker();"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Injecting Moment.js Library Using Plain JavaScript in Web Scraper",
    "codeDescription": "This code snippet demonstrates how to inject the Moment.js library into a Web Scraper page function using plain JavaScript. It creates a script element, sets its source to the Moment.js CDN, and appends it to the document body. After loading, it uses Moment.js to format the current date and time.",
    "codeLanguage": "javascript",
    "codeTokens": 191,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/add_external_libraries_web_scraper.md#2025-04-18_snippet_0",
    "pageTitle": "Adding External Libraries to Web Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const libraryUrl = 'https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.24.0/moment.min.js';\n\n    // Inject Moment.js\\\n    await new Promise((resolve) => {\n        const script = document.createElement('script');\n        script.src = libraryUrl;\n        script.addEventListener('load', resolve);\n        document.body.append(script);\n    });\n\n    // Confirm that it works.\\\n    const now = moment().format('ddd, hA');\n    context.log.info(`NOW: ${now}`);\n}"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Accessing Element Content with JavaScript in Chrome DevTools Console",
    "codeDescription": "This snippet demonstrates how to interact with a webpage element using JavaScript in the Chrome DevTools Console. It shows how to access the text content and HTML of an element, as well as how to modify the element's text.",
    "codeLanguage": "javascript",
    "codeTokens": 69,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/data_extraction/browser_devtools.md#2025-04-18_snippet_0",
    "pageTitle": "Introduction to Browser DevTools for Web Scraping",
    "codeList": [
      {
        "language": "javascript",
        "code": "temp1.textContent;"
      },
      {
        "language": "javascript",
        "code": "temp1.outerHTML;"
      },
      {
        "language": "javascript",
        "code": "temp1.textContent = 'Hello World!';"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Handling Validation Errors in Python",
    "codeDescription": "Shows how to catch and handle dataset validation errors in Python using try-except blocks with the Apify SDK.",
    "codeLanguage": "python",
    "codeTokens": 78,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/actor_definition/dataset_schema/validation.md#2025-04-18_snippet_2",
    "pageTitle": "Dataset Schema Validation in Apify Actors",
    "codeList": [
      {
        "language": "python",
        "code": "try:\n    await Actor.push_data(items)\nexcept ApifyApiError as error:\n    if \"invalidItems\" in error.data:\n        validation_errors = e.data[\"invalidItems\"]"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Saving Downloaded File to Disk using fs/promises in JavaScript",
    "codeDescription": "This code demonstrates how to save a downloaded file to disk using the fs/promises module in JavaScript. It writes the fileData to a specified file path.",
    "codeLanguage": "javascript",
    "codeTokens": 66,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/common_use_cases/submitting_a_form_with_a_file_attachment.md#2025-04-18_snippet_3",
    "pageTitle": "Submitting a Form with File Attachment in Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "await fs.writeFile('./file.pdf', fileData);"
      }
    ],
    "relevance": 0.872
  },
  {
    "codeTitle": "Opening Request Queue and Dataset in Orchestrator",
    "codeDescription": "Code to open a request queue and dataset in the Orchestrator Actor, which will be shared with the scraper Actor runs for storing results and managing requests.",
    "codeLanguage": "typescript",
    "codeTokens": 79,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/multiple-runs-scrape.md#2025-04-18_snippet_3",
    "pageTitle": "Parallel Website Scraping with Multiple Actor Runs",
    "codeList": [
      {
        "language": "typescript",
        "code": "import { Actor } from 'apify';\n\nconst requestQueue = await Actor.openRequestQueue();\nconst dataset = await Actor.openDataset();"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Basic Crawler Implementation",
    "codeDescription": "Implementation of a basic crawler that visits URLs and extracts page titles using CheerioCrawler",
    "codeLanguage": "javascript",
    "codeTokens": 96,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/scraping_basics_javascript/crawling/pro_scraping.md#2025-04-18_snippet_3",
    "pageTitle": "Professional Web Scraping with Crawlee",
    "codeList": [
      {
        "language": "javascript",
        "code": "import { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n    requestHandler: async ({ $, request }) => {\n        console.log('URL:', request.url);\n        console.log('Title:', $('h1').text().trim());\n    },\n});"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Modifying Request URL with Puppeteer",
    "codeDescription": "Shows how to intercept and modify a request URL in Puppeteer, changing the destination from one SoundCloud profile to another.",
    "codeLanguage": "javascript",
    "codeTokens": 214,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/puppeteer_playwright/reading_intercepting_requests.md#2025-04-18_snippet_10",
    "pageTitle": "Reading and Intercepting Network Requests with Puppeteer/Playwright",
    "codeList": [
      {
        "language": "javascript",
        "code": "import puppeteer from 'puppeteer';\n\nconst browser = await puppeteer.launch({ headless: false });\nconst page = await browser.newPage();\n\nawait page.setRequestInterception(true);\n\n// Listen for all requests\npage.on('request', async (req) => {\n    // If it doesn't match, continue the route normally\n    if (!/soundcloud.com\\/tiesto/.test(req.url())) return req.continue();\n    // Otherwise, continue  the route, but replace \"tiesto\"\n    // in the URL with \"mestomusic\"\n    await req.continue({ url: req.url().replace('tiesto', 'mestomusic') });\n});\n\nawait page.goto('https://soundcloud.com/tiesto/following');\n\nawait page.waitForTimeout(10000);\nawait browser.close();"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Accessing Request Queues via API Client in JavaScript",
    "codeDescription": "This code shows how to access a request queue using the JavaScript API client. You can access a request queue by its name or ID, including request queues from other users.",
    "codeLanguage": "JavaScript",
    "codeTokens": 74,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/request_queue.md#2025-04-18_snippet_15",
    "pageTitle": "Managing Request Queues in Apify Platform",
    "codeList": [
      {
        "language": "JavaScript",
        "code": "const otherQueueClient = apifyClient.requestQueue('jane-doe/old-queue');"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Initializing Dataset Client in Python",
    "codeDescription": "Example showing how to initialize and save a reference to a specific dataset using the Python API client. This allows easy access to dataset operations in Python applications.",
    "codeLanguage": "python",
    "codeTokens": 68,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_7",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "python",
        "code": "my_dataset_client = apify_client.dataset('jane-doe/my-dataset')"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Launching Puppeteer with Local MITM Proxy in JavaScript",
    "codeDescription": "This snippet shows how to launch Puppeteer with the configured local proxy. It uses command-line arguments to disable sandbox mode for compatibility with Docker containers and specifies the proxy server address.",
    "codeLanguage": "javascript",
    "codeTokens": 95,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/using_proxy_to_intercept_requests_puppeteer.md#2025-04-18_snippet_2",
    "pageTitle": "Using man-in-the-middle proxy to intercept requests in Puppeteer",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Launch puppeteer with local proxy\nconst browser = await puppeteer.launch({\n    args: ['--no-sandbox', `--proxy-server=localhost:${proxyPort}`],\n});"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Setting Memory Limits in actor.json",
    "codeDescription": "Reference to setting memory limits in the actor.json configuration file to control platform usage costs for pay-per-result Actors.",
    "codeLanguage": "markdown",
    "codeTokens": 47,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/get_most_of_actors/monetizing_your_actor.md#2025-04-18_snippet_2",
    "pageTitle": "Monetizing your Actor on Apify Store",
    "codeList": [
      {
        "language": "markdown",
        "code": "`actor.json`"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Using Apify Proxy with Session Mode in PHP",
    "codeDescription": "This snippet illustrates how to use Apify's proxy service with session mode in PHP. It configures a Guzzle HTTP client to maintain the same IP address between requests using a session identifier.",
    "codeLanguage": "php",
    "codeTokens": 174,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/php/using_apify_from_php.md#2025-04-18_snippet_13",
    "pageTitle": "PHP Integration with Apify API",
    "codeList": [
      {
        "language": "php",
        "code": "$client = new \\GuzzleHttp\\Client([\n    // Replace <YOUR_PROXY_PASSWORD> below with your password\n    // found at https://console.apify.com/proxy\n    'proxy' => 'http://session-my_session:<YOUR_PROXY_PASSWORD>@proxy.apify.com:8000'\n]);\n\n// Both responses should contain the same clientIp\n$response = $client->get(\"https://api.apify.com/v2/browser-info\");\necho $response->getBody();\n\n$response = $client->get(\"https://api.apify.com/v2/browser-info\");\necho $response->getBody();"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Accessing Environment Variables in Python",
    "codeDescription": "Shows how to access environment variables in Python using the os.environ dictionary.",
    "codeLanguage": "Python",
    "codeTokens": 44,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/actors/development/programming_interface/environment_variables.md#2025-04-18_snippet_1",
    "pageTitle": "Actor Environment Variables in Apify",
    "codeList": [
      {
        "language": "python",
        "code": "import os\nprint(os.environ['APIFY_USER_ID'])"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Fingerprinting Function Calls in JavaScript",
    "codeDescription": "Example of JavaScript function calls used for browser fingerprinting, including WebGL parameters, codec support checking, and permission queries.",
    "codeLanguage": "javascript",
    "codeTokens": 176,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/webscraping/anti_scraping/techniques/fingerprinting.md#2025-04-18_snippet_0",
    "pageTitle": "Browser Fingerprinting Guide",
    "codeList": [
      {
        "language": "javascript",
        "code": "// Get the WebGL vendor information\nWebGLRenderingContext.getParameter(37445);\n\n// Get the WebGL renderer information\nWebGLRenderingContext.getParameter(37446);\n\n// Pass any codec into this function (ex. \"audio/aac\"). It will return\n// either \"maybe,\" \"probably,\" or \"\" indicating whether\n// or not the browser can play that codec. An empty\n// string means that  it can't be played.\nHTMLMediaElement.canPlayType('some/codec');\n\n// can ask for a permission if it is not already enabled.\n// allows you to know which permissions the user has\n// enabled, and which are disabled\nnavigator.permissions.query('some_permission');"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Accessing Datasets with Python API Client",
    "codeDescription": "This snippet shows how to access a dataset using the Python API client. It creates a dataset client instance that can be used to interact with a dataset belonging to another user.",
    "codeLanguage": "Python",
    "codeTokens": 73,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/platform/storage/dataset.md#2025-04-18_snippet_23",
    "pageTitle": "Dataset Storage for Web Scraping and Data Processing",
    "codeList": [
      {
        "language": "Python",
        "code": "other_dataset_client = apify_client.dataset('jane-doe/old-dataset')"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Injecting Moment.js Library Using jQuery in Web Scraper",
    "codeDescription": "This code snippet shows how to inject the Moment.js library into a Web Scraper page function using jQuery. It uses the $.getScript() method to load the library from the CDN. After loading, it demonstrates the use of Moment.js by formatting the current date and time.",
    "codeLanguage": "javascript",
    "codeTokens": 160,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/tutorials/node_js/add_external_libraries_web_scraper.md#2025-04-18_snippet_1",
    "pageTitle": "Adding External Libraries to Web Scraper",
    "codeList": [
      {
        "language": "javascript",
        "code": "async function pageFunction(context) {\n    const libraryUrl = 'https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.24.0/moment.min.js';\n\n    const $ = context.jQuery;\n\n    // Inject Moment.js\\\n    await $.getScript(libraryUrl);\n\n    // Confirm that it works.\\\n    const now = moment().format('ddd, hA');\n    context.log.info(`NOW: ${now}`);\n}"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Initializing Stats in Main JavaScript File",
    "codeDescription": "Demonstrates how to import and initialize the Stats utility class in the main JavaScript file of an Apify Actor, alongside other initializations.",
    "codeLanguage": "javascript",
    "codeTokens": 71,
    "codeId": "https://github.com/apify/apify-docs/blob/master/sources/academy/platform/expert_scraping_with_apify/solutions/saving_stats.md#2025-04-18_snippet_1",
    "pageTitle": "Saving Run Statistics for Apify Actors",
    "codeList": [
      {
        "language": "javascript",
        "code": "// ...\nimport Stats from './Stats.js';\n\nawait Actor.init();\nawait asinTracker.initialize();\nawait Stats.initialize();\n// ..."
      }
    ],
    "relevance": 0.87
  }
]