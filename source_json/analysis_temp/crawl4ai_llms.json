[
  {
    "codeTitle": "Setting up and running a basic web crawl with Crawl4AI in Python",
    "codeDescription": "This snippet demonstrates how to set up a simple web crawl using AsyncWebCrawler with default configurations for BrowserConfig and CrawlerRunConfig. It shows how to make a request to a URL and print the resulting markdown content.",
    "codeLanguage": "python",
    "codeTokens": 186,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-11_snippet_0",
    "pageTitle": "Simple Web Crawling with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    browser_config = BrowserConfig()  # Default browser configuration\n    run_config = CrawlerRunConfig()   # Default crawl run configuration\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n        print(result.markdown)  # Print clean markdown content\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.995
  },
  {
    "codeTitle": "Demonstrating Asynchronous Web Crawling with Crawl4AI in Python",
    "codeDescription": "This snippet shows how to use the AsyncWebCrawler class from Crawl4AI to asynchronously crawl a website and extract content. It demonstrates the creation of a crawler instance, running it on a URL, and printing the extracted markdown content.",
    "codeLanguage": "python",
    "codeTokens": 161,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/index.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI: Open-Source LLM-Friendly Web Crawler & Scraper",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Create an instance of AsyncWebCrawler\n    async with AsyncWebCrawler() as crawler:\n        # Run the crawler on a URL\n        result = await crawler.arun(url=\"https://crawl4ai.com\")\n\n        # Print the extracted content\n        print(result.markdown)\n\n# Run the async main function\nasyncio.run(main())"
      }
    ],
    "relevance": 0.995
  },
  {
    "codeTitle": "Basic Web Crawling with AsyncWebCrawler in Python",
    "codeDescription": "A simple Python example demonstrating how to use Crawl4AI's AsyncWebCrawler class to crawl a web page and extract markdown content. The example shows the asynchronous API pattern with async/await syntax.",
    "codeLanguage": "python",
    "codeTokens": 128,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import *\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n        )\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.99
  },
  {
    "codeTitle": "Using AsyncWebCrawler with Context Manager in Python",
    "codeDescription": "Demonstrates the recommended way to use AsyncWebCrawler within an async context manager. This approach automatically handles resource management and cleanup.",
    "codeLanguage": "python",
    "codeTokens": 74,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-11_snippet_2",
    "pageTitle": "AsyncWebCrawler Class Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "async with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    # The crawler automatically starts/closes resources"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Extracting Structured Data Using LLM in Python with Crawl4AI",
    "codeDescription": "This snippet demonstrates how to use language model-based extraction to retrieve structured pricing data from OpenAI's website. It defines a Pydantic model for OpenAI model fees and uses an AsyncWebCrawler with LLMExtractionStrategy to extract the data.",
    "codeLanguage": "python",
    "codeTokens": 461,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_10",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\nimport os, json\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(\n        ..., description=\"Fee for output token for the OpenAI model.\"\n    )\n\nasync def extract_structured_data_using_llm(provider: str, api_token: str = None, extra_headers: dict = None):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n    \n    # Skip if API token is missing (for providers that require it)\n    if api_token is None and provider != \"ollama\":\n        print(f\"API token is required for {provider}. Skipping this example.\")\n        return\n\n    extra_args = {\"extra_headers\": extra_headers} if extra_headers else {}\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\",\n            word_count_threshold=1,\n            extraction_strategy=LLMExtractionStrategy(\n                provider=provider,\n                api_token=api_token,\n                schema=OpenAIModelFee.schema(),\n                extraction_type=\"schema\",\n                instruction=\"\"\"Extract all model names along with fees for input and output tokens.\"\n                \"{model_name: 'GPT-4', input_fee: 'US$10.00 / 1M tokens', output_fee: 'US$30.00 / 1M tokens'}.\"\"\",\n                **extra_args\n            ),\n            bypass_cache=True,\n        )\n        print(json.loads(result.extracted_content)[:5])\n\n# Usage:\nawait extract_structured_data_using_llm(\"openai/gpt-4o-mini\", os.getenv(\"OPENAI_API_KEY\"))"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Configuring and Running AsyncWebCrawler with Extraction and LLM Filtering in Python",
    "codeDescription": "This code snippet demonstrates how to set up and run an AsyncWebCrawler with custom browser configuration, extraction strategy, LLM-based content filtering, and crawler run configuration. It includes initializing the crawler, defining extraction schema, setting up LLM content filtering, and executing the crawl.",
    "codeLanguage": "python",
    "codeTokens": 592,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-11_snippet_7",
    "pageTitle": "Browser, Crawler & LLM Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    # 1) Browser config: headless, bigger viewport, no proxy\n    browser_conf = BrowserConfig(\n        headless=True,\n        viewport_width=1280,\n        viewport_height=720\n    )\n\n    # 2) Example extraction strategy\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"div.article\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n    extraction = JsonCssExtractionStrategy(schema)\n\n    # 3) Example LLM content filtering\n\n    gemini_config = LLMConfig(\n        provider=\"gemini/gemini-1.5-pro\" \n        api_token = \"env:GEMINI_API_TOKEN\"\n    )\n\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config=gemini_config,  # or your preferred provider\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=500,  # Adjust based on your needs\n        verbose=True\n    )\n\n    md_generator = DefaultMarkdownGenerator(\n    content_filter=filter,\n    options={\"ignore_links\": True}\n\n    # 4) Crawler run config: skip cache, use extraction\n    run_conf = CrawlerRunConfig(\n        markdown_generator=md_generator,\n        extraction_strategy=extraction,\n        cache_mode=CacheMode.BYPASS,\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        # 4) Execute the crawl\n        result = await crawler.arun(url=\"https://example.com/news\", config=run_conf)\n\n        if result.success:\n            print(\"Extracted content:\", result.extracted_content)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Implementing Basic Deep Crawl with BFSDeepCrawlStrategy in Python",
    "codeDescription": "This snippet demonstrates how to set up a basic deep crawl using the BFSDeepCrawlStrategy in Crawl4AI. It configures a 2-level deep crawl, stays within the same domain, and prints basic information about the crawled pages.",
    "codeLanguage": "python",
    "codeTokens": 287,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_0",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.deep_crawling import BFSDeepCrawlStrategy\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\n\nasync def main():\n    # Configure a 2-level deep crawl\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BFSDeepCrawlStrategy(\n            max_depth=2, \n            include_external=False\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        verbose=True\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun(\"https://example.com\", config=config)\n        \n        print(f\"Crawled {len(results)} pages in total\")\n        \n        # Access individual results\n        for result in results[:3]:  # Show first 3 results\n            print(f\"URL: {result.url}\")\n            print(f\"Depth: {result.metadata.get('depth', 0)}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Using Content Filters with DefaultMarkdownGenerator in Crawl4AI",
    "codeDescription": "Demonstrates how to use the DefaultMarkdownGenerator with a PruningContentFilter to process and filter Markdown output. This example compares raw and filtered Markdown lengths, showing how the filter affects the output based on a specified threshold.",
    "codeLanguage": "python",
    "codeTokens": 219,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#2025-04-11_snippet_2",
    "pageTitle": "Getting Started with Crawl4AI: Web Crawling and Scraping Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=PruningContentFilter(threshold=0.4, threshold_type=\"fixed\")\n)\n\nconfig = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    markdown_generator=md_generator\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://news.ycombinator.com\", config=config)\n    print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n    print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Extracting Structured Data using LLM in Python with Crawl4AI",
    "codeDescription": "This snippet demonstrates how to use Crawl4AI with an LLM-based extraction strategy to parse web content into a structured format. It supports both open-source and closed-source LLM providers, and uses a Pydantic model to define the expected data schema.",
    "codeLanguage": "python",
    "codeTokens": 552,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#2025-04-11_snippet_5",
    "pageTitle": "Getting Started with Crawl4AI: Web Crawling and Scraping Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "import os\nimport json\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(\n        ..., description=\"Fee for output token for the OpenAI model.\"\n    )\n\nasync def extract_structured_data_using_llm(\n    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None\n):\n    print(f\"\\n--- Extracting Structured Data with {provider} ---\")\n\n    if api_token is None and provider != \"ollama\":\n        print(f\"API token is required for {provider}. Skipping this example.\")\n        return\n\n    browser_config = BrowserConfig(headless=True)\n\n    extra_args = {\"temperature\": 0, \"top_p\": 0.9, \"max_tokens\": 2000}\n    if extra_headers:\n        extra_args[\"extra_headers\"] = extra_headers\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        word_count_threshold=1,\n        page_timeout=80000,\n        extraction_strategy=LLMExtractionStrategy(\n            llm_config = LLMConfig(provider=provider,api_token=api_token),\n            schema=OpenAIModelFee.model_json_schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content.\"\"\",\n            extra_args=extra_args,\n        ),\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://openai.com/api/pricing/\", config=crawler_config\n        )\n        print(result.extracted_content)\n\nif __name__ == \"__main__\":\n\n    asyncio.run(\n        extract_structured_data_using_llm(\n            provider=\"openai/gpt-4o\", api_token=os.getenv(\"OPENAI_API_KEY\")\n        )\n    )"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "CSS-based Data Extraction with JsonCssExtractionStrategy",
    "codeDescription": "Demonstrates how to use JsonCssExtractionStrategy for structured data extraction from HTML. This example defines a schema with CSS selectors to extract titles and links from HTML items, processes raw HTML, and returns structured JSON data without requiring LLM processing.",
    "codeLanguage": "python",
    "codeTokens": 326,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#2025-04-11_snippet_4",
    "pageTitle": "Getting Started with Crawl4AI: Web Crawling and Scraping Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        # The JSON output is stored in 'extracted_content'\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.985
  },
  {
    "codeTitle": "Configuring Browser Behavior with BrowserConfig in Python",
    "codeDescription": "This snippet demonstrates how to set up a BrowserConfig object to control browser-specific settings like browser type, headless mode, viewport dimensions, proxy, and user agent for web crawling operations.",
    "codeLanguage": "python",
    "codeTokens": 151,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-11_snippet_0",
    "pageTitle": "Browser and Crawler Configuration in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_cfg = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    viewport_width=1280,\n    viewport_height=720,\n    proxy=\"http://user:pass@proxy:8080\",\n    user_agent=\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36\",\n)"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Generating Extraction Schemas with LLMs in Crawl4AI",
    "codeDescription": "Shows how to use LLM-based tools to automatically generate extraction schemas for structured data. The example demonstrates using both OpenAI (requiring an API token) and Ollama (open source, no token needed) to create a reusable schema for JsonCssExtractionStrategy.",
    "codeLanguage": "python",
    "codeTokens": 275,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#2025-04-11_snippet_3",
    "pageTitle": "Getting Started with Crawl4AI: Web Crawling and Scraping Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Generate a schema (one-time cost)\nhtml = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\"\n\n# Using OpenAI (requires API token)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")  # Required for OpenAI\n)\n\n# Or using Ollama (open source, no token needed)\nschema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(schema)"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Initializing LLMExtractionStrategy in Python for Crawl4AI",
    "codeDescription": "This snippet demonstrates the initialization of LLMExtractionStrategy class with various parameters for configuring LLM-based extraction, including provider settings, extraction configuration, chunking parameters, and API configuration.",
    "codeLanguage": "python",
    "codeTokens": 285,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_0",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "LLMExtractionStrategy(\n    # Required Parameters\n    provider: str = DEFAULT_PROVIDER,     # LLM provider (e.g., \"ollama/llama2\")\n    api_token: Optional[str] = None,      # API token\n    \n    # Extraction Configuration\n    instruction: str = None,              # Custom extraction instruction\n    schema: Dict = None,                  # Pydantic model schema for structured data\n    extraction_type: str = \"block\",       # \"block\" or \"schema\"\n    \n    # Chunking Parameters\n    chunk_token_threshold: int = 4000,    # Maximum tokens per chunk\n    overlap_rate: float = 0.1,           # Overlap between chunks\n    word_token_rate: float = 0.75,       # Word to token conversion rate\n    apply_chunking: bool = True,         # Enable/disable chunking\n    \n    # API Configuration\n    base_url: str = None,                # Base URL for API\n    extra_args: Dict = {},               # Additional provider arguments\n    verbose: bool = False                # Enable verbose logging\n)"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Initializing BrowserConfig Class in Python",
    "codeDescription": "The BrowserConfig class constructor with its parameters that controls browser initialization and behavior. It includes options for browser type, headless mode, proxy settings, viewport dimensions, and other browser-specific configurations.",
    "codeLanguage": "python",
    "codeTokens": 154,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-11_snippet_0",
    "pageTitle": "Browser, Crawler & LLM Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "class BrowserConfig:\n    def __init__(\n        browser_type=\"chromium\",\n        headless=True,\n        proxy_config=None,\n        viewport_width=1080,\n        viewport_height=600,\n        verbose=True,\n        use_persistent_context=False,\n        user_data_dir=None,\n        cookies=None,\n        headers=None,\n        user_agent=None,\n        text_mode=False,\n        light_mode=False,\n        extra_args=None,\n        # ... other advanced parameters omitted here\n    ):\n        ..."
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Configuring LLMExtractionStrategy with CrawlerRunConfig in Crawl4AI",
    "codeDescription": "Complete example demonstrating how to use LLM-based extraction in Crawl4AI. This snippet defines a Pydantic model for products, creates an LLM extraction strategy with OpenAI, configures the crawler, and processes the results. It showcases chunking, schema definition, and usage tracking.",
    "codeLanguage": "python",
    "codeTokens": 507,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-11_snippet_0",
    "pageTitle": "LLM-Based JSON Extraction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import os\nimport asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Product(BaseModel):\n    name: str\n    price: str\n\nasync def main():\n    # 1. Define the LLM extraction strategy\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv('OPENAI_API_KEY')),\n        schema=Product.schema_json(), # Or use model_json_schema()\n        extraction_type=\"schema\",\n        instruction=\"Extract all product objects with 'name' and 'price' from the content.\",\n        chunk_token_threshold=1000,\n        overlap_rate=0.0,\n        apply_chunking=True,\n        input_format=\"markdown\",   # or \"html\", \"fit_markdown\"\n        extra_args={\"temperature\": 0.0, \"max_tokens\": 800}\n    )\n\n    # 2. Build the crawler config\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strategy,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3. Create a browser config if needed\n    browser_cfg = BrowserConfig(headless=True)\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        # 4. Let's say we want to crawl a single page\n        result = await crawler.arun(\n            url=\"https://example.com/products\",\n            config=crawl_config\n        )\n\n        if result.success:\n            # 5. The extracted content is presumably JSON\n            data = json.loads(result.extracted_content)\n            print(\"Extracted items:\", data)\n            \n            # 6. Show usage stats\n            llm_strategy.show_usage()  # prints token usage\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.98
  },
  {
    "codeTitle": "Cosine Similarity Extraction in Python with Crawl4AI",
    "codeDescription": "This snippet demonstrates how to use cosine similarity extraction strategy in Crawl4AI. It sets up a CosineStrategy with various parameters for semantic clustering and uses an AsyncWebCrawler to extract relevant content from a news article based on contextual similarity.",
    "codeLanguage": "python",
    "codeTokens": 277,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_11",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.extraction_strategy import CosineStrategy\n\nasync def cosine_similarity_extraction():\n    async with AsyncWebCrawler() as crawler:\n        strategy = CosineStrategy(\n            word_count_threshold=10,\n            max_dist=0.2, # Maximum distance between two words\n            linkage_method=\"ward\", # Linkage method for hierarchical clustering (ward, complete, average, single)\n            top_k=3, # Number of top keywords to extract\n            sim_threshold=0.3, # Similarity threshold for clustering\n            semantic_filter=\"McDonald's economic impact, American consumer trends\", # Keywords to filter the content semantically using embeddings\n            verbose=True\n        )\n        \n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business/consumer/how-mcdonalds-e-coli-crisis-inflation-politics-reflect-american-story-rcna177156\",\n            extraction_strategy=strategy\n        )\n        print(json.loads(result.extracted_content)[:5])\n\nasyncio.run(cosine_similarity_extraction())"
      }
    ],
    "relevance": 0.978
  },
  {
    "codeTitle": "Defining the CrawlResult Class in Python",
    "codeDescription": "The CrawlResult class definition showing all available fields that encapsulate data returned from a crawl operation, including URL, HTML content, success status, and various optional fields for additional data.",
    "codeLanguage": "python",
    "codeTokens": 237,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-11_snippet_0",
    "pageTitle": "CrawlResult Class Reference",
    "codeList": [
      {
        "language": "python",
        "code": "class CrawlResult(BaseModel):\n    url: str\n    html: str\n    success: bool\n    cleaned_html: Optional[str] = None\n    media: Dict[str, List[Dict]] = {}\n    links: Dict[str, List[Dict]] = {}\n    downloaded_files: Optional[List[str]] = None\n    screenshot: Optional[str] = None\n    pdf : Optional[bytes] = None\n    markdown: Optional[Union[str, MarkdownGenerationResult]] = None\n    extracted_content: Optional[str] = None\n    metadata: Optional[dict] = None\n    error_message: Optional[str] = None\n    session_id: Optional[str] = None\n    response_headers: Optional[dict] = None\n    status_code: Optional[int] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    dispatch_result: Optional[DispatchResult] = None\n    ..."
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Dynamic Content Extraction with JavaScript Interaction using Crawl4AI in Python",
    "codeDescription": "This snippet demonstrates how to use Crawl4AI to extract data from dynamic web pages that require JavaScript interaction. It uses the JsonCssExtractionStrategy to parse structured data and includes custom JavaScript for interacting with page elements.",
    "codeLanguage": "python",
    "codeTokens": 552,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#2025-04-11_snippet_7",
    "pageTitle": "Getting Started with Crawl4AI: Web Crawling and Scraping Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_structured_data_using_css_extractor():\n    print(\"\\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---\")\n    schema = {\n        \"name\": \"KidoCode Courses\",\n        \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n        \"fields\": [\n            {\n                \"name\": \"section_title\",\n                \"selector\": \"h3.heading-50\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"section_description\",\n                \"selector\": \".charge-content\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_name\",\n                \"selector\": \".text-block-93\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_description\",\n                \"selector\": \".course-content-text\",\n                \"type\": \"text\",\n            },\n            {\n                \"name\": \"course_icon\",\n                \"selector\": \".image-92\",\n                \"type\": \"attribute\",\n                \"attribute\": \"src\",\n            },\n        ],\n    }\n\n    browser_config = BrowserConfig(headless=True, java_script_enabled=True)\n\n    js_click_tabs = \"\"\"\n    (async () => {\n        const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");\n        for(let tab of tabs) {\n            tab.scrollIntoView();\n            tab.click();\n            await new Promise(r => setTimeout(r, 500));\n        }\n    })();\n    \"\"\"\n\n    crawler_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        js_code=[js_click_tabs],\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\", config=crawler_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(companies)} companies\")\n        print(json.dumps(companies[0], indent=2))\n\nasync def main():\n    await extract_structured_data_using_css_extractor()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Implementing BestFirstCrawlingStrategy with KeywordRelevanceScorer in Python",
    "codeDescription": "This code snippet shows how to use the BestFirstCrawlingStrategy with a KeywordRelevanceScorer for intelligent crawling. It prioritizes pages based on keyword relevance, allowing for more focused and efficient crawling of relevant content.",
    "codeLanguage": "python",
    "codeTokens": 182,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_3",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\n# Create a scorer\nscorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7\n)\n\n# Configure the strategy\nstrategy = BestFirstCrawlingStrategy(\n    max_depth=2,\n    include_external=False,\n    url_scorer=scorer,\n    max_pages=25,              # Maximum number of pages to crawl (optional)\n)"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Complete Crawler Implementation Example in Python",
    "codeDescription": "Full example showing how to configure and use the AsyncWebCrawler with both browser and run configurations. Demonstrates error handling, screenshot capture, and async/await pattern.",
    "codeLanguage": "python",
    "codeTokens": 312,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-11_snippet_3",
    "pageTitle": "Browser and Crawler Configuration in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Configure the browser\n    browser_cfg = BrowserConfig(\n        headless=False,\n        viewport_width=1280,\n        viewport_height=720,\n        proxy=\"http://user:pass@myproxy:8080\",\n        text_mode=True\n    )\n\n    # Configure the run\n    run_cfg = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        session_id=\"my_session\",\n        css_selector=\"main.article\",\n        excluded_tags=[\"script\", \"style\"],\n        exclude_external_links=True,\n        wait_for=\"css:.article-loaded\",\n        screenshot=True,\n        stream=True\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/news\",\n            config=run_cfg\n        )\n        if result.success:\n            print(\"Final cleaned_html length:\", len(result.cleaned_html))\n            if result.screenshot:\n                print(\"Screenshot captured (base64, length):\", len(result.screenshot))\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Configuring and Running Advanced Crawl4AI Features in Python",
    "codeDescription": "This code demonstrates how to set up and execute a crawl using Crawl4AI with advanced features such as proxy usage, PDF and screenshot capture, SSL certificate retrieval, custom headers, and session reuse. It configures both browser and crawler settings, performs the crawl, and handles the results including saving PDF and screenshot outputs.",
    "codeLanguage": "python",
    "codeTokens": 475,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-11_snippet_6",
    "pageTitle": "Advanced Features in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import os, asyncio\nfrom base64 import b64decode\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # 1. Browser config with proxy + headless\n    browser_cfg = BrowserConfig(\n        proxy_config={\n            \"server\": \"http://proxy.example.com:8080\",\n            \"username\": \"myuser\",\n            \"password\": \"mypass\",\n        },\n        headless=True,\n    )\n\n    # 2. Crawler config with PDF, screenshot, SSL, custom headers, and ignoring caches\n    crawler_cfg = CrawlerRunConfig(\n        pdf=True,\n        screenshot=True,\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS,\n        headers={\"Accept-Language\": \"en-US,en;q=0.8\"},\n        storage_state=\"my_storage.json\",  # Reuse session from a previous sign-in\n        verbose=True,\n    )\n\n    # 3. Crawl\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url = \"https://secure.example.com/protected\", \n            config=crawler_cfg\n        )\n        \n        if result.success:\n            print(\"[OK] Crawled the secure page. Links found:\", len(result.links.get(\"internal\", [])))\n            \n            # Save PDF & screenshot\n            if result.pdf:\n                with open(\"result.pdf\", \"wb\") as f:\n                    f.write(b64decode(result.pdf))\n            if result.screenshot:\n                with open(\"result.png\", \"wb\") as f:\n                    f.write(b64decode(result.screenshot))\n            \n            # Check SSL cert\n            if result.ssl_certificate:\n                print(\"SSL Issuer CN:\", result.ssl_certificate.issuer.get(\"CN\", \"\"))\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.975
  },
  {
    "codeTitle": "Multi-URL Concurrent Crawling with Crawl4AI in Python",
    "codeDescription": "This example shows how to use Crawl4AI for crawling multiple URLs concurrently. It demonstrates both streaming and batch modes of operation, using the MemoryAdaptiveDispatcher for automatic concurrency adjustment based on system resources.",
    "codeLanguage": "python",
    "codeTokens": 340,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#2025-04-11_snippet_6",
    "pageTitle": "Getting Started with Crawl4AI: Web Crawling and Scraping Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def quick_parallel_example():\n    urls = [\n        \"https://example.com/page1\",\n        \"https://example.com/page2\",\n        \"https://example.com/page3\"\n    ]\n    \n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        # Stream results as they complete\n        async for result in await crawler.arun_many(urls, config=run_conf):\n            if result.success:\n                print(f\"[OK] {result.url}, length: {len(result.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {result.url} => {result.error_message}\")\n\n        # Or get all results at once (default behavior)\n        run_conf = run_conf.clone(stream=False)\n        results = await crawler.arun_many(urls, config=run_conf)\n        for res in results:\n            if res.success:\n                print(f\"[OK] {res.url}, length: {len(res.markdown.raw_markdown)}\")\n            else:\n                print(f\"[ERROR] {res.url} => {res.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(quick_parallel_example())"
      }
    ],
    "relevance": 0.972
  },
  {
    "codeTitle": "Function Signature for arun_many in Python",
    "codeDescription": "The function signature for arun_many shows its parameters and return type. It accepts a list of URLs or tasks, an optional configuration, and an optional dispatcher for concurrency control.",
    "codeLanguage": "python",
    "codeTokens": 200,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-11_snippet_0",
    "pageTitle": "arun_many Function Reference",
    "codeList": [
      {
        "language": "python",
        "code": "async def arun_many(\n    urls: Union[List[str], List[Any]],\n    config: Optional[CrawlerRunConfig] = None,\n    dispatcher: Optional[BaseDispatcher] = None,\n    ...\n) -> Union[List[CrawlResult], AsyncGenerator[CrawlResult, None]]:\n    \"\"\"\n    Crawl multiple URLs concurrently or in batches.\n\n    :param urls: A list of URLs (or tasks) to crawl.\n    :param config: (Optional) A default `CrawlerRunConfig` applying to each crawl.\n    :param dispatcher: (Optional) A concurrency controller (e.g. MemoryAdaptiveDispatcher).\n    ...\n    :return: Either a list of `CrawlResult` objects, or an async generator if streaming is enabled.\n    \"\"\""
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Basic Session Usage with AsyncWebCrawler in Python",
    "codeDescription": "Demonstrates how to use BrowserConfig and CrawlerRunConfig to maintain state across multiple requests using a session_id. This example shows how to create a session, perform sequential requests, and properly clean up the session when done.",
    "codeLanguage": "python",
    "codeTokens": 205,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-11_snippet_0",
    "pageTitle": "Session Management in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    session_id = \"my_session\"\n\n    # Define configurations\n    config1 = CrawlerRunConfig(\n        url=\"https://example.com/page1\", session_id=session_id\n    )\n    config2 = CrawlerRunConfig(\n        url=\"https://example.com/page2\", session_id=session_id\n    )\n\n    # First request\n    result1 = await crawler.arun(config=config1)\n\n    # Subsequent request using the same session\n    result2 = await crawler.arun(config=config2)\n\n    # Clean up when done\n    await crawler.crawler_strategy.kill_session(session_id)"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Comprehensive AsyncWebCrawler Usage Example in Python",
    "codeDescription": "A complete example demonstrating the usage of AsyncWebCrawler with BrowserConfig and CrawlerRunConfig. It includes setting up a custom extraction strategy, handling the crawl result, and error checking.",
    "codeLanguage": "python",
    "codeTokens": 391,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-11_snippet_5",
    "pageTitle": "AsyncWebCrawler Class Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nimport json\n\nasync def main():\n    # 1. Browser config\n    browser_cfg = BrowserConfig(\n        browser_type=\"firefox\",\n        headless=False,\n        verbose=True\n    )\n\n    # 2. Run config\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"article.post\",\n        \"fields\": [\n            {\n                \"name\": \"title\", \n                \"selector\": \"h2\", \n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"url\", \n                \"selector\": \"a\", \n                \"type\": \"attribute\", \n                \"attribute\": \"href\"\n            }\n        ]\n    }\n\n    run_cfg = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        word_count_threshold=15,\n        remove_overlay_elements=True,\n        wait_for=\"css:.post\"  # Wait for posts to appear\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/blog\",\n            config=run_cfg\n        )\n\n        if result.success:\n            print(\"Cleaned HTML length:\", len(result.cleaned_html))\n            if result.extracted_content:\n                articles = json.loads(result.extracted_content)\n                print(\"Extracted articles:\", articles[:2])\n        else:\n            print(\"Error:\", result.error_message)\n\nasyncio.run(main())"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Running E-commerce Data Extraction with Crawl4AI in Python",
    "codeDescription": "This code snippet demonstrates how to use the Crawl4AI library to extract structured data from an e-commerce HTML page using the defined schema. It sets up an AsyncWebCrawler and uses JsonCssExtractionStrategy to parse the HTML.",
    "codeLanguage": "python",
    "codeTokens": 306,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-11_snippet_3",
    "pageTitle": "JSON Schema-Based Web Extraction Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\necommerce_schema = {\n    # ... the advanced schema from above ...\n}\n\nasync def extract_ecommerce_data():\n    strategy = JsonCssExtractionStrategy(ecommerce_schema, verbose=True)\n    \n    config = CrawlerRunConfig()\n    \n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html\",\n            extraction_strategy=strategy,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n        \n        # Parse the JSON output\n        data = json.loads(result.extracted_content)\n        print(json.dumps(data, indent=2) if data else \"No data found.\")\n\nasyncio.run(extract_ecommerce_data())"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Defining E-commerce Product Extraction Schema in Python",
    "codeDescription": "This snippet defines a comprehensive schema for extracting structured data from an e-commerce product catalog HTML page. It includes nested fields for product details, features, reviews, and related products.",
    "codeLanguage": "python",
    "codeTokens": 616,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-11_snippet_2",
    "pageTitle": "JSON Schema-Based Web Extraction Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "schema = {\n    \"name\": \"E-commerce Product Catalog\",\n    \"baseSelector\": \"div.category\",\n    # (1) We can define optional baseFields if we want to extract attributes \n    # from the category container\n    \"baseFields\": [\n        {\"name\": \"data_cat_id\", \"type\": \"attribute\", \"attribute\": \"data-cat-id\"}, \n    ],\n    \"fields\": [\n        {\n            \"name\": \"category_name\",\n            \"selector\": \"h2.category-name\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"products\",\n            \"selector\": \"div.product\",\n            \"type\": \"nested_list\",    # repeated sub-objects\n            \"fields\": [\n                {\n                    \"name\": \"name\",\n                    \"selector\": \"h3.product-name\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"price\",\n                    \"selector\": \"p.product-price\",\n                    \"type\": \"text\"\n                },\n                {\n                    \"name\": \"details\",\n                    \"selector\": \"div.product-details\",\n                    \"type\": \"nested\",  # single sub-object\n                    \"fields\": [\n                        {\n                            \"name\": \"brand\",\n                            \"selector\": \"span.brand\",\n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"model\",\n                            \"selector\": \"span.model\",\n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"features\",\n                    \"selector\": \"ul.product-features li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\"name\": \"feature\", \"type\": \"text\"} \n                    ]\n                },\n                {\n                    \"name\": \"reviews\",\n                    \"selector\": \"div.review\",\n                    \"type\": \"nested_list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"reviewer\", \n                            \"selector\": \"span.reviewer\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"rating\", \n                            \"selector\": \"span.rating\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"comment\", \n                            \"selector\": \"p.review-text\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                },\n                {\n                    \"name\": \"related_products\",\n                    \"selector\": \"ul.related-products li\",\n                    \"type\": \"list\",\n                    \"fields\": [\n                        {\n                            \"name\": \"name\", \n                            \"selector\": \"span.related-name\", \n                            \"type\": \"text\"\n                        },\n                        {\n                            \"name\": \"price\", \n                            \"selector\": \"span.related-price\", \n                            \"type\": \"text\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Cloning CrawlerRunConfig with Helper Methods in Python",
    "codeDescription": "Example demonstrating how to create a base crawler configuration and clone it for different scenarios. This approach allows creating variations for streaming and debugging while maintaining consistent base settings.",
    "codeLanguage": "python",
    "codeTokens": 151,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-11_snippet_5",
    "pageTitle": "Browser, Crawler & LLM Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "# Create a base configuration\nbase_config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    word_count_threshold=200,\n    wait_until=\"networkidle\"\n)\n\n# Create variations for different use cases\nstream_config = base_config.clone(\n    stream=True,  # Enable streaming mode\n    cache_mode=CacheMode.BYPASS\n)\n\ndebug_config = base_config.clone(\n    page_timeout=120000,  # Longer timeout for debugging\n    verbose=True\n)"
      }
    ],
    "relevance": 0.97
  },
  {
    "codeTitle": "Comprehensive example of web crawling with Crawl4AI",
    "codeDescription": "This complete example demonstrates common usage patterns for web crawling with Crawl4AI. It includes setting up configurations, performing the crawl, and processing various types of results including content, images, and links. It also shows how to handle errors and enable caching.",
    "codeLanguage": "python",
    "codeTokens": 341,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-11_snippet_5",
    "pageTitle": "Simple Web Crawling with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_config = BrowserConfig(verbose=True)\n    run_config = CrawlerRunConfig(\n        # Content filtering\n        word_count_threshold=10,\n        excluded_tags=['form', 'header'],\n        exclude_external_links=True,\n        \n        # Content processing\n        process_iframes=True,\n        remove_overlay_elements=True,\n        \n        # Cache control\n        cache_mode=CacheMode.ENABLED  # Use cache if available\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n        \n        if result.success:\n            # Print clean content\n            print(\"Content:\", result.markdown[:500])  # First 500 chars\n            \n            # Process images\n            for image in result.media[\"images\"]:\n                print(f\"Found image: {image['src']}\")\n            \n            # Process links\n            for link in result.links[\"internal\"]:\n                print(f\"Internal link: {link['href']}\")\n                \n        else:\n            print(f\"Crawl failed: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.968
  },
  {
    "codeTitle": "Building Advanced Crawler with Multiple Filters in Crawl4AI",
    "codeDescription": "Creates a sophisticated crawler implementation combining multiple filtering techniques, scoring, and async execution.",
    "codeLanguage": "python",
    "codeTokens": 588,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_11",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    DomainFilter,\n    URLPatternFilter,\n    ContentTypeFilter\n)\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\nasync def run_advanced_crawler():\n    # Create a sophisticated filter chain\n    filter_chain = FilterChain([\n        # Domain boundaries\n        DomainFilter(\n            allowed_domains=[\"docs.example.com\"],\n            blocked_domains=[\"old.docs.example.com\"]\n        ),\n        \n        # URL patterns to include\n        URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\", \"*blog*\"]),\n        \n        # Content type filtering\n        ContentTypeFilter(allowed_types=[\"text/html\"])\n    ])\n\n    # Create a relevance scorer\n    keyword_scorer = KeywordRelevanceScorer(\n        keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n        weight=0.7\n    )\n\n    # Set up the configuration\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BestFirstCrawlingStrategy(\n            max_depth=2,\n            include_external=False,\n            filter_chain=filter_chain,\n            url_scorer=keyword_scorer\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        stream=True,\n        verbose=True\n    )\n\n    # Execute the crawl\n    results = []\n    async with AsyncWebCrawler() as crawler:\n        async for result in await crawler.arun(\"https://docs.example.com\", config=config):\n            results.append(result)\n            score = result.metadata.get(\"score\", 0)\n            depth = result.metadata.get(\"depth\", 0)\n            print(f\"Depth: {depth} | Score: {score:.2f} | {result.url}\")\n\n    # Analyze the results\n    print(f\"Crawled {len(results)} high-value pages\")\n    print(f\"Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}\")\n\n    # Group by depth\n    depth_counts = {}\n    for result in results:\n        depth = result.metadata.get(\"depth\", 0)\n        depth_counts[depth] = depth_counts.get(depth, 0) + 1\n\n    print(\"Pages crawled by depth:\")\n    for depth, count in sorted(depth_counts.items()):\n        print(f\"  Depth {depth}: {count} pages\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_advanced_crawler())"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Extracting Article Data with Crawl4AI in Python",
    "codeDescription": "This function demonstrates how to use Crawl4AI to extract structured article data from a webpage. It utilizes CSS selection, exclusion logic, and pattern-based extraction to fine-tune the extracted data.",
    "codeLanguage": "python",
    "codeTokens": 439,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_7",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_main_articles(url: str):\n    schema = {\n        \"name\": \"ArticleBlock\",\n        \"baseSelector\": \"div.article-block\",\n        \"fields\": [\n            {\"name\": \"headline\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"summary\", \"selector\": \".summary\", \"type\": \"text\"},\n            {\n                \"name\": \"metadata\",\n                \"type\": \"nested\",\n                \"fields\": [\n                    {\"name\": \"author\", \"selector\": \".author\", \"type\": \"text\"},\n                    {\"name\": \"date\", \"selector\": \".date\", \"type\": \"text\"}\n                ]\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Keep only #main-content\n        css_selector=\"#main-content\",\n        \n        # Filtering\n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],  \n        exclude_external_links=True,\n        exclude_domains=[\"somebadsite.com\"],\n        exclude_external_images=True,\n\n        # Extraction\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        \n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=url, config=config)\n        if not result.success:\n            print(f\"Error: {result.error_message}\")\n            return None\n        return json.loads(result.extracted_content)\n\nasync def main():\n    articles = await extract_main_articles(\"https://news.ycombinator.com/newest\")\n    if articles:\n        print(\"Extracted Articles:\", articles[:2])  # Show first 2\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Filtering Links and Media with Crawl4AI in Python",
    "codeDescription": "Demonstrates how to configure and use AsyncWebCrawler to filter external links, exclude specific domains, and manage image content. The code shows how to set up crawler configuration, process crawl results, and handle both internal/external links and images.",
    "codeLanguage": "python",
    "codeTokens": 412,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#2025-04-11_snippet_4",
    "pageTitle": "Link and Media Extraction in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    # Suppose we want to keep only internal links, remove certain domains, \n    # and discard external images from the final crawl data.\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,\n        exclude_domains=[\"spammyads.com\"],\n        exclude_social_media_links=True,   # skip Twitter, Facebook, etc.\n        exclude_external_images=True,      # keep only images from main domain\n        wait_for_images=True,             # ensure images are loaded\n        verbose=True\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://www.example.com\", config=crawler_cfg)\n\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n            \n            # 1. Links\n            in_links = result.links.get(\"internal\", [])\n            ext_links = result.links.get(\"external\", [])\n            print(\"Internal link count:\", len(in_links))\n            print(\"External link count:\", len(ext_links))  # should be zero with exclude_external_links=True\n            \n            # 2. Images\n            images = result.media.get(\"images\", [])\n            print(\"Images found:\", len(images))\n            \n            # Let's see a snippet of these images\n            for i, img in enumerate(images[:3]):\n                print(f\"  - {img['src']} (alt={img.get('alt','')}, score={img.get('score','N/A')})\"\n        else:\n            print(\"[ERROR] Failed to crawl. Reason:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Implementing Knowledge Graph Extraction with LLMExtractionStrategy in Python",
    "codeDescription": "This code snippet demonstrates how to use LLMExtractionStrategy with a Pydantic schema to extract entities and relationships from web content and build a knowledge graph. It includes the setup of the extraction strategy, crawler configuration, and the main crawling process.",
    "codeLanguage": "python",
    "codeTokens": 450,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-11_snippet_4",
    "pageTitle": "LLM-Based JSON Extraction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import os\nimport json\nimport asyncio\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass Entity(BaseModel):\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    entity1: Entity\n    entity2: Entity\n    description: str\n    relation_type: str\n\nclass KnowledgeGraph(BaseModel):\n    entities: List[Entity]\n    relationships: List[Relationship]\n\nasync def main():\n    # LLM extraction strategy\n    llm_strat = LLMExtractionStrategy(\n        provider=\"openai/gpt-4\",\n        api_token=os.getenv('OPENAI_API_KEY'),\n        schema=KnowledgeGraph.schema_json(),\n        extraction_type=\"schema\",\n        instruction=\"Extract entities and relationships from the content. Return valid JSON.\",\n        chunk_token_threshold=1400,\n        apply_chunking=True,\n        input_format=\"html\",\n        extra_args={\"temperature\": 0.1, \"max_tokens\": 1500}\n    )\n\n    crawl_config = CrawlerRunConfig(\n        extraction_strategy=llm_strat,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n        # Example page\n        url = \"https://www.nbcnews.com/business\"\n        result = await crawler.arun(url=url, config=crawl_config)\n\n        if result.success:\n            with open(\"kb_result.json\", \"w\", encoding=\"utf-8\") as f:\n                f.write(result.extracted_content)\n            llm_strat.show_usage()\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Executing JavaScript & Extracting Structured Data without LLMs in Python",
    "codeDescription": "This example shows how to execute JavaScript and extract structured data without using LLMs. It uses a custom JSON CSS extraction strategy to scrape course information from a website.",
    "codeLanguage": "python",
    "codeTokens": 507,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_9",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nimport json\n\nasync def main():\n    schema = {\n    \"name\": \"KidoCode Courses\",\n    \"baseSelector\": \"section.charge-methodology .w-tab-content > div\",\n    \"fields\": [\n        {\n            \"name\": \"section_title\",\n            \"selector\": \"h3.heading-50\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"section_description\",\n            \"selector\": \".charge-content\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"course_name\",\n            \"selector\": \".text-block-93\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"course_description\",\n            \"selector\": \".course-content-text\",\n            \"type\": \"text\",\n        },\n        {\n            \"name\": \"course_icon\",\n            \"selector\": \".image-92\",\n            \"type\": \"attribute\",\n            \"attribute\": \"src\"\n        }\n    ]\n}\n\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    browser_config = BrowserConfig(\n        headless=False,\n        verbose=True\n    )\n    run_config = CrawlerRunConfig(\n        extraction_strategy=extraction_strategy,\n        js_code=[\"\"\"(async () => {const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\");for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r => setTimeout(r, 500));}})();\"\"\"],\n        cache_mode=CacheMode.BYPASS\n    )\n        \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        \n        result = await crawler.arun(\n            url=\"https://www.kidocode.com/degrees/technology\",\n            config=run_config\n        )\n\n        companies = json.loads(result.extracted_content)\n        print(f\"Successfully extracted {len(companies)} companies\")\n        print(json.dumps(companies[0], indent=2))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Focused Content Extraction with LLMContentFilter",
    "codeDescription": "Configuration for LLMContentFilter that selectively extracts specific types of content such as technical documentation and code examples. This approach reformats content into clear, well-structured markdown.",
    "codeLanguage": "python",
    "codeTokens": 107,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_6",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "filter = LLMContentFilter(\n    instruction=\"\"\"\n    Focus on extracting specific types of content:\n    - Technical documentation\n    - Code examples\n    - API references\n    Reformat the content into clear, well-structured markdown\n    \"\"\",\n    chunk_token_threshold=4096\n)"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Implementing Topic-Based Segmentation with TextTiling",
    "codeDescription": "A class that uses NLTK's TextTilingTokenizer to create topic-coherent chunks. This approach identifies topical boundaries in text for more meaningful segmentation.",
    "codeLanguage": "python",
    "codeTokens": 126,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-11_snippet_2",
    "pageTitle": "Chunking Strategies for Text Processing",
    "codeList": [
      {
        "language": "python",
        "code": "from nltk.tokenize import TextTilingTokenizer\n\nclass TopicSegmentationChunking:\n    def __init__(self):\n        self.tokenizer = TextTilingTokenizer()\n\n    def chunk(self, text):\n        return self.tokenizer.tokenize(text)\n\n# Example Usage\ntext = \"\"\"This is an introduction.\nThis is a detailed discussion on the topic.\"\"\"\nchunker = TopicSegmentationChunking()\nprint(chunker.chunk(text))"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Configuring Crawl4AI with BrowserConfig and CrawlerRunConfig",
    "codeDescription": "Example of configuring Crawl4AI using BrowserConfig and CrawlerRunConfig. BrowserConfig controls browser behavior while CrawlerRunConfig manages crawl settings. This example shows how to disable caching with CacheMode.BYPASS to ensure fresh content.",
    "codeLanguage": "python",
    "codeTokens": 195,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#2025-04-11_snippet_1",
    "pageTitle": "Getting Started with Crawl4AI: Web Crawling and Scraping Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def main():\n    browser_conf = BrowserConfig(headless=True)  # or False to see the browser\n    run_conf = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler(config=browser_conf) as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_conf\n        )\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.965
  },
  {
    "codeTitle": "Basic Web Crawling with AsyncWebCrawler in Python",
    "codeDescription": "A minimal script that demonstrates how to use AsyncWebCrawler to fetch a webpage and convert it to Markdown. The crawler launches a headless Chromium browser, fetches the specified URL, and automatically generates Markdown output.",
    "codeLanguage": "python",
    "codeTokens": 134,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#2025-04-11_snippet_0",
    "pageTitle": "Getting Started with Crawl4AI: Web Crawling and Scraping Tutorial",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\")\n        print(result.markdown[:300])  # Print first 300 chars\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.963
  },
  {
    "codeTitle": "Using LLMExtractionStrategy with Pydantic Schema in Python",
    "codeDescription": "This example demonstrates how to use LLMExtractionStrategy with a Pydantic schema for structured data extraction. It includes defining a schema, creating the strategy, and using it with a crawler.",
    "codeLanguage": "python",
    "codeTokens": 196,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_6",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from pydantic import BaseModel\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Define schema\nclass Article(BaseModel):\n    title: str\n    content: str\n    author: str\n\n# Create strategy\nstrategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"ollama/llama2\"),\n    schema=Article.schema(),\n    instruction=\"Extract article details\"\n)\n\n# Use with crawler\nresult = await crawler.arun(\n    url=\"https://example.com/article\",\n    extraction_strategy=strategy\n)\n\n# Access extracted data\ndata = json.loads(result.extracted_content)"
      }
    ],
    "relevance": 0.962
  },
  {
    "codeTitle": "Configuring and Using Hooks in AsyncWebCrawler with Python",
    "codeDescription": "This snippet demonstrates how to configure AsyncWebCrawler, define hook functions, and attach them to the crawler. It includes examples of browser configuration, crawler run configuration, and implementations for various hooks such as on_browser_created, on_page_context_created, before_goto, and others.",
    "codeLanguage": "python",
    "codeTokens": 1274,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/hooks-auth.md#2025-04-11_snippet_0",
    "pageTitle": "Implementing Hooks and Authentication in AsyncWebCrawler",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom playwright.async_api import Page, BrowserContext\n\nasync def main():\n    print(\" Hooks Example: Demonstrating recommended usage\")\n\n    # 1) Configure the browser\n    browser_config = BrowserConfig(\n        headless=True,\n        verbose=True\n    )\n\n    # 2) Configure the crawler run\n    crawler_run_config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\",\n        wait_for=\"body\",\n        cache_mode=CacheMode.BYPASS\n    )\n\n    # 3) Create the crawler instance\n    crawler = AsyncWebCrawler(config=browser_config)\n\n    #\n    # Define Hook Functions\n    #\n\n    async def on_browser_created(browser, **kwargs):\n        # Called once the browser instance is created (but no pages or contexts yet)\n        print(\"[HOOK] on_browser_created - Browser created successfully!\")\n        # Typically, do minimal setup here if needed\n        return browser\n\n    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):\n        # Called right after a new page + context are created (ideal for auth or route config).\n        print(\"[HOOK] on_page_context_created - Setting up page & context.\")\n        \n        # Example 1: Route filtering (e.g., block images)\n        async def route_filter(route):\n            if route.request.resource_type == \"image\":\n                print(f\"[HOOK] Blocking image request: {route.request.url}\")\n                await route.abort()\n            else:\n                await route.continue_()\n\n        await context.route(\"**\", route_filter)\n\n        # Example 2: (Optional) Simulate a login scenario\n        # (We do NOT create or close pages here, just do quick steps if needed)\n        # e.g., await page.goto(\"https://example.com/login\")\n        # e.g., await page.fill(\"input[name='username']\", \"testuser\")\n        # e.g., await page.fill(\"input[name='password']\", \"password123\")\n        # e.g., await page.click(\"button[type='submit']\")\n        # e.g., await page.wait_for_selector(\"#welcome\")\n        # e.g., await context.add_cookies([...])\n        # Then continue\n\n        # Example 3: Adjust the viewport\n        await page.set_viewport_size({\"width\": 1080, \"height\": 600})\n        return page\n\n    async def before_goto(\n        page: Page, context: BrowserContext, url: str, **kwargs\n    ):\n        # Called before navigating to each URL.\n        print(f\"[HOOK] before_goto - About to navigate: {url}\")\n        # e.g., inject custom headers\n        await page.set_extra_http_headers({\n            \"Custom-Header\": \"my-value\"\n        })\n        return page\n\n    async def after_goto(\n        page: Page, context: BrowserContext, \n        url: str, response, **kwargs\n    ):\n        # Called after navigation completes.\n        print(f\"[HOOK] after_goto - Successfully loaded: {url}\")\n        # e.g., wait for a certain element if we want to verify\n        try:\n            await page.wait_for_selector('.content', timeout=1000)\n            print(\"[HOOK] Found .content element!\")\n        except:\n            print(\"[HOOK] .content not found, continuing anyway.\")\n        return page\n\n    async def on_user_agent_updated(\n        page: Page, context: BrowserContext, \n        user_agent: str, **kwargs\n    ):\n        # Called whenever the user agent updates.\n        print(f\"[HOOK] on_user_agent_updated - New user agent: {user_agent}\")\n        return page\n\n    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):\n        # Called after custom JavaScript execution begins.\n        print(\"[HOOK] on_execution_started - JS code is running!\")\n        return page\n\n    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):\n        # Called before final HTML retrieval.\n        print(\"[HOOK] before_retrieve_html - We can do final actions\")\n        # Example: Scroll again\n        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n        return page\n\n    async def before_return_html(\n        page: Page, context: BrowserContext, html: str, **kwargs\n    ):\n        # Called just before returning the HTML in the result.\n        print(f\"[HOOK] before_return_html - HTML length: {len(html)}\")\n        return page\n\n    #\n    # Attach Hooks\n    #\n\n    crawler.crawler_strategy.set_hook(\"on_browser_created\", on_browser_created)\n    crawler.crawler_strategy.set_hook(\n        \"on_page_context_created\", on_page_context_created\n    )\n    crawler.crawler_strategy.set_hook(\"before_goto\", before_goto)\n    crawler.crawler_strategy.set_hook(\"after_goto\", after_goto)\n    crawler.crawler_strategy.set_hook(\n        \"on_user_agent_updated\", on_user_agent_updated\n    )\n    crawler.crawler_strategy.set_hook(\n        \"on_execution_started\", on_execution_started\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_retrieve_html\", before_retrieve_html\n    )\n    crawler.crawler_strategy.set_hook(\n        \"before_return_html\", before_return_html\n    )\n\n    await crawler.start()\n\n    # 4) Run the crawler on an example page\n    url = \"https://example.com\"\n    result = await crawler.arun(url, config=crawler_run_config)\n    \n    if result.success:\n        print(\"\\nCrawled URL:\", result.url)\n        print(\"HTML length:\", len(result.html))\n    else:\n        print(\"Error:\", result.error_message)\n\n    await crawler.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Content Filtering Pipeline Implementation in Python",
    "codeDescription": "Demonstrates a complete pipeline for extracting pricing features from websites using CosineStrategy. It includes a custom async function that processes extraction results and returns structured data with extracted content and metadata.",
    "codeLanguage": "python",
    "codeTokens": 179,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_9",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "strategy = CosineStrategy(\n    semantic_filter=\"pricing plans features\",\n    word_count_threshold=15,\n    sim_threshold=0.5,\n    top_k=3\n)\n\nasync def extract_pricing_features(url: str):\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=url,\n            extraction_strategy=strategy\n        )\n        \n        if result.success:\n            content = json.loads(result.extracted_content)\n            return {\n                'pricing_features': content,\n                'clusters': len(content),\n                'similarity_scores': [item['score'] for item in content]\n            }"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Basic Usage of CosineStrategy with AsyncWebCrawler in Python",
    "codeDescription": "Demonstrates how to initialize a CosineStrategy object with basic parameters and use it with AsyncWebCrawler to extract content from a URL. The strategy is configured with semantic filtering, word count threshold, and similarity threshold.",
    "codeLanguage": "python",
    "codeTokens": 162,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_0",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.extraction_strategy import CosineStrategy\n\nstrategy = CosineStrategy(\n    semantic_filter=\"product reviews\",    # Target content type\n    word_count_threshold=10,             # Minimum words per cluster\n    sim_threshold=0.3                    # Similarity threshold\n)\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com/reviews\",\n        extraction_strategy=strategy\n    )\n    \n    content = result.extracted_content"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Using OverlappingWindowChunking with LLMExtractionStrategy in Python",
    "codeDescription": "This example demonstrates how to use OverlappingWindowChunking strategy with LLMExtractionStrategy for processing long articles. It shows creating a chunking strategy and using it with an extraction strategy.",
    "codeLanguage": "python",
    "codeTokens": 185,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_8",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.chunking_strategy import OverlappingWindowChunking\nfrom crawl4ai import LLMConfig\n\n# Create chunking strategy\nchunker = OverlappingWindowChunking(\n    window_size=500,  # 500 words per chunk\n    overlap=50        # 50 words overlap\n)\n\n# Use with extraction strategy\nstrategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"ollama/llama2\"),\n    chunking_strategy=chunker\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/long-article\",\n    extraction_strategy=strategy\n)"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Configuring DFSDeepCrawlStrategy in Python",
    "codeDescription": "This snippet demonstrates how to set up the DFSDeepCrawlStrategy, which uses a depth-first approach for crawling. It shows the configuration of parameters such as max_depth, include_external, max_pages, and score_threshold for depth-first search crawling.",
    "codeLanguage": "python",
    "codeTokens": 161,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_2",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.deep_crawling import DFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = DFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=30,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.5,       # Minimum score for URLs to be crawled (optional)\n)"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Setting Extraction Strategy in CrawlerRunConfig for Python",
    "codeDescription": "Demonstrates how to configure an extraction strategy in CrawlerRunConfig for advanced data extraction using CSS or LLM-based methods. The extracted data will be available in the result's extracted_content field.",
    "codeLanguage": "python",
    "codeTokens": 75,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-11_snippet_6",
    "pageTitle": "Crawl4AI Parameter Guide for arun() Method",
    "codeList": [
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    extraction_strategy=my_css_or_llm_strategy\n)"
      }
    ],
    "relevance": 0.96
  },
  {
    "codeTitle": "Load More Example with Hacker News",
    "codeDescription": "Illustrates a multi-step process to load initial content, then click a 'Load More' button to fetch additional content while maintaining the same session with js_only and session_id parameters.",
    "codeLanguage": "python",
    "codeTokens": 378,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-11_snippet_3",
    "pageTitle": "Dynamic Page Interaction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Step 1: Load initial Hacker News page\n    config = CrawlerRunConfig(\n        wait_for=\"css:.athing:nth-child(30)\"  # Wait for 30 items\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"Initial items loaded.\")\n\n        # Step 2: Let's scroll and click the \"More\" link\n        load_more_js = [\n            \"window.scrollTo(0, document.body.scrollHeight);\",\n            # The \"More\" link at page bottom\n            \"document.querySelector('a.morelink')?.click();\"  \n        ]\n        \n        next_page_conf = CrawlerRunConfig(\n            js_code=load_more_js,\n            wait_for=\"\"\"js:() => {\n                return document.querySelectorAll('.athing').length > 30;\n            }\"\"\",\n            # Mark that we do not re-navigate, but run JS in the same session:\n            js_only=True,\n            session_id=\"hn_session\"\n        )\n\n        # Re-use the same crawler session\n        result2 = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # same URL but continuing session\n            config=next_page_conf\n        )\n        total_items = result2.cleaned_html.count(\"athing\")\n        print(\"Items after load-more:\", total_items)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.958
  },
  {
    "codeTitle": "Implementing PruningContentFilter in Crawl4AI",
    "codeDescription": "Example showing how to create and use a PruningContentFilter that discards less relevant nodes based on text density, link density, and tag importance. The filter is configured with threshold settings and integrated into the web crawler workflow.",
    "codeLanguage": "python",
    "codeTokens": 364,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-11_snippet_0",
    "pageTitle": "Fit Markdown with Pruning & BM25 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # Step 1: Create a pruning filter\n    prune_filter = PruningContentFilter(\n        # Lower  more content retained, higher  more content pruned\n        threshold=0.45,           \n        # \"fixed\" or \"dynamic\"\n        threshold_type=\"dynamic\",  \n        # Ignore nodes with <5 words\n        min_word_threshold=5      \n    )\n\n    # Step 2: Insert it into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\n    \n    # Step 3: Pass it to CrawlerRunConfig\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\", \n            config=config\n        )\n        \n        if result.success:\n            # 'fit_markdown' is your pruned content, focusing on \"denser\" text\n            print(\"Raw Markdown length:\", len(result.markdown.raw_markdown))\n            print(\"Fit Markdown length:\", len(result.markdown.fit_markdown))\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Implementing Combined Filters for Deep Crawling in Python",
    "codeDescription": "This code snippet shows how to combine multiple filters using FilterChain in Crawl4AI. It demonstrates the use of URLPatternFilter, DomainFilter, and ContentTypeFilter to create a sophisticated filtering strategy for targeted crawling.",
    "codeLanguage": "python",
    "codeTokens": 210,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_7",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    URLPatternFilter,\n    DomainFilter,\n    ContentTypeFilter\n)\n\n# Create a chain of filters\nfilter_chain = FilterChain([\n    # Only follow URLs with specific patterns\n    URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\"]),\n    \n    # Only crawl specific domains\n    DomainFilter(\n        allowed_domains=[\"docs.example.com\"],\n        blocked_domains=[\"old.docs.example.com\"]\n    ),\n    \n    # Only include specific content types\n    ContentTypeFilter(allowed_types=[\"text/html\"])\n])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=2,\n        filter_chain=filter_chain\n    )\n)"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Generating Filtered Markdown with AsyncWebCrawler in Python",
    "codeDescription": "This code demonstrates how to use AsyncWebCrawler with a PruningContentFilter to generate both raw and filtered markdown from a crawled webpage. It utilizes the DefaultMarkdownGenerator with custom options and prints the results.",
    "codeLanguage": "python",
    "codeTokens": 264,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_7",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(threshold=0.6),\n            options={\"ignore_links\": True}\n        )\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://news.example.com/tech\", config=config)\n        if result.success:\n            print(\"Raw markdown:\\n\", result.markdown)\n            \n            # If a filter is used, we also have .fit_markdown:\n            md_object = result.markdown  # or your equivalent\n            print(\"Filtered markdown:\\n\", md_object.fit_markdown)\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Implementing Streaming Mode Crawling in Python",
    "codeDescription": "This code snippet shows how to use Crawl4AI in streaming mode, where results are processed as they become available. This is beneficial for real-time applications, progressive display, or when handling many pages to reduce memory pressure.",
    "codeLanguage": "python",
    "codeTokens": 143,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_5",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=True  # Enable streaming\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Returns an async iterator\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        # Process each result as it becomes available\n        process_result(result)"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Using MemoryAdaptiveDispatcher in Batch and Stream Modes with Python",
    "codeDescription": "This code demonstrates how to use the MemoryAdaptiveDispatcher in both batch and stream modes. It configures the dispatcher with memory thresholds and check intervals, then shows how to use it with AsyncWebCrawler for crawling multiple URLs.",
    "codeLanguage": "python",
    "codeTokens": 374,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, MemoryAdaptiveDispatcher\nimport asyncio\n\n# Configure the dispatcher (optional, defaults are used if not provided)\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=80.0,  # Pause if memory usage exceeds 80%\n    check_interval=0.5,  # Check memory every 0.5 seconds\n)\n\nasync def batch_mode():\n    async with AsyncWebCrawler() as crawler:\n        results = await crawler.arun_many(\n            urls=[\"https://docs.crawl4ai.com\", \"https://github.com/unclecode/crawl4ai\"],\n            config=CrawlerRunConfig(stream=False),  # Batch mode\n            dispatcher=dispatcher,\n        )\n        for result in results:\n            print(f\"Crawled: {result.url} with status code: {result.status_code}\")\n\nasync def stream_mode():\n    async with AsyncWebCrawler() as crawler:\n        # OR, for streaming:\n        async for result in await crawler.arun_many(\n            urls=[\"https://docs.crawl4ai.com\", \"https://github.com/unclecode/crawl4ai\"],\n            config=CrawlerRunConfig(stream=True),\n            dispatcher=dispatcher,\n        ):\n            print(f\"Crawled: {result.url} with status code: {result.status_code}\")\n\nprint(\"Dispatcher in batch mode:\")\nasyncio.run(batch_mode())\nprint(\"-\" * 50)\nprint(\"Dispatcher in stream mode:\")\nasyncio.run(stream_mode())"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Initializing RateLimiter with Custom Settings in Python",
    "codeDescription": "Example of creating a RateLimiter instance with custom settings in Crawl4AI. This configuration sets custom delay ranges, max delay cap, retry attempts, and HTTP status codes that trigger rate limiting.",
    "codeLanguage": "python",
    "codeTokens": 168,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_1",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import RateLimiter\n\n# Create a RateLimiter with custom settings\nrate_limiter = RateLimiter(\n    base_delay=(2.0, 4.0),  # Random delay between 2-4 seconds\n    max_delay=30.0,         # Cap delay at 30 seconds\n    max_retries=5,          # Retry up to 5 times on rate-limiting errors\n    rate_limit_codes=[429, 503]  # Handle these HTTP status codes\n)"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Implementing Streaming Web Crawler in Python with crawl4ai",
    "codeDescription": "Demonstrates how to set up a streaming web crawler that processes results in real-time using AsyncWebCrawler with MemoryAdaptiveDispatcher. Includes memory management and detailed monitoring configuration.",
    "codeLanguage": "python",
    "codeTokens": 248,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_6",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "async def crawl_streaming():\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=True  # Enable streaming mode\n    )\n    \n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=10,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # Process results as they become available\n        async for result in await crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        ):\n            if result.success:\n                # Process each result immediately\n                await process_result(result)\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Handling CrawlResult in Python for crawl4ai",
    "codeDescription": "This async function demonstrates how to access and process various properties of the CrawlResult object. It handles basic info, HTML content, Markdown output, media and links, extracted content, and screenshots/PDFs. The function also includes error handling for unsuccessful crawls.",
    "codeLanguage": "python",
    "codeTokens": 334,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-11_snippet_7",
    "pageTitle": "CrawlResult Class Reference",
    "codeList": [
      {
        "language": "python",
        "code": "async def handle_result(result: CrawlResult):\n    if not result.success:\n        print(\"Crawl error:\", result.error_message)\n        return\n    \n    # Basic info\n    print(\"Crawled URL:\", result.url)\n    print(\"Status code:\", result.status_code)\n    \n    # HTML\n    print(\"Original HTML size:\", len(result.html))\n    print(\"Cleaned HTML size:\", len(result.cleaned_html or \"\"))\n\n    # Markdown output\n    if result.markdown:\n        print(\"Raw Markdown:\", result.markdown.raw_markdown[:300])\n        print(\"Citations Markdown:\", result.markdown.markdown_with_citations[:300])\n        if result.markdown.fit_markdown:\n            print(\"Fit Markdown:\", result.markdown.fit_markdown[:200])\n\n    # Media & Links\n    if \"images\" in result.media:\n        print(\"Image count:\", len(result.media[\"images\"]))\n    if \"internal\" in result.links:\n        print(\"Internal link count:\", len(result.links[\"internal\"]))\n\n    # Extraction strategy result\n    if result.extracted_content:\n        print(\"Structured data:\", result.extracted_content)\n    \n    # Screenshot/PDF\n    if result.screenshot:\n        print(\"Screenshot length:\", len(result.screenshot))\n    if result.pdf:\n        print(\"PDF bytes length:\", len(result.pdf))"
      }
    ],
    "relevance": 0.955
  },
  {
    "codeTitle": "Basic Web Crawling with Crawl4AI",
    "codeDescription": "Demonstrates a simple web crawl using AsyncWebCrawler to fetch content from a webpage and print the first 500 characters of the extracted markdown.",
    "codeLanguage": "python",
    "codeTokens": 140,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def simple_crawl():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True # By default this is False, meaning the cache will be used\n        )\n        print(result.markdown.raw_markdown[:500])  # Print the first 500 characters\n        \nasyncio.run(simple_crawl())"
      }
    ],
    "relevance": 0.953
  },
  {
    "codeTitle": "Generating Extraction Schema using LLM in Python",
    "codeDescription": "This code snippet shows how to use the schema generation utility in Crawl4AI to automatically create an extraction schema for a given HTML structure using OpenAI's GPT-4 model.",
    "codeLanguage": "python",
    "codeTokens": 226,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-11_snippet_5",
    "pageTitle": "JSON Schema-Based Web Extraction Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, JsonXPathExtractionStrategy\nfrom crawl4ai import LLMConfig\n\n# Sample HTML with product information\nhtml = \"\"\"\n<div class=\"product-card\">\n    <h2 class=\"title\">Gaming Laptop</h2>\n    <div class=\"price\">$999.99</div>\n    <div class=\"specs\">\n        <ul>\n            <li>16GB RAM</li>\n            <li>1TB SSD</li>\n        </ul>\n    </div>\n</div>\n\"\"\"\n\n# Option 1: Using OpenAI (requires API token)\ncss_schema = JsonCssExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"css\", \n    llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-openai-token\")\n)"
      }
    ],
    "relevance": 0.952
  },
  {
    "codeTitle": "Combining PruningContentFilter and BM25ContentFilter in Two Passes with Python",
    "codeDescription": "This code demonstrates a two-pass approach to content filtering. It first applies PruningContentFilter to remove noisy content, then uses BM25ContentFilter to rank the remaining content against a user query. This method allows for more precise content extraction without re-crawling the webpage.",
    "codeLanguage": "python",
    "codeTokens": 568,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_9",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter\nfrom bs4 import BeautifulSoup\n\nasync def main():\n    # 1. Crawl with minimal or no markdown generator, just get raw HTML\n    config = CrawlerRunConfig(\n        # If you only want raw HTML, you can skip passing a markdown_generator\n        # or provide one but focus on .html in this example\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/tech-article\", config=config)\n\n        if not result.success or not result.html:\n            print(\"Crawl failed or no HTML content.\")\n            return\n        \n        raw_html = result.html\n        \n        # 2. First pass: PruningContentFilter on raw HTML\n        pruning_filter = PruningContentFilter(threshold=0.5, min_word_threshold=50)\n        \n        # filter_content returns a list of \"text chunks\" or cleaned HTML sections\n        pruned_chunks = pruning_filter.filter_content(raw_html)\n        # This list is basically pruned content blocks, presumably in HTML or text form\n        \n        # For demonstration, let's combine these chunks back into a single HTML-like string\n        # or you could do further processing. It's up to your pipeline design.\n        pruned_html = \"\\n\".join(pruned_chunks)\n        \n        # 3. Second pass: BM25ContentFilter with a user query\n        bm25_filter = BM25ContentFilter(\n            user_query=\"machine learning\",\n            bm25_threshold=1.2,\n            language=\"english\"\n        )\n        \n        # returns a list of text chunks\n        bm25_chunks = bm25_filter.filter_content(pruned_html)  \n        \n        if not bm25_chunks:\n            print(\"Nothing matched the BM25 query after pruning.\")\n            return\n        \n        # 4. Combine or display final results\n        final_text = \"\\n---\\n\".join(bm25_chunks)\n        \n        print(\"==== PRUNED OUTPUT (first pass) ====\")\n        print(pruned_html[:500], \"... (truncated)\")  # preview\n\n        print(\"\\n==== BM25 OUTPUT (second pass) ====\")\n        print(final_text[:500], \"... (truncated)\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Handling Dynamic Content in Web Crawling",
    "codeDescription": "Shows how to handle dynamic content by executing JavaScript code during crawling to interact with elements like clicking a 'Load More' button and waiting for content to load.",
    "codeLanguage": "python",
    "codeTokens": 251,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "async def crawl_dynamic_content():\n    # You can use wait_for to wait for a condition to be met before returning the result\n    # wait_for = \"() => {\n    #     return Array.from(document.querySelectorAll('article.tease-card')).length > 10;\n    # }\"\n\n    # wait_for can be also just a css selector\n    # wait_for = \"article.tease-card:nth-child(10)\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        js_code = [\n            \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\"\n        ]\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            js_code=js_code,\n            # wait_for=wait_for,\n            bypass_cache=True,\n        )\n        print(result.markdown.raw_markdown[:500])  # Print first 500 characters\n\nasyncio.run(crawl_dynamic_content())"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Implementing Custom Scraping Strategy for Crawl4AI in Python",
    "codeDescription": "This code snippet shows how to create a custom scraping strategy by inheriting from ContentScrapingStrategy. It demonstrates the structure of the ScrapingResult object that needs to be returned.",
    "codeLanguage": "python",
    "codeTokens": 400,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_9",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links\n\nclass CustomScrapingStrategy(ContentScrapingStrategy):\n    def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # Implement your custom scraping logic here\n        return ScrapingResult(\n            cleaned_html=\"<html>...</html>\",  # Cleaned HTML content\n            success=True,                     # Whether scraping was successful\n            media=Media(\n                images=[                      # List of images found\n                    MediaItem(\n                        src=\"https://example.com/image.jpg\",\n                        alt=\"Image description\",\n                        desc=\"Surrounding text\",\n                        score=1,\n                        type=\"image\",\n                        group_id=1,\n                        format=\"jpg\",\n                        width=800\n                    )\n                ],\n                videos=[],                    # List of videos (same structure as images)\n                audios=[]                     # List of audio files (same structure as images)\n            ),\n            links=Links(\n                internal=[                    # List of internal links\n                    Link(\n                        href=\"https://example.com/page\",\n                        text=\"Link text\",\n                        title=\"Link title\",\n                        base_domain=\"example.com\"\n                    )\n                ],\n                external=[]                   # List of external links (same structure)\n            ),\n            metadata={                        # Additional metadata\n                \"title\": \"Page Title\",\n                \"description\": \"Page description\"\n            }\n        )\n\n    async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:\n        # For simple cases, you can use the sync version\n        return await asyncio.to_thread(self.scrap, url, html, **kwargs)"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Excluding External and Social Media Links in Crawl4AI",
    "codeDescription": "This example shows how to configure Crawl4AI to exclude external links and social media links during crawling. It uses CrawlerRunConfig to set exclusion parameters and then performs a crawl with these settings.",
    "codeLanguage": "python",
    "codeTokens": 238,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#2025-04-11_snippet_1",
    "pageTitle": "Link and Media Extraction in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    crawler_cfg = CrawlerRunConfig(\n        exclude_external_links=True,          # No links outside primary domain\n        exclude_social_media_links=True       # Skip recognized social media domains\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            \"https://www.example.com\",\n            config=crawler_cfg\n        )\n        if result.success:\n            print(\"[OK] Crawled:\", result.url)\n            print(\"Internal links count:\", len(result.links.get(\"internal\", [])))\n            print(\"External links count:\", len(result.links.get(\"external\", [])))  \n            # Likely zero external links in this scenario\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Configuring Crawler Parameters in Python",
    "codeDescription": "This code snippet demonstrates how to configure various crawler parameters for a Crawl4AI request. It includes browser configuration, performance settings, anti-detection features, and session management options.",
    "codeLanguage": "python",
    "codeTokens": 274,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_10",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "request = {\n    \"urls\": \"https://example.com\",\n    \"crawler_params\": {\n        # Browser Configuration\n        \"headless\": True,                    # Run in headless mode\n        \"browser_type\": \"chromium\",          # chromium/firefox/webkit\n        \"user_agent\": \"custom-agent\",        # Custom user agent\n        \"proxy\": \"http://proxy:8080\",        # Proxy configuration\n        \n        # Performance & Behavior\n        \"page_timeout\": 30000,               # Page load timeout (ms)\n        \"verbose\": True,                     # Enable detailed logging\n        \"semaphore_count\": 5,               # Concurrent request limit\n        \n        # Anti-Detection Features\n        \"simulate_user\": True,               # Simulate human behavior\n        \"magic\": True,                       # Advanced anti-detection\n        \"override_navigator\": True,          # Override navigator properties\n        \n        # Session Management\n        \"user_data_dir\": \"./browser-data\",   # Browser profile location\n        \"use_managed_browser\": True,         # Use persistent browser\n    }\n}"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Combining Lazy Load with Image and Domain Filters in Python",
    "codeDescription": "Shows how to combine lazy-loading configuration with image and domain filtering options to create a comprehensive crawling strategy.",
    "codeLanguage": "python",
    "codeTokens": 97,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/lazy-loading.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI Image Handling Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    wait_for_images=True,\n    scan_full_page=True,\n    scroll_delay=0.5,\n\n    # Filter out external images if you only want local ones\n    exclude_external_images=True,\n\n    # Exclude certain domains for links\n    exclude_domains=[\"spammycdn.com\"],\n)"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Configuring BFSDeepCrawlStrategy in Python",
    "codeDescription": "This snippet shows how to configure the BFSDeepCrawlStrategy with various parameters such as max_depth, include_external, max_pages, and score_threshold. It demonstrates the key options available for customizing the breadth-first search crawling strategy.",
    "codeLanguage": "python",
    "codeTokens": 158,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_1",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n\n# Basic configuration\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=2,               # Crawl initial page + 2 levels deep\n    include_external=False,    # Stay within the same domain\n    max_pages=50,              # Maximum number of pages to crawl (optional)\n    score_threshold=0.3,       # Minimum score for URLs to be crawled (optional)\n)"
      }
    ],
    "relevance": 0.95
  },
  {
    "codeTitle": "Using JsonCssExtractionStrategy for Product List Extraction in Python",
    "codeDescription": "This example shows how to use JsonCssExtractionStrategy for extracting product information from an HTML page using CSS selectors. It includes defining a schema and using the strategy with a crawler.",
    "codeLanguage": "python",
    "codeTokens": 229,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_7",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\n# Define schema\nschema = {\n    \"name\": \"Product List\",\n    \"baseSelector\": \".product-card\",\n    \"fields\": [\n        {\n            \"name\": \"title\",\n            \"selector\": \"h2.title\",\n            \"type\": \"text\"\n        },\n        {\n            \"name\": \"price\",\n            \"selector\": \".price\",\n            \"type\": \"text\",\n            \"transform\": \"strip\"\n        },\n        {\n            \"name\": \"image\",\n            \"selector\": \"img\",\n            \"type\": \"attribute\",\n            \"attribute\": \"src\"\n        }\n    ]\n}\n\n# Create and use strategy\nstrategy = JsonCssExtractionStrategy(schema)\nresult = await crawler.arun(\n    url=\"https://example.com/products\",\n    extraction_strategy=strategy\n)"
      }
    ],
    "relevance": 0.948
  },
  {
    "codeTitle": "Initializing JsonCssExtractionStrategy in Python for CSS-based Extraction",
    "codeDescription": "This snippet illustrates the initialization of JsonCssExtractionStrategy class for CSS selector-based structured data extraction. It includes the schema structure for defining extraction rules.",
    "codeLanguage": "python",
    "codeTokens": 233,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_2",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "JsonCssExtractionStrategy(\n    schema: Dict[str, Any],    # Extraction schema\n    verbose: bool = False      # Enable verbose logging\n)\n\n# Schema Structure\nschema = {\n    \"name\": str,              # Schema name\n    \"baseSelector\": str,      # Base CSS selector\n    \"fields\": [               # List of fields to extract\n        {\n            \"name\": str,      # Field name\n            \"selector\": str,  # CSS selector\n            \"type\": str,     # Field type: \"text\", \"attribute\", \"html\", \"regex\"\n            \"attribute\": str, # For type=\"attribute\"\n            \"pattern\": str,  # For type=\"regex\"\n            \"transform\": str, # Optional: \"lowercase\", \"uppercase\", \"strip\"\n            \"default\": Any    # Default value if extraction fails\n        }\n    ]\n}"
      }
    ],
    "relevance": 0.948
  },
  {
    "codeTitle": "Comprehensive Example of CrawlerRunConfig Usage in Python",
    "codeDescription": "A full example demonstrating the use of multiple CrawlerRunConfig parameters with AsyncWebCrawler. It includes content processing, JavaScript execution, extraction strategy, and media capture options.",
    "codeLanguage": "python",
    "codeTokens": 457,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-11_snippet_7",
    "pageTitle": "Crawl4AI Parameter Guide for arun() Method",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    # Example schema\n    schema = {\n        \"name\": \"Articles\",\n        \"baseSelector\": \"article.post\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\",  \"selector\": \"a\",  \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n\n    run_config = CrawlerRunConfig(\n        # Core\n        verbose=True,\n        cache_mode=CacheMode.ENABLED,\n        check_robots_txt=True,   # Respect robots.txt rules\n        \n        # Content\n        word_count_threshold=10,\n        css_selector=\"main.content\",\n        excluded_tags=[\"nav\", \"footer\"],\n        exclude_external_links=True,\n        \n        # Page & JS\n        js_code=\"document.querySelector('.show-more')?.click();\",\n        wait_for=\"css:.loaded-block\",\n        page_timeout=30000,\n        \n        # Extraction\n        extraction_strategy=JsonCssExtractionStrategy(schema),\n        \n        # Session\n        session_id=\"persistent_session\",\n        \n        # Media\n        screenshot=True,\n        pdf=True,\n        \n        # Anti-bot\n        simulate_user=True,\n        magic=True,\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/posts\", config=run_config)\n        if result.success:\n            print(\"HTML length:\", len(result.cleaned_html))\n            print(\"Extraction JSON:\", result.extracted_content)\n            if result.screenshot:\n                print(\"Screenshot length:\", len(result.screenshot))\n            if result.pdf:\n                print(\"PDF bytes length:\", len(result.pdf))\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.947
  },
  {
    "codeTitle": "Implementing Batch Processing with MemoryAdaptiveDispatcher in Python",
    "codeDescription": "Complete example of batch processing URLs with AsyncWebCrawler using MemoryAdaptiveDispatcher. This asynchronous function configures the crawler, sets up memory-adaptive dispatching, and processes all results after completion.",
    "codeLanguage": "python",
    "codeTokens": 261,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_5",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "async def crawl_batch():\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        stream=False  # Default: get all results at once\n    )\n    \n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=10,\n        monitor=CrawlerMonitor(\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # Get all results at once\n        results = await crawler.arun_many(\n            urls=urls,\n            config=run_config,\n            dispatcher=dispatcher\n        )\n        \n        # Process all results after completion\n        for result in results:\n            if result.success:\n                await process_result(result)\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Extracting Crypto Prices using XPath with Raw HTML in Python",
    "codeDescription": "Shows how to use JsonXPathExtractionStrategy with raw HTML input for testing. Demonstrates XPath selectors and the raw:// scheme for direct HTML processing without network requests.",
    "codeLanguage": "python",
    "codeTokens": 485,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-11_snippet_1",
    "pageTitle": "JSON Schema-Based Web Extraction Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonXPathExtractionStrategy\n\nasync def extract_crypto_prices_xpath():\n    # 1. Minimal dummy HTML with some repeating rows\n    dummy_html = \"\"\"\n    <html>\n      <body>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Bitcoin</h2>\n          <span class='coin-price'>$28,000</span>\n        </div>\n        <div class='crypto-row'>\n          <h2 class='coin-name'>Ethereum</h2>\n          <span class='coin-price'>$1,800</span>\n        </div>\n      </body>\n    </html>\n    \"\"\"\n\n    # 2. Define the JSON schema (XPath version)\n    schema = {\n        \"name\": \"Crypto Prices via XPath\",\n        \"baseSelector\": \"//div[@class='crypto-row']\",\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \".//h2[@class='coin-name']\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \".//span[@class='coin-price']\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 3. Place the strategy in the CrawlerRunConfig\n    config = CrawlerRunConfig(\n        extraction_strategy=JsonXPathExtractionStrategy(schema, verbose=True)\n    )\n\n    # 4. Use raw:// scheme to pass dummy_html directly\n    raw_url = f\"raw://{dummy_html}\"\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=raw_url,\n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin rows\")\n        if data:\n            print(\"First item:\", data[0])\n\nasyncio.run(extract_crypto_prices_xpath())"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Crawling Dynamic Content with Session Management in Python",
    "codeDescription": "Example of crawling GitHub commits across multiple pages while preserving session state. This uses JsonCssExtractionStrategy for content extraction and employs JavaScript execution to navigate between pages.",
    "codeLanguage": "python",
    "codeTokens": 429,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-11_snippet_1",
    "pageTitle": "Session Management in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_configs import CrawlerRunConfig\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai.cache_context import CacheMode\n\nasync def crawl_dynamic_content():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"github_commits_session\"\n        url = \"https://github.com/microsoft/TypeScript/commits/main\"\n        all_commits = []\n\n        # Define extraction schema\n        schema = {\n            \"name\": \"Commit Extractor\",\n            \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n            \"fields\": [{\n                \"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\"\n            }],\n        }\n        extraction_strategy = JsonCssExtractionStrategy(schema)\n\n        # JavaScript and wait configurations\n        js_next_page = \"\"\"document.querySelector('a[data-testid=\"pagination-next-button\"]').click();\"\"\"\n        wait_for = \"\"\"() => document.querySelectorAll('li.Box-sc-g0xbh4-0').length > 0\"\"\"\n\n        # Crawl multiple pages\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                extraction_strategy=extraction_strategy,\n                js_code=js_next_page if page > 0 else None,\n                wait_for=wait_for if page > 0 else None,\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            if result.success:\n                commits = json.loads(result.extracted_content)\n                all_commits.extend(commits)\n                print(f\"Page {page + 1}: Found {len(commits)} commits\")\n\n        # Clean up session\n        await crawler.crawler_strategy.kill_session(session_id)\n        return all_commits"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Structured Data Extraction with CSS Selectors in Python",
    "codeDescription": "Complete example showing CSS-based structured data extraction from raw HTML. The example defines a schema for targeting elements, processes HTML directly (using raw:// protocol), and retrieves structured JSON data from the extracted_content field.",
    "codeLanguage": "python",
    "codeTokens": 302,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-11_snippet_3",
    "pageTitle": "CrawlResult Object and Output Structure in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    schema = {\n        \"name\": \"Example Items\",\n        \"baseSelector\": \"div.item\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"h2\", \"type\": \"text\"},\n            {\"name\": \"link\", \"selector\": \"a\", \"type\": \"attribute\", \"attribute\": \"href\"}\n        ]\n    }\n    raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\"\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"raw://\" + raw_html,\n            config=CrawlerRunConfig(\n                cache_mode=CacheMode.BYPASS,\n                extraction_strategy=JsonCssExtractionStrategy(schema)\n            )\n        )\n        data = json.loads(result.extracted_content)\n        print(data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Using Custom Hooks in Web Crawling Workflow",
    "codeDescription": "Shows how to implement custom hooks that execute at specific stages of the crawling process, allowing for customization of the crawling behavior such as setting headers or processing content.",
    "codeLanguage": "python",
    "codeTokens": 162,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_7",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "async def custom_hook_workflow():\n    async with AsyncWebCrawler() as crawler:\n        # Set a 'before_goto' hook to run custom code just before navigation\n        crawler.crawler_strategy.set_hook(\"before_goto\", lambda page: print(\"[Hook] Preparing to navigate...\"))\n        \n        # Perform the crawl operation\n        result = await crawler.arun(\n            url=\"https://crawl4ai.com\",\n            bypass_cache=True\n        )\n        print(result.markdown.raw_markdown[:500])  # Display the first 500 characters\n\nasyncio.run(custom_hook_workflow())"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Implementing Basic URL Pattern Filter in Python",
    "codeDescription": "This snippet demonstrates how to use a basic URL pattern filter with Crawl4AI. It configures the crawler to only follow URLs containing 'blog' or 'docs', allowing for targeted crawling of specific content types.",
    "codeLanguage": "python",
    "codeTokens": 142,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_6",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter\n\n# Only follow URLs containing \"blog\" or \"docs\"\nurl_filter = URLPatternFilter(patterns=[\"*blog*\", \"*docs*\"])\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([url_filter])\n    )\n)"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "LLM-Based Extraction in Crawl4AI",
    "codeDescription": "This snippet demonstrates how to use LLMExtractionStrategy for AI-driven parsing of crawled content. It extracts a headline and summary from the content using OpenAI's GPT-4 model.",
    "codeLanguage": "python",
    "codeTokens": 278,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_6",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nimport json\nfrom pydantic import BaseModel, Field\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\n\nclass ArticleData(BaseModel):\n    headline: str\n    summary: str\n\nasync def main():\n    llm_strategy = LLMExtractionStrategy(\n        llm_config = LLMConfig(provider=\"openai/gpt-4\",api_token=\"sk-YOUR_API_KEY\")\n        schema=ArticleData.schema(),\n        extraction_type=\"schema\",\n        instruction=\"Extract 'headline' and a short 'summary' from the content.\"\n    )\n\n    config = CrawlerRunConfig(\n        exclude_external_links=True,\n        word_count_threshold=20,\n        extraction_strategy=llm_strategy\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        article = json.loads(result.extracted_content)\n        print(article)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Configuring Crawl Operations with CrawlerRunConfig in Python",
    "codeDescription": "This snippet shows how to configure a CrawlerRunConfig object to control crawl behavior including content waiting conditions, filtering settings, and link handling. Parameters control what content is extracted and how the crawler interacts with pages.",
    "codeLanguage": "python",
    "codeTokens": 126,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-11_snippet_1",
    "pageTitle": "Browser and Crawler Configuration in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nrun_cfg = CrawlerRunConfig(\n    wait_for=\"css:.main-content\",\n    word_count_threshold=15,\n    excluded_tags=[\"nav\", \"footer\"],\n    exclude_external_links=True,\n    stream=True,  # Enable streaming for arun_many()\n)"
      }
    ],
    "relevance": 0.945
  },
  {
    "codeTitle": "Handling Media and Links from CrawlResult in Python",
    "codeDescription": "Examples showing how to access and process media (images, videos, audio) and links (internal and external) from a CrawlResult object, including checking relevance scores and extracting link information.",
    "codeLanguage": "python",
    "codeTokens": 88,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-11_snippet_4",
    "pageTitle": "CrawlResult Class Reference",
    "codeList": [
      {
        "language": "python",
        "code": "images = result.media.get(\"images\", [])\nfor img in images:\n    if img.get(\"score\", 0) > 5:\n        print(\"High-value image:\", img[\"src\"])"
      },
      {
        "language": "python",
        "code": "for link in result.links[\"internal\"]:\n    print(f\"Internal link to {link['href']} with text {link['text']}\")"
      }
    ],
    "relevance": 0.943
  },
  {
    "codeTitle": "Comprehensive Crawl4AI Example in Python",
    "codeDescription": "This complete script demonstrates crawling a Wikipedia page, saving its HTML content to a local file, crawling the local file, and crawling the raw HTML content. It verifies consistency between the different crawling methods.",
    "codeLanguage": "python",
    "codeTokens": 637,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-11_snippet_3",
    "pageTitle": "Prefix-Based Input Handling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import os\nimport sys\nimport asyncio\nfrom pathlib import Path\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def main():\n    wikipedia_url = \"https://en.wikipedia.org/wiki/apple\"\n    script_dir = Path(__file__).parent\n    html_file_path = script_dir / \"apple.html\"\n\n    async with AsyncWebCrawler() as crawler:\n        # Step 1: Crawl the Web URL\n        print(\"\\n=== Step 1: Crawling the Wikipedia URL ===\")\n        web_config = CrawlerRunConfig(bypass_cache=True)\n        result = await crawler.arun(url=wikipedia_url, config=web_config)\n\n        if not result.success:\n            print(f\"Failed to crawl {wikipedia_url}: {result.error_message}\")\n            return\n\n        with open(html_file_path, 'w', encoding='utf-8') as f:\n            f.write(result.html)\n        web_crawl_length = len(result.markdown)\n        print(f\"Length of markdown from web crawl: {web_crawl_length}\\n\")\n\n        # Step 2: Crawl from the Local HTML File\n        print(\"=== Step 2: Crawling from the Local HTML File ===\")\n        file_url = f\"file://{html_file_path.resolve()}\"\n        file_config = CrawlerRunConfig(bypass_cache=True)\n        local_result = await crawler.arun(url=file_url, config=file_config)\n\n        if not local_result.success:\n            print(f\"Failed to crawl local file {file_url}: {local_result.error_message}\")\n            return\n\n        local_crawl_length = len(local_result.markdown)\n        assert web_crawl_length == local_crawl_length, \"Markdown length mismatch\"\n        print(\" Markdown length matches between web and local file crawl.\\n\")\n\n        # Step 3: Crawl Using Raw HTML Content\n        print(\"=== Step 3: Crawling Using Raw HTML Content ===\")\n        with open(html_file_path, 'r', encoding='utf-8') as f:\n            raw_html_content = f.read()\n        raw_html_url = f\"raw:{raw_html_content}\"\n        raw_config = CrawlerRunConfig(bypass_cache=True)\n        raw_result = await crawler.arun(url=raw_html_url, config=raw_config)\n\n        if not raw_result.success:\n            print(f\"Failed to crawl raw HTML content: {raw_result.error_message}\")\n            return\n\n        raw_crawl_length = len(raw_result.markdown)\n        assert web_crawl_length == raw_crawl_length, \"Markdown length mismatch\"\n        print(\" Markdown length matches between web and raw HTML crawl.\\n\")\n\n        print(\"All tests passed successfully!\")\n    if html_file_path.exists():\n        os.remove(html_file_path)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.943
  },
  {
    "codeTitle": "Advanced Session Crawling with Custom Execution Hooks in Python",
    "codeDescription": "Demonstrates using custom hooks to handle complex scenarios in session management. This example uses an on_execution_started hook to wait for dynamic content to load before proceeding, ensuring proper data extraction across paginated content.",
    "codeLanguage": "python",
    "codeTokens": 368,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-11_snippet_3",
    "pageTitle": "Session Management in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "async def advanced_session_crawl_with_hooks():\n    first_commit = \"\"\n\n    async def on_execution_started(page):\n        nonlocal first_commit\n        try:\n            while True:\n                await page.wait_for_selector(\"li.commit-item h4\")\n                commit = await page.query_selector(\"li.commit-item h4\")\n                commit = await commit.evaluate(\"(element) => element.textContent\").strip()\n                if commit and commit != first_commit:\n                    first_commit = commit\n                    break\n                await asyncio.sleep(0.5)\n        except Exception as e:\n            print(f\"Warning: New content didn't appear: {e}\")\n\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"commit_session\"\n        url = \"https://github.com/example/repo/commits/main\"\n        crawler.crawler_strategy.set_hook(\"on_execution_started\", on_execution_started)\n\n        js_next_page = \"\"\"document.querySelector('a.pagination-next').click();\"\"\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=js_next_page if page > 0 else None,\n                css_selector=\"li.commit-item\",\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {len(result.extracted_content)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(advanced_session_crawl_with_hooks())"
      }
    ],
    "relevance": 0.942
  },
  {
    "codeTitle": "Testing Crawl4AI Deployment with Python",
    "codeDescription": "This Python script demonstrates how to test a Crawl4AI deployment by submitting a crawl job and waiting for the result. It includes a class for interacting with the Crawl4AI API and a test function for basic crawling.",
    "codeLanguage": "python",
    "codeTokens": 352,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_9",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import requests\nimport json\nimport time\nimport sys\n\nclass Crawl4AiTester:\n    def __init__(self, base_url: str = \"http://localhost:11235\"):\n        self.base_url = base_url\n        \n    def submit_and_wait(self, request_data: dict, timeout: int = 300) -> dict:\n        # Submit crawl job\n        response = requests.post(f\"{self.base_url}/crawl\", json=request_data)\n        task_id = response.json()[\"task_id\"]\n        print(f\"Task ID: {task_id}\")\n        \n        # Poll for result\n        start_time = time.time()\n        while True:\n            if time.time() - start_time > timeout:\n                raise TimeoutError(f\"Task {task_id} timeout\")\n                \n            result = requests.get(f\"{self.base_url}/task/{task_id}\")\n            status = result.json()\n            \n            if status[\"status\"] == \"completed\":\n                return status\n                \n            time.sleep(2)\n\ndef test_deployment():\n    tester = Crawl4AiTester()\n    \n    # Test basic crawl\n    request = {\n        \"urls\": \"https://www.nbcnews.com/business\",\n        \"priority\": 10\n    }\n    \n    result = tester.submit_and_wait(request)\n    print(\"Basic crawl successful!\")\n    print(f\"Content length: {len(result['result']['markdown'])}\")\n\nif __name__ == \"__main__\":\n    test_deployment()"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Integrated JavaScript Execution and Waiting for Crawling in Python",
    "codeDescription": "Advanced technique combining JavaScript execution and waiting logic for handling dynamic content. This approach uses a self-executing async JavaScript function to click the pagination button and wait for new content to appear before proceeding.",
    "codeLanguage": "python",
    "codeTokens": 301,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-11_snippet_4",
    "pageTitle": "Session Management in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "async def integrated_js_and_wait_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"integrated_session\"\n        url = \"https://github.com/example/repo/commits/main\"\n\n        js_next_page_and_wait = \"\"\"\n        (async () => {\n            const getCurrentCommit = () => document.querySelector('li.commit-item h4').textContent.trim();\n            const initialCommit = getCurrentCommit();\n            document.querySelector('a.pagination-next').click();\n            while (getCurrentCommit() === initialCommit) {\n                await new Promise(resolve => setTimeout(resolve, 100));\n            }\n        })();\n        \"\"\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=js_next_page_and_wait if page > 0 else None,\n                css_selector=\"li.commit-item\",\n                js_only=page > 0,\n                cache_mode=CacheMode.BYPASS\n            )\n\n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {len(result.extracted_content)} commits\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(integrated_js_and_wait_crawl())"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Basic JavaScript Execution with Crawl4AI",
    "codeDescription": "Demonstrates how to execute JavaScript commands in a webpage using CrawlerRunConfig with both single and multiple JS commands. The example scrolls to the bottom of the page and optionally clicks a 'Load More' button.",
    "codeLanguage": "python",
    "codeTokens": 294,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-11_snippet_0",
    "pageTitle": "Dynamic Page Interaction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Single JS command\n    config = CrawlerRunConfig(\n        js_code=\"window.scrollTo(0, document.body.scrollHeight);\"\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Example site\n            config=config\n        )\n        print(\"Crawled length:\", len(result.cleaned_html))\n\n    # Multiple commands\n    js_commands = [\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        # 'More' link on Hacker News\n        \"document.querySelector('a.morelink')?.click();\",  \n    ]\n    config = CrawlerRunConfig(js_code=js_commands)\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",  # Another pass\n            config=config\n        )\n        print(\"After scroll+click, length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Implementing Deep Crawling with BestFirstCrawlingStrategy in Python",
    "codeDescription": "This snippet demonstrates how to set up deep crawling using the BestFirstCrawlingStrategy with custom filters and scorers. It includes configuration for domain filtering, URL pattern matching, content type filtering, and keyword relevance scoring.",
    "codeLanguage": "python",
    "codeTokens": 486,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "import time\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\nfrom crawl4ai.deep_crawling import DomainFilter, ContentTypeFilter, FilterChain, URLPatternFilter, KeywordRelevanceScorer, BestFirstCrawlingStrategy\nimport asyncio\n\n# Create a filter chain to filter urls based on patterns, domains and content type\nfilter_chain = FilterChain(\n    [\n        DomainFilter(\n            allowed_domains=[\"docs.crawl4ai.com\"],\n            blocked_domains=[\"old.docs.crawl4ai.com\"],\n        ),\n        URLPatternFilter(patterns=[\"*core*\", \"*advanced*\"],),\n        ContentTypeFilter(allowed_types=[\"text/html\"]),\n    ]\n)\n\n# Create a keyword scorer that prioritises the pages with certain keywords first\nkeyword_scorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"], weight=0.7\n)\n\n# Set up the configuration\ndeep_crawl_config = CrawlerRunConfig(\n    deep_crawl_strategy=BestFirstCrawlingStrategy(\n        max_depth=2,\n        include_external=False,\n        filter_chain=filter_chain,\n        url_scorer=keyword_scorer,\n    ),\n    scraping_strategy=LXMLWebScrapingStrategy(),\n    stream=True,\n    verbose=True,\n)\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        start_time = time.perf_counter()\n        results = []\n        async for result in await crawler.arun(url=\"https://docs.crawl4ai.com\", config=deep_crawl_config):\n            print(f\"Crawled: {result.url} (Depth: {result.metadata['depth']}), score: {result.metadata['score']:.2f}\")\n            results.append(result)\n        duration = time.perf_counter() - start_time\n        print(f\"\\n Crawled {len(results)} high-value pages in {duration:.2f} seconds\")\n\nasyncio.run(main())"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Basic Session-Based Crawling Example in Python",
    "codeDescription": "A simple implementation of session-based crawling with AsyncWebCrawler. This code demonstrates reusing the same session ID across multiple requests, executing JavaScript to load more content dynamically, and properly cleaning up sessions.",
    "codeLanguage": "python",
    "codeTokens": 241,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-11_snippet_2",
    "pageTitle": "Session Management in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\nfrom crawl4ai.cache_context import CacheMode\n\nasync def basic_session_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"dynamic_content_session\"\n        url = \"https://example.com/dynamic-content\"\n\n        for page in range(3):\n            config = CrawlerRunConfig(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.load-more-button').click();\" if page > 0 else None,\n                css_selector=\".content-item\",\n                cache_mode=CacheMode.BYPASS\n            )\n            \n            result = await crawler.arun(config=config)\n            print(f\"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items\")\n\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasyncio.run(basic_session_crawl())"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "LLM Content Filtering Implementation",
    "codeDescription": "Shows how to implement LLM-based content filtering for markdown generation using the DefaultMarkdownGenerator with LLMContentFilter. Includes configuration for LLM provider and API token setup.",
    "codeLanguage": "python",
    "codeTokens": 221,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_6",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import LLMContentFilter\nfrom crawl4ai import LLMConfig\nimport asyncio\n\nllm_config = LLMConfig(provider=\"gemini/gemini-1.5-pro\", api_token=\"env:GEMINI_API_KEY\")\n\nmarkdown_generator = DefaultMarkdownGenerator(\n    content_filter=LLMContentFilter(llm_config=llm_config, instruction=\"Extract key concepts and summaries\")\n)\n\nconfig = CrawlerRunConfig(markdown_generator=markdown_generator)\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://docs.crawl4ai.com\", config=config)\n        print(result.markdown.fit_markdown)\n\nasyncio.run(main())"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Implementing BM25ContentFilter in Crawl4AI",
    "codeDescription": "Example demonstrating how to use BM25ContentFilter which applies the BM25 text ranking algorithm to identify content chunks that best match a specific user query. The example shows configuration parameters and integration with the crawler.",
    "codeLanguage": "python",
    "codeTokens": 298,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-11_snippet_1",
    "pageTitle": "Fit Markdown with Pruning & BM25 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # 1) A BM25 filter with a user query\n    bm25_filter = BM25ContentFilter(\n        user_query=\"startup fundraising tips\",\n        # Adjust for stricter or looser results\n        bm25_threshold=1.2  \n    )\n\n    # 2) Insert into a Markdown Generator\n    md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\n    \n    # 3) Pass to crawler config\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\", \n            config=config\n        )\n        if result.success:\n            print(\"Fit Markdown (BM25 query-based):\")\n            print(result.markdown.fit_markdown)\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Extracting Crypto Prices using CSS Selectors in Python",
    "codeDescription": "Demonstrates how to extract cryptocurrency price data using Crawl4AI's JsonCssExtractionStrategy. The example shows configuration of the schema, crawler setup, and data extraction process using CSS selectors.",
    "codeLanguage": "python",
    "codeTokens": 427,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-11_snippet_0",
    "pageTitle": "JSON Schema-Based Web Extraction Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import json\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def extract_crypto_prices():\n    # 1. Define a simple extraction schema\n    schema = {\n        \"name\": \"Crypto Prices\",\n        \"baseSelector\": \"div.crypto-row\",    # Repeated elements\n        \"fields\": [\n            {\n                \"name\": \"coin_name\",\n                \"selector\": \"h2.coin-name\",\n                \"type\": \"text\"\n            },\n            {\n                \"name\": \"price\",\n                \"selector\": \"span.coin-price\",\n                \"type\": \"text\"\n            }\n        ]\n    }\n\n    # 2. Create the extraction strategy\n    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)\n\n    # 3. Set up your crawler config (if needed)\n    config = CrawlerRunConfig(\n        # e.g., pass js_code or wait_for if the page is dynamic\n        # wait_for=\"css:.crypto-row:nth-child(20)\"\n        cache_mode = CacheMode.BYPASS,\n        extraction_strategy=extraction_strategy,\n    )\n\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        # 4. Run the crawl and extraction\n        result = await crawler.arun(\n            url=\"https://example.com/crypto-prices\",\n            \n            config=config\n        )\n\n        if not result.success:\n            print(\"Crawl failed:\", result.error_message)\n            return\n\n        # 5. Parse the extracted JSON\n        data = json.loads(result.extracted_content)\n        print(f\"Extracted {len(data)} coin entries\")\n        print(json.dumps(data[0], indent=2) if data else \"No data found\")\n\nasyncio.run(extract_crypto_prices())"
      }
    ],
    "relevance": 0.94
  },
  {
    "codeTitle": "Error Handling with AsyncWebCrawler and CosineStrategy in Python",
    "codeDescription": "Provides a complete error handling pattern for extraction with CosineStrategy, including checking for successful extraction, handling empty content results, and catching exceptions during the extraction process.",
    "codeLanguage": "python",
    "codeTokens": 137,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_12",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "try:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    \n    if result.success:\n        content = json.loads(result.extracted_content)\n        if not content:\n            print(\"No relevant content found\")\n    else:\n        print(f\"Extraction failed: {result.error_message}\")\n        \nexcept Exception as e:\n    print(f\"Error during extraction: {str(e)}\")"
      }
    ],
    "relevance": 0.938
  },
  {
    "codeTitle": "Multi-Step Interaction with GitHub Commits",
    "codeDescription": "A comprehensive example showing how to navigate through multiple pages of GitHub commits by clicking 'Next Page' buttons while maintaining session state and waiting for new content to load between steps.",
    "codeLanguage": "python",
    "codeTokens": 620,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-11_snippet_6",
    "pageTitle": "Dynamic Page Interaction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def multi_page_commits():\n    browser_cfg = BrowserConfig(\n        headless=False,  # Visible for demonstration\n        verbose=True\n    )\n    session_id = \"github_ts_commits\"\n    \n    base_wait = \"\"\"js:() => {\n        const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n        return commits.length > 0;\n    }\"\"\"\n\n    # Step 1: Load initial commits\n    config1 = CrawlerRunConfig(\n        wait_for=base_wait,\n        session_id=session_id,\n        cache_mode=CacheMode.BYPASS,\n        # Not using js_only yet since it's our first load\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://github.com/microsoft/TypeScript/commits/main\",\n            config=config1\n        )\n        print(\"Initial commits loaded. Count:\", result.cleaned_html.count(\"commit\"))\n\n        # Step 2: For subsequent pages, we run JS to click 'Next Page' if it exists\n        js_next_page = \"\"\"\n        const selector = 'a[data-testid=\"pagination-next-button\"]';\n        const button = document.querySelector(selector);\n        if (button) button.click();\n        \"\"\"\n        \n        # Wait until new commits appear\n        wait_for_more = \"\"\"js:() => {\n            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');\n            if (!window.firstCommit && commits.length>0) {\n                window.firstCommit = commits[0].textContent;\n                return false;\n            }\n            // If top commit changes, we have new commits\n            const topNow = commits[0]?.textContent.trim();\n            return topNow && topNow !== window.firstCommit;\n        }\"\"\"\n\n        for page in range(2):  # let's do 2 more \"Next\" pages\n            config_next = CrawlerRunConfig(\n                session_id=session_id,\n                js_code=js_next_page,\n                wait_for=wait_for_more,\n                js_only=True,       # We're continuing from the open tab\n                cache_mode=CacheMode.BYPASS\n            )\n            result2 = await crawler.arun(\n                url=\"https://github.com/microsoft/TypeScript/commits/main\",\n                config=config_next\n            )\n            print(f\"Page {page+2} commits count:\", result2.cleaned_html.count(\"commit\"))\n\n        # Optionally kill session\n        await crawler.crawler_strategy.kill_session(session_id)\n\nasync def main():\n    await multi_page_commits()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Configuring LLM Provider Settings in Python",
    "codeDescription": "Example of initializing LLM configuration with OpenAI provider settings. Sets up the provider and API token using environment variables for secure credential management.",
    "codeLanguage": "python",
    "codeTokens": 71,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-11_snippet_5",
    "pageTitle": "Browser and Crawler Configuration in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "llm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv(\"OPENAI_API_KEY\"))"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Implementing Robots.txt Compliant Web Crawler",
    "codeDescription": "Demonstrates how to implement a web crawler that respects robots.txt rules using AsyncWebCrawler. Includes error handling for robots.txt restrictions and concurrent request management.",
    "codeLanguage": "python",
    "codeTokens": 262,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_8",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    urls = [\n        \"https://example1.com\",\n        \"https://example2.com\",\n        \"https://example3.com\"\n    ]\n    \n    config = CrawlerRunConfig(\n        cache_mode=CacheMode.ENABLED,\n        check_robots_txt=True,  # Will respect robots.txt for each URL\n        semaphore_count=3      # Max concurrent requests\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        async for result in crawler.arun_many(urls, config=config):\n            if result.success:\n                print(f\"Successfully crawled {result.url}\")\n            elif result.status_code == 403 and \"robots.txt\" in result.error_message:\n                print(f\"Skipped {result.url} - blocked by robots.txt\")\n            else:\n                print(f\"Failed to crawl {result.url}: {result.error_message}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Optimizing LLMExtractionStrategy for Long Documents in Python",
    "codeDescription": "This snippet shows how to optimize LLMExtractionStrategy for processing long documents by adjusting chunk size and overlap rate. It's part of the best practices section.",
    "codeLanguage": "python",
    "codeTokens": 92,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_9",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "# For long documents\nstrategy = LLMExtractionStrategy(\n    chunk_token_threshold=2000,  # Smaller chunks\n    overlap_rate=0.1           # 10% overlap\n)"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Combining Chunking with Cosine Similarity for Content Extraction",
    "codeDescription": "A class that uses TF-IDF vectorization and cosine similarity to find the most relevant chunks for a given query. This workflow demonstrates how to integrate chunking with similarity-based content extraction.",
    "codeLanguage": "python",
    "codeTokens": 245,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-11_snippet_5",
    "pageTitle": "Chunking Strategies for Text Processing",
    "codeList": [
      {
        "language": "python",
        "code": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass CosineSimilarityExtractor:\n    def __init__(self, query):\n        self.query = query\n        self.vectorizer = TfidfVectorizer()\n\n    def find_relevant_chunks(self, chunks):\n        vectors = self.vectorizer.fit_transform([self.query] + chunks)\n        similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n        return [(chunks[i], similarities[i]) for i in range(len(chunks))]\n\n# Example Workflow\ntext = \"\"\"This is a sample document. It has multiple sentences. \nWe are testing chunking and similarity.\"\"\"\n\nchunker = SlidingWindowChunking(window_size=5, step=3)\nchunks = chunker.chunk(text)\nquery = \"testing chunking\"\nextractor = CosineSimilarityExtractor(query)\nrelevant_chunks = extractor.find_relevant_chunks(chunks)\n\nprint(relevant_chunks)"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Implementing Robots.txt Compliance in Crawl4AI",
    "codeDescription": "Shows how to enable built-in robots.txt support with SQLite caching. This feature checks if crawling is allowed for a specific URL and returns an appropriate status code if access is blocked by robots.txt.",
    "codeLanguage": "python",
    "codeTokens": 96,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-11_snippet_6",
    "pageTitle": "Crawl4AI 0.4.3 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(check_robots_txt=True)\nresult = await crawler.arun(url, config=config)\nif result.status_code == 403:\n    print(\"Access blocked by robots.txt\")"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Implementing Robots.txt Compliance in Crawl4AI (Python)",
    "codeDescription": "Demonstrates how to enable robots.txt checking and compliance in Crawl4AI, which respects crawling rules specified by websites.",
    "codeLanguage": "python",
    "codeTokens": 162,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-11_snippet_5",
    "pageTitle": "Advanced Features in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Enable robots.txt checking in config\n    config = CrawlerRunConfig(\n        check_robots_txt=True  # Will check and respect robots.txt rules\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            \"https://example.com\",\n            config=config\n        )\n        \n        if not result.success and result.status_code == 403:\n            print(\"Access denied by robots.txt\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Initializing AsyncWebCrawler in Python",
    "codeDescription": "Constructor for the AsyncWebCrawler class, detailing its parameters and usage. It includes options for custom crawler strategies, browser configuration, and various legacy parameters.",
    "codeLanguage": "python",
    "codeTokens": 251,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-11_snippet_0",
    "pageTitle": "AsyncWebCrawler Class Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "class AsyncWebCrawler:\n    def __init__(\n        self,\n        crawler_strategy: Optional[AsyncCrawlerStrategy] = None,\n        config: Optional[BrowserConfig] = None,\n        always_bypass_cache: bool = False,           # deprecated\n        always_by_pass_cache: Optional[bool] = None, # also deprecated\n        base_directory: str = ...,\n        thread_safe: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Create an AsyncWebCrawler instance.\n\n        Args:\n            crawler_strategy: \n                (Advanced) Provide a custom crawler strategy if needed.\n            config: \n                A BrowserConfig object specifying how the browser is set up.\n            always_bypass_cache: \n                (Deprecated) Use CrawlerRunConfig.cache_mode instead.\n            base_directory:     \n                Folder for storing caches/logs (if relevant).\n            thread_safe: \n                If True, attempts some concurrency safeguards. Usually False.\n            **kwargs: \n                Additional legacy or debugging parameters.\n        \"\"\"\n    )"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Session-Based Dynamic Content Crawling in Python",
    "codeDescription": "This configuration demonstrates how to set up Crawl4AI for crawling dynamic content using a session-based approach. It includes JavaScript execution and waiting for specific conditions before content extraction.",
    "codeLanguage": "python",
    "codeTokens": 150,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_15",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "request = {\n    \"urls\": \"https://example.com\",\n    \"crawler_params\": {\n        \"session_id\": \"dynamic_session\",\n        \"headless\": False,\n        \"page_timeout\": 60000\n    },\n    \"js_code\": [\"window.scrollTo(0, document.body.scrollHeight);\"],\n    \"wait_for\": \"js:() => document.querySelectorAll('.item').length > 10\",\n    \"extra\": {\n        \"delay_before_return_html\": 2.0\n    }\n}"
      }
    ],
    "relevance": 0.935
  },
  {
    "codeTitle": "Extracting Links and Media with AsyncWebCrawler in Python",
    "codeDescription": "This snippet demonstrates how to use AsyncWebCrawler to extract internal and external links, as well as media items from a web page. It shows how to access and print information about the extracted links and media.",
    "codeLanguage": "python",
    "codeTokens": 225,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#2025-04-11_snippet_0",
    "pageTitle": "Link and Media Extraction in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\"https://www.example.com\")\n    if result.success:\n        internal_links = result.links.get(\"internal\", [])\n        external_links = result.links.get(\"external\", [])\n        print(f\"Found {len(internal_links)} internal links.\")\n        print(f\"Found {len(internal_links)} external links.\")\n        print(f\"Found {len(result.media)} media items.\")\n\n        # Each link is typically a dictionary with fields like:\n        # { \"href\": \"...\", \"text\": \"...\", \"title\": \"...\", \"base_domain\": \"...\" }\n        if internal_links:\n            print(\"Sample Internal Link:\", internal_links[0])\n    else:\n        print(\"Crawl failed:\", result.error_message)"
      }
    ],
    "relevance": 0.933
  },
  {
    "codeTitle": "Session-Based Multi-Page Crawling",
    "codeDescription": "Demonstrates session-based crawling for navigating through multi-page content where each page loads sequentially with the same session context, maintaining browser state across interactions.",
    "codeLanguage": "python",
    "codeTokens": 187,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_9",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "async def multi_page_session_crawl():\n    async with AsyncWebCrawler() as crawler:\n        session_id = \"page_navigation_session\"\n        url = \"https://example.com/paged-content\"\n\n        for page_number in range(1, 4):\n            result = await crawler.arun(\n                url=url,\n                session_id=session_id,\n                js_code=\"document.querySelector('.next-page-button').click();\" if page_number > 1 else None,\n                css_selector=\".content-section\",\n                bypass_cache=True\n            )\n            print(f\"Page {page_number} Content:\")\n            print(result.markdown.raw_markdown[:500])  # Print first 500 characters\n\n# asyncio.run(multi_page_session_crawl())"
      }
    ],
    "relevance": 0.933
  },
  {
    "codeTitle": "Implementing Full-Page Screenshot and PDF Capture with Crawl4AI in Python",
    "codeDescription": "Demonstrates how to use AsyncWebCrawler to capture both screenshots and PDFs from a webpage. The script sets up the crawler, processes a Wikipedia page, and saves both screenshot and PDF outputs to local files. It includes path configuration, async handling, and file saving operations.",
    "codeLanguage": "python",
    "codeTokens": 327,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/full_page_screenshot_and_pdf_export.md#2025-04-11_snippet_0",
    "pageTitle": "Full-Page Screenshot and PDF Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import os, sys\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode\n\n# Adjust paths as needed\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.append(parent_dir)\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        # Request both PDF and screenshot\n        result = await crawler.arun(\n            url='https://en.wikipedia.org/wiki/List_of_common_misconceptions',\n            cache_mode=CacheMode.BYPASS,\n            pdf=True,\n            screenshot=True\n        )\n        \n        if result.success:\n            # Save screenshot\n            if result.screenshot:\n                from base64 import b64decode\n                with open(os.path.join(__location__, \"screenshot.png\"), \"wb\") as f:\n                    f.write(b64decode(result.screenshot))\n            \n            # Save PDF\n            if result.pdf:\n                pdf_bytes = b64decode(result.pdf)\n                with open(os.path.join(__location__, \"page.pdf\"), \"wb\") as f:\n                    f.write(pdf_bytes)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Advanced Custom Clustering Configuration in Python",
    "codeDescription": "Shows how to customize the clustering behavior in CosineStrategy by specifying alternative linkage methods, adjusting cluster sizes, and using a multilingual embedding model for better language support.",
    "codeLanguage": "python",
    "codeTokens": 107,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_8",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "strategy = CosineStrategy(\n    linkage_method='complete',  # Alternative clustering method\n    max_dist=0.4,              # Larger clusters\n    model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'  # Multilingual support\n)"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Accessing Crawler Result Fields in Python",
    "codeDescription": "Demonstrates how to access various fields from the crawler result including success status, status code, headers, links, markdown content, and extracted content. Also includes error handling.",
    "codeLanguage": "python",
    "codeTokens": 131,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-11_snippet_8",
    "pageTitle": "CrawlResult Object and Output Structure in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "if result.success:\n    print(result.status_code, result.response_headers)\n    print(\"Links found:\", len(result.links.get(\"internal\", [])))\n    if result.markdown:\n        print(\"Markdown snippet:\", result.markdown.raw_markdown[:200])\n    if result.extracted_content:\n        print(\"Structured JSON:\", result.extracted_content)\nelse:\n    print(\"Error:\", result.error_message)"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "JavaScript-Based Waiting Condition in Crawl4AI",
    "codeDescription": "Demonstrates how to use JavaScript-based wait conditions for more complex scenarios, such as waiting for a specific number of elements to be present on the page.",
    "codeLanguage": "python",
    "codeTokens": 95,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-11_snippet_2",
    "pageTitle": "Dynamic Page Interaction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "wait_condition = \"\"\"() => {\n    const items = document.querySelectorAll('.athing');\n    return items.length > 50;  // Wait for at least 51 items\n}\"\"\"\n\nconfig = CrawlerRunConfig(wait_for=f\"js:{wait_condition}\")"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Handling SSL Certificates in Crawl4AI (Python)",
    "codeDescription": "Illustrates how to fetch, analyze, and export SSL certificates during crawling for security compliance or data analysis purposes.",
    "codeLanguage": "python",
    "codeTokens": 303,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-11_snippet_2",
    "pageTitle": "Advanced Features in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio, os\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    tmp_dir = os.path.join(os.getcwd(), \"tmp\")\n    os.makedirs(tmp_dir, exist_ok=True)\n    \n    config = CrawlerRunConfig(\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://example.com\", config=config)\n        \n        if result.success and result.ssl_certificate:\n            cert = result.ssl_certificate\n            print(\"\\nCertificate Information:\")\n            print(f\"Issuer (CN): {cert.issuer.get('CN', '')}\")\n            print(f\"Valid until: {cert.valid_until}\")\n            print(f\"Fingerprint: {cert.fingerprint}\")\n\n            # Export in multiple formats:\n            cert.to_json(os.path.join(tmp_dir, \"certificate.json\"))\n            cert.to_pem(os.path.join(tmp_dir, \"certificate.pem\"))\n            cert.to_der(os.path.join(tmp_dir, \"certificate.der\"))\n            \n            print(\"\\nCertificate exported to JSON/PEM/DER in 'tmp' folder.\")\n        else:\n            print(\"[ERROR] No certificate or crawl failed.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Multiple File Download Implementation in Python",
    "codeDescription": "Complete example showing how to download multiple files using Crawl4AI with custom download path and JavaScript automation.",
    "codeLanguage": "python",
    "codeTokens": 285,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-11_snippet_4",
    "pageTitle": "Download Handling Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\nimport os\nfrom pathlib import Path\n\nasync def download_multiple_files(url: str, download_path: str):\n    config = BrowserConfig(accept_downloads=True, downloads_path=download_path)\n    async with AsyncWebCrawler(config=config) as crawler:\n        run_config = CrawlerRunConfig(\n            js_code=\"\"\"\n                const downloadLinks = document.querySelectorAll('a[download]');\n                for (const link of downloadLinks) {\n                    link.click();\n                    // Delay between clicks\n                    await new Promise(r => setTimeout(r, 2000));  \n                }\n            \"\"\",\n            wait_for=10  # Wait for all downloads to start\n        )\n        result = await crawler.arun(url=url, config=run_config)\n\n        if result.downloaded_files:\n            print(\"Downloaded files:\")\n            for file in result.downloaded_files:\n                print(f\"- {file}\")\n        else:\n            print(\"No files downloaded.\")\n\n# Usage\ndownload_path = os.path.join(Path.home(), \".crawl4ai\", \"downloads\")\nos.makedirs(download_path, exist_ok=True)\n\nasyncio.run(download_multiple_files(\"https://www.python.org/downloads/windows/\", download_path))"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Implementing Magic Mode in Crawl4AI for Anti-Bot Detection Bypass using Python",
    "codeDescription": "This code demonstrates the Magic Mode feature of Crawl4AI, which bypasses anti-bot detection. It uses AsyncWebCrawler with the magic parameter set to True to crawl a protected website.",
    "codeLanguage": "python",
    "codeTokens": 159,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI v0.3.72 Release Announcement and Feature Demonstration",
    "codeList": [
      {
        "language": "python",
        "code": "async def magic_mode_demo():\n    async with AsyncWebCrawler() as crawler:  # Enables anti-bot detection bypass\n        result = await crawler.arun(\n            url=\"https://www.reuters.com/markets/us/global-markets-view-usa-pix-2024-08-29/\",\n            magic=True  # Enables magic mode\n        )\n        print(result.markdown)  # Shows the full content in Markdown format\n\n# Run the demo\nawait magic_mode_demo()"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Advanced News Crawling Configuration in Python",
    "codeDescription": "This example demonstrates an advanced configuration for crawling news websites. It includes settings for handling overlays, content thresholds, and CSS selectors for targeted extraction.",
    "codeLanguage": "python",
    "codeTokens": 140,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_12",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "request = {\n    \"urls\": \"https://www.nbcnews.com/business\",\n    \"crawler_params\": {\n        \"headless\": True,\n        \"page_timeout\": 30000,\n        \"remove_overlay_elements\": True      # Remove popups\n    },\n    \"extra\": {\n        \"word_count_threshold\": 50,          # Longer content blocks\n        \"bypass_cache\": True                 # Fresh content\n    },\n    \"css_selector\": \".article-body\"\n}"
      }
    ],
    "relevance": 0.93
  },
  {
    "codeTitle": "Displaying Token Usage Information in Crawl4AI's LLMExtractionStrategy",
    "codeDescription": "Example demonstrating how to view token usage statistics after using an LLM for extraction. The show_usage() method prints usage reports including total tokens consumed and chunk call information if the provider returns such data.",
    "codeLanguage": "python",
    "codeTokens": 91,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-11_snippet_3",
    "pageTitle": "LLM-Based JSON Extraction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "llm_strategy = LLMExtractionStrategy(...)\n# ...\nllm_strategy.show_usage()\n# e.g. \"Total usage: 1241 tokens across 2 chunk calls\""
      }
    ],
    "relevance": 0.928
  },
  {
    "codeTitle": "LLM Extraction with Custom Parameters in Python",
    "codeDescription": "This example shows how to configure Crawl4AI for LLM-based extraction. It includes settings for the LLM provider, custom schema, and content processing options.",
    "codeLanguage": "python",
    "codeTokens": 153,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_14",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "request = {\n    \"urls\": \"https://openai.com/pricing\",\n    \"extraction_config\": {\n        \"type\": \"llm\",\n        \"params\": {\n            \"provider\": \"openai/gpt-4\",\n            \"schema\": pricing_schema\n        }\n    },\n    \"crawler_params\": {\n        \"verbose\": True,\n        \"page_timeout\": 60000\n    },\n    \"extra\": {\n        \"word_count_threshold\": 1,\n        \"only_text\": True\n    }\n}"
      }
    ],
    "relevance": 0.928
  },
  {
    "codeTitle": "Configuring MemoryAdaptiveDispatcher in Python for Crawl4AI",
    "codeDescription": "Implementation of a MemoryAdaptiveDispatcher that automatically manages crawling concurrency based on system memory usage. Includes memory thresholds, check intervals, and optional rate limiting and monitoring components.",
    "codeLanguage": "python",
    "codeTokens": 195,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_3",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher\n\ndispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=90.0,  # Pause if memory exceeds this\n    check_interval=1.0,             # How often to check memory\n    max_session_permit=10,          # Maximum concurrent tasks\n    rate_limiter=RateLimiter(       # Optional rate limiting\n        base_delay=(1.0, 2.0),\n        max_delay=30.0,\n        max_retries=2\n    ),\n    monitor=CrawlerMonitor(         # Optional monitoring\n        max_visible_rows=15,\n        display_mode=DisplayMode.DETAILED\n    )\n)"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Initializing CosineStrategy in Python for Content Similarity Extraction",
    "codeDescription": "This code snippet shows the initialization of CosineStrategy class used for content similarity-based extraction and clustering. It includes parameters for content filtering, clustering, and model configuration.",
    "codeLanguage": "python",
    "codeTokens": 206,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_1",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "CosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,        # Topic/keyword filter\n    word_count_threshold: int = 10,     # Minimum words per cluster\n    sim_threshold: float = 0.3,         # Similarity threshold\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,             # Maximum cluster distance\n    linkage_method: str = 'ward',       # Clustering method\n    top_k: int = 3,                    # Top clusters to return\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False              # Enable verbose logging\n)"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Technical Documentation Extraction Use Case in Python",
    "codeDescription": "Demonstrates CosineStrategy configuration for extracting technical specifications and documentation. Uses stricter similarity matching for technical content while allowing related technical sections to be grouped together.",
    "codeLanguage": "python",
    "codeTokens": 98,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_7",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "strategy = CosineStrategy(\n    semantic_filter=\"technical specifications documentation\",\n    word_count_threshold=30,\n    sim_threshold=0.6,        # Stricter matching for technical content\n    max_dist=0.3             # Allow related technical sections\n)"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Implementing Step-by-Step Button Clicking with Crawl4AI Sessions",
    "codeDescription": "This code demonstrates how to maintain state across multiple arun() calls using a session ID. It first loads the initial page and then clicks a 'Next' button to load additional content, waiting for specific elements to appear before proceeding.",
    "codeLanguage": "python",
    "codeTokens": 279,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/tutorial_dynamic_clicks.md#2025-04-11_snippet_0",
    "pageTitle": "Tutorial: Clicking Buttons to Load More Content with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CacheMode\n\njs_code = [\n    # This JS finds the \"Next\" button and clicks it\n    \"const nextButton = document.querySelector('button.next'); nextButton && nextButton.click();\"\n]\n\nwait_for_condition = \"css:.new-content-class\"\n\nasync with AsyncWebCrawler(headless=True, verbose=True) as crawler:\n    # 1. Load the initial page\n    result_initial = await crawler.arun(\n        url=\"https://example.com\",\n        cache_mode=CacheMode.BYPASS,\n        session_id=\"my_session\"\n    )\n\n    # 2. Click the 'Next' button and wait for new content\n    result_next = await crawler.arun(\n        url=\"https://example.com\",\n        session_id=\"my_session\",\n        js_code=js_code,\n        wait_for=wait_for_condition,\n        js_only=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n# `result_next` now contains the updated HTML after clicking 'Next'"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Implementing Single-call Multi-click Approach with Crawl4AI",
    "codeDescription": "This code shows how to execute multiple button clicks in a single arun() call by using a more complex JavaScript snippet. The async JavaScript function clicks all module items sequentially, waiting between clicks, before returning control to Crawl4AI for content extraction.",
    "codeLanguage": "python",
    "codeTokens": 262,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/tutorial_dynamic_clicks.md#2025-04-11_snippet_1",
    "pageTitle": "Tutorial: Clicking Buttons to Load More Content with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CacheMode\n\njs_code = [\n    # Example JS that clicks multiple modules:\n    \"\"\"\n    (async () => {\n      const modules = document.querySelectorAll('.module-item');\n      for (let i = 0; i < modules.length; i++) {\n        modules[i].scrollIntoView();\n        modules[i].click();\n        // Wait for each module's content to load, adjust 100ms as needed\n        await new Promise(r => setTimeout(r, 100));\n      }\n    })();\n    \"\"\"\n]\n\nasync with AsyncWebCrawler(headless=True, verbose=True) as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        js_code=js_code,\n        wait_for=\"css:.final-loaded-content-class\",\n        cache_mode=CacheMode.BYPASS\n    )\n\n# `result` now contains all content after all modules have been clicked in one go."
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Implementing Proxy Rotation in AsyncWebCrawler",
    "codeDescription": "Demonstrates how to implement proxy rotation using RoundRobinProxyStrategy with AsyncWebCrawler. The code shows proxy configuration setup and verification of IP addresses through multiple crawl requests.",
    "codeLanguage": "python",
    "codeTokens": 502,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "import re\nfrom crawl4ai import (\n    AsyncWebCrawler,\n    BrowserConfig,\n    CrawlerRunConfig,\n    CacheMode,\n    RoundRobinProxyStrategy,\n)\nimport asyncio\nfrom crawl4ai.proxy_strategy import ProxyConfig\nasync def main():\n    # Load proxies and create rotation strategy\n    proxies = ProxyConfig.from_env()\n    #eg: export PROXIES=\"ip1:port1:username1:password1,ip2:port2:username2:password2\"\n    if not proxies:\n        print(\"No proxies found in environment. Set PROXIES env variable!\")\n        return\n        \n    proxy_strategy = RoundRobinProxyStrategy(proxies)\n    \n    # Create configs\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS,\n        proxy_rotation_strategy=proxy_strategy\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        urls = [\"https://httpbin.org/ip\"] * (len(proxies) * 2)  # Test each proxy twice\n\n        print(\"\\n Initializing crawler with proxy rotation...\")\n        async with AsyncWebCrawler(config=browser_config) as crawler:\n            print(\"\\n Starting batch crawl with proxy rotation...\")\n            results = await crawler.arun_many(\n                urls=urls,\n                config=run_config\n            )\n            for result in results:\n                if result.success:\n                    ip_match = re.search(r'(?:[0-9]{1,3}\\.){3}[0-9]{1,3}', result.html)\n                    current_proxy = run_config.proxy_config if run_config.proxy_config else None\n                    \n                    if current_proxy and ip_match:\n                        print(f\"URL {result.url}\")\n                        print(f\"Proxy {current_proxy.server} -> Response IP: {ip_match.group(0)}\")\n                        verified = ip_match.group(0) == current_proxy.ip\n                        if verified:\n                            print(f\" Proxy working! IP matches: {current_proxy.ip}\")\n                        else:\n                            print(\" Proxy failed or IP mismatch!\")\n                    print(\"---\")\n\nasyncio.run(main())"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Anti-Detection Configuration for Crawl4AI in Python",
    "codeDescription": "This code snippet showcases advanced anti-detection settings for Crawl4AI. It includes user simulation, magic mode, navigator override, and custom headers to mimic human browsing behavior.",
    "codeLanguage": "python",
    "codeTokens": 127,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_13",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "request = {\n    \"urls\": \"https://example.com\",\n    \"crawler_params\": {\n        \"simulate_user\": True,\n        \"magic\": True,\n        \"override_navigator\": True,\n        \"user_agent\": \"Mozilla/5.0 ...\",\n        \"headers\": {\n            \"Accept-Language\": \"en-US,en;q=0.9\"\n        }\n    }\n}"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Interactive Profile Management with Crawl Integration",
    "codeDescription": "Example showing how to use the interactive profile manager with crawling functionality",
    "codeLanguage": "python",
    "codeTokens": 188,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-11_snippet_5",
    "pageTitle": "Setting up Browser Profiles with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import BrowserProfiler, AsyncWebCrawler, BrowserConfig\n\n# Define a function to use a profile for crawling\nasync def crawl_with_profile(profile_path, url):\n    browser_config = BrowserConfig(\n        headless=True,\n        use_managed_browser=True,\n        user_data_dir=profile_path\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url)\n        return result\n\nasync def main():\n    # Create a profiler instance\n    profiler = BrowserProfiler()\n    \n    # Launch the interactive profile manager\n    # Passing the crawl function as a callback adds a \"crawl with profile\" option\n    await profiler.interactive_manager(crawl_callback=crawl_with_profile)\n    \nasyncio.run(main())"
      }
    ],
    "relevance": 0.925
  },
  {
    "codeTitle": "Creating a Custom Content Filter in Crawl4AI",
    "codeDescription": "Example showing how to create a custom content filtering strategy by subclassing the RelevantContentFilter base class and implementing the filter_content method. Provides the framework for developing site-specific or specialized filtering logic.",
    "codeLanguage": "python",
    "codeTokens": 110,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-11_snippet_5",
    "pageTitle": "Fit Markdown with Pruning & BM25 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.content_filter_strategy import RelevantContentFilter\n\nclass MyCustomFilter(RelevantContentFilter):\n    def filter_content(self, html, min_word_threshold=None):\n        # parse HTML, implement custom logic\n        return [block for block in ... if ... some condition...]"
      }
    ],
    "relevance": 0.923
  },
  {
    "codeTitle": "Structured Data Extraction Example",
    "codeDescription": "Complete command example showing structured data extraction using CSS configuration and schema with verbose JSON output.",
    "codeLanguage": "bash",
    "codeTokens": 58,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_16",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crwl https://example.com \\\n    -e extract_css.yml \\\n    -s css_schema.json \\\n    -o json \\\n    -v"
      }
    ],
    "relevance": 0.922
  },
  {
    "codeTitle": "Setting Up SemaphoreDispatcher in Python for Crawl4AI",
    "codeDescription": "Code for creating a SemaphoreDispatcher that provides simple concurrency control with a fixed limit. Includes configuration for maximum concurrent tasks, rate limiting, and monitoring options.",
    "codeLanguage": "python",
    "codeTokens": 148,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_4",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_dispatcher import SemaphoreDispatcher\n\ndispatcher = SemaphoreDispatcher(\n    max_session_permit=20,         # Maximum concurrent tasks\n    rate_limiter=RateLimiter(      # Optional rate limiting\n        base_delay=(0.5, 1.0),\n        max_delay=10.0\n    ),\n    monitor=CrawlerMonitor(        # Optional monitoring\n        max_visible_rows=15,\n        display_mode=DisplayMode.DETAILED\n    )\n)"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Customizing CrawlerRunConfig for web crawling with Crawl4AI",
    "codeDescription": "This snippet illustrates how to customize the CrawlerRunConfig object to set various options for the web crawl. It shows how to set minimum word count, exclude external links, remove overlay elements, and process iframes.",
    "codeLanguage": "python",
    "codeTokens": 141,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-11_snippet_2",
    "pageTitle": "Simple Web Crawling with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    word_count_threshold=10,        # Minimum words per content block\n    exclude_external_links=True,    # Remove external links\n    remove_overlay_elements=True,   # Remove popups/modals\n    process_iframes=True           # Process iframe content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com\",\n    config=run_config\n)"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Combining CSS Selection Methods in Crawl4AI with Python",
    "codeDescription": "This example demonstrates how to combine css_selector and target_elements in Crawl4AI to achieve fine-grained control over content extraction. It focuses on specific elements while preserving page context for links and media.",
    "codeLanguage": "python",
    "codeTokens": 274,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_10",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    # Target specific content but preserve page context\n    config = CrawlerRunConfig(\n        # Focus markdown on main content and sidebar\n        target_elements=[\"#main-content\", \".sidebar\"],\n        \n        # Global filters applied to entire page\n        excluded_tags=[\"nav\", \"footer\", \"header\"],\n        exclude_external_links=True,\n        \n        # Use basic content thresholds\n        word_count_threshold=15,\n        \n        cache_mode=CacheMode.BYPASS\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/article\",\n            config=config\n        )\n        \n        print(f\"Content focuses on specific elements, but all links still analyzed\")\n        print(f\"Internal links: {len(result.links.get('internal', []))}\")\n        print(f\"External links: {len(result.links.get('external', []))}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Processing the CrawlResult object in Crawl4AI",
    "codeDescription": "This code snippet shows how to access various properties of the CrawlResult object returned by the crawler.arun() method. It demonstrates accessing different content formats, checking success status, and extracting media and links from the crawl result.",
    "codeLanguage": "python",
    "codeTokens": 214,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-11_snippet_1",
    "pageTitle": "Simple Web Crawling with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "result = await crawler.arun(\n    url=\"https://example.com\",\n    config=CrawlerRunConfig(fit_markdown=True)\n)\n\n# Different content formats\nprint(result.html)         # Raw HTML\nprint(result.cleaned_html) # Cleaned HTML\nprint(result.markdown.raw_markdown) # Raw markdown from cleaned html\nprint(result.markdown.fit_markdown) # Most relevant content in markdown\n\n# Check success status\nprint(result.success)      # True if crawl succeeded\nprint(result.status_code)  # HTTP status code (e.g., 200, 404)\n\n# Access extracted media and links\nprint(result.media)        # Dictionary of found media (images, videos, audio)\nprint(result.links)        # Dictionary of internal and external links"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Triggering Downloads with JavaScript in Python",
    "codeDescription": "Illustrates how to trigger file downloads using JavaScript code injection and wait timing in CrawlerRunConfig.",
    "codeLanguage": "python",
    "codeTokens": 125,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-11_snippet_2",
    "pageTitle": "Download Handling Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_configs import CrawlerRunConfig\n\nconfig = CrawlerRunConfig(\n    js_code=\"\"\"\n        const downloadLink = document.querySelector('a[href$=\".exe\"]');\n        if (downloadLink) {\n            downloadLink.click();\n        }\n    \"\"\",\n    wait_for=5  # Wait 5 seconds for the download to start\n)\n\nresult = await crawler.arun(url=\"https://www.python.org/downloads/\", config=config)"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Implementing Non-Streaming Mode Crawling in Python",
    "codeDescription": "This snippet demonstrates how to use Crawl4AI in non-streaming mode, where all results are collected before being returned. This is useful when you need the complete dataset before processing or for batch operations on all results together.",
    "codeLanguage": "python",
    "codeTokens": 143,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_4",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),\n    stream=False  # Default behavior\n)\n\nasync with AsyncWebCrawler() as crawler:\n    # Wait for ALL results to be collected before returning\n    results = await crawler.arun(\"https://example.com\", config=config)\n    \n    for result in results:\n        process_result(result)"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Configuring Lazy Image Loading with Crawl4AI in Python",
    "codeDescription": "Demonstrates how to set up AsyncWebCrawler with configurations for handling lazy-loaded images. Includes settings for image waiting, full page scanning, and scroll delays to ensure complete image capture.",
    "codeLanguage": "python",
    "codeTokens": 342,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/lazy-loading.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI Image Handling Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig\nfrom crawl4ai.async_configs import CacheMode\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Force the crawler to wait until images are fully loaded\n        wait_for_images=True,\n\n        # Option 1: If you want to automatically scroll the page to load images\n        scan_full_page=True,  # Tells the crawler to try scrolling the entire page\n        scroll_delay=0.5,     # Delay (seconds) between scroll steps\n\n        # Option 2: If the site uses a 'Load More' or JS triggers for images,\n        # you can also specify js_code or wait_for logic here.\n\n        cache_mode=CacheMode.BYPASS,\n        verbose=True\n    )\n\n    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:\n        result = await crawler.arun(\"https://www.example.com/gallery\", config=config)\n        \n        if result.success:\n            images = result.media.get(\"images\", [])\n            print(\"Images found:\", len(images))\n            for i, img in enumerate(images[:5]):\n                print(f\"[Image {i}] URL: {img['src']}, Score: {img.get('score','N/A')}\")\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Generating XPath Schema with Ollama Provider in Python",
    "codeDescription": "Example showing how to generate an XPath extraction schema using the Ollama LLM provider, which is an open-source alternative that doesn't require an API token. The generated schema can be used for repeated extractions without additional LLM calls.",
    "codeLanguage": "python",
    "codeTokens": 135,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-11_snippet_6",
    "pageTitle": "JSON Schema-Based Web Extraction Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "xpath_schema = JsonXPathExtractionStrategy.generate_schema(\n    html,\n    schema_type=\"xpath\",\n    llm_config = LLMConfig(provider=\"ollama/llama3.3\", api_token=None)  # Not needed for Ollama\n)\n\n# Use the generated schema for fast, repeated extractions\nstrategy = JsonCssExtractionStrategy(css_schema)"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "LLMContentFilter Implementation for Intelligent Content Extraction",
    "codeDescription": "Demonstrates how to use LLMContentFilter with OpenAI's GPT-4o to intelligently extract and format content. This approach uses language models to understand context and produce high-quality markdown based on custom instructions.",
    "codeLanguage": "python",
    "codeTokens": 295,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_4",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig\nfrom crawl4ai.content_filter_strategy import LLMContentFilter\n\nasync def main():\n    # Initialize LLM filter with specific instruction\n    filter = LLMContentFilter(\n        llm_config = LLMConfig(provider=\"openai/gpt-4o\",api_token=\"your-api-token\"), #or use environment variable\n        instruction=\"\"\"\n        Focus on extracting the core educational content.\n        Include:\n        - Key concepts and explanations\n        - Important code examples\n        - Essential technical details\n        Exclude:\n        - Navigation elements\n        - Sidebars\n        - Footer content\n        Format the output as clean markdown with proper code blocks and headers.\n        \"\"\",\n        chunk_token_threshold=4096,  # Adjust based on your needs\n        verbose=True\n    )\n\n    config = CrawlerRunConfig(\n        content_filter=filter\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        print(result.markdown.fit_markdown)  # Filtered markdown content"
      }
    ],
    "relevance": 0.92
  },
  {
    "codeTitle": "Implementing Browser Session Reuse",
    "codeDescription": "Demonstrates how to create and reuse browser sessions across multiple crawls to improve efficiency and reduce resource usage.",
    "codeLanguage": "python",
    "codeTokens": 107,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI Release 0.4.1 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "session_id = await crawler.create_session()\n\n# Use the same session for multiple crawls\nawait crawler.crawl(\n    url=\"https://example.com/page1\",\n    session_id=session_id  # Reuse the session\n)\nawait crawler.crawl(\n    url=\"https://example.com/page2\",\n    session_id=session_id\n)"
      }
    ],
    "relevance": 0.918
  },
  {
    "codeTitle": "Configuring Extraction Strategy with Dynamic Content in Crawl4AI",
    "codeDescription": "Demonstrates how to set up a JsonCssExtractionStrategy with CrawlerRunConfig for extracting data from dynamically loaded content. The example shows configuration for extracting commit information using CSS selectors and handling JavaScript-based pagination.",
    "codeLanguage": "python",
    "codeTokens": 173,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-11_snippet_7",
    "pageTitle": "Dynamic Page Interaction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nschema = {\n    \"name\": \"Commits\",\n    \"baseSelector\": \"li.Box-sc-g0xbh4-0\",\n    \"fields\": [\n        {\"name\": \"title\", \"selector\": \"h4.markdown-title\", \"type\": \"text\"}\n    ]\n}\nconfig = CrawlerRunConfig(\n    session_id=\"ts_commits_session\",\n    js_code=js_next_page,\n    wait_for=wait_for_more,\n    extraction_strategy=JsonCssExtractionStrategy(schema)\n)"
      }
    ],
    "relevance": 0.918
  },
  {
    "codeTitle": "Implementing Content Relevance Filter in Crawl4AI",
    "codeDescription": "Configures a content relevance filter that analyzes page content using BM25-based semantic similarity matching.",
    "codeLanguage": "python",
    "codeTokens": 140,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_10",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter\n\n# Create a content relevance filter\nrelevance_filter = ContentRelevanceFilter(\n    query=\"Web crawling and data extraction with Python\",\n    threshold=0.7  # Minimum similarity score (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([relevance_filter])\n    )\n)"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Content Filtering Example Usage in Crawl4AI",
    "codeDescription": "This snippet shows a practical example of using various content filtering and exclusion options in CrawlerRunConfig, demonstrating how to combine multiple parameters for precise control over crawled content.",
    "codeLanguage": "python",
    "codeTokens": 206,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_3",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    config = CrawlerRunConfig(\n        css_selector=\"main.content\", \n        word_count_threshold=10,\n        excluded_tags=[\"nav\", \"footer\"],\n        exclude_external_links=True,\n        exclude_social_media_links=True,\n        exclude_domains=[\"ads.com\", \"spammytrackers.net\"],\n        exclude_external_images=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://news.ycombinator.com\", config=config)\n        print(\"Cleaned HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Pattern-Based Extraction using JsonCssExtractionStrategy in Crawl4AI",
    "codeDescription": "This snippet demonstrates how to use JsonCssExtractionStrategy for structured data extraction based on CSS selectors. It extracts news items from Hacker News, including title and link.",
    "codeLanguage": "python",
    "codeTokens": 349,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_5",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nimport json\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\nfrom crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n\nasync def main():\n    # Minimal schema for repeated items\n    schema = {\n        \"name\": \"News Items\",\n        \"baseSelector\": \"tr.athing\",\n        \"fields\": [\n            {\"name\": \"title\", \"selector\": \"span.titleline a\", \"type\": \"text\"},\n            {\n                \"name\": \"link\", \n                \"selector\": \"span.titleline a\", \n                \"type\": \"attribute\", \n                \"attribute\": \"href\"\n            }\n        ]\n    }\n\n    config = CrawlerRunConfig(\n        # Content filtering\n        excluded_tags=[\"form\", \"header\"],\n        exclude_domains=[\"adsite.com\"],\n        \n        # CSS selection or entire page\n        css_selector=\"table.itemlist\",\n\n        # No caching for demonstration\n        cache_mode=CacheMode.BYPASS,\n\n        # Extraction strategy\n        extraction_strategy=JsonCssExtractionStrategy(schema)\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        data = json.loads(result.extracted_content)\n        print(\"Sample extracted item:\", data[:1])  # Show first item\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Basic DefaultMarkdownGenerator Configuration in Crawl4AI",
    "codeDescription": "A minimal code snippet demonstrating how to use DefaultMarkdownGenerator without additional filtering to convert web content to markdown. The crawler fetches content from a URL and outputs the raw markdown.",
    "codeLanguage": "python",
    "codeTokens": 186,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_0",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    config = CrawlerRunConfig(\n        markdown_generator=DefaultMarkdownGenerator()\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        \n        if result.success:\n            print(\"Raw Markdown Output:\\n\")\n            print(result.markdown)  # The unfiltered markdown from the page\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Interactive Q&A Example",
    "codeDescription": "Complete example showing a workflow for interactive Q&A, first viewing content in markdown format and then asking specific questions.",
    "codeLanguage": "bash",
    "codeTokens": 85,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_18",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "# First crawl and view\ncrwl https://example.com -o markdown\n\n# Then ask questions\ncrwl https://example.com -q \"What are the main points?\"\ncrwl https://example.com -q \"Summarize the conclusions\""
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Defining Blog Post Extraction Schema in Python",
    "codeDescription": "This schema demonstrates how to extract blog post information including the post URL (as a base field attribute), title, date, summary, and author. It uses the JsonCssExtractionStrategy for parsing.",
    "codeLanguage": "python",
    "codeTokens": 214,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#2025-04-11_snippet_4",
    "pageTitle": "JSON Schema-Based Web Extraction Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "schema = {\n  \"name\": \"Blog Posts\",\n  \"baseSelector\": \"a.blog-post-card\",\n  \"baseFields\": [\n    {\"name\": \"post_url\", \"type\": \"attribute\", \"attribute\": \"href\"}\n  ],\n  \"fields\": [\n    {\"name\": \"title\", \"selector\": \"h2.post-title\", \"type\": \"text\", \"default\": \"No Title\"},\n    {\"name\": \"date\", \"selector\": \"time.post-date\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"summary\", \"selector\": \"p.post-summary\", \"type\": \"text\", \"default\": \"\"},\n    {\"name\": \"author\", \"selector\": \"span.post-author\", \"type\": \"text\", \"default\": \"\"}\n  ]\n}"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Using Automatic Schema Generation with LLMs in Crawl4AI",
    "codeDescription": "Shows how to generate extraction schemas instantly using LLMs instead of manually writing CSS/XPath selectors. This feature can analyze HTML content and create appropriate extraction rules based on natural language queries.",
    "codeLanguage": "python",
    "codeTokens": 88,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI 0.4.3 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "schema = JsonCssExtractionStrategy.generate_schema(\n    html_content,\n    schema_type=\"CSS\",\n    query=\"Extract product name, price, and description\"\n)"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Configuration Options for CosineStrategy in Python",
    "codeDescription": "Shows all available configuration parameters for the CosineStrategy class, including content filtering options, clustering parameters, and model configuration. This provides a reference for the full parameter set available when creating a CosineStrategy instance.",
    "codeLanguage": "python",
    "codeTokens": 226,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_1",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "CosineStrategy(\n    # Content Filtering\n    semantic_filter: str = None,       # Keywords/topic for content filtering\n    word_count_threshold: int = 10,    # Minimum words per cluster\n    sim_threshold: float = 0.3,        # Similarity threshold (0.0 to 1.0)\n    \n    # Clustering Parameters\n    max_dist: float = 0.2,            # Maximum distance for clustering\n    linkage_method: str = 'ward',      # Clustering linkage method\n    top_k: int = 3,                   # Number of top categories to extract\n    \n    # Model Configuration\n    model_name: str = 'sentence-transformers/all-MiniLM-L6-v2',  # Embedding model\n    \n    verbose: bool = False             # Enable logging\n)"
      }
    ],
    "relevance": 0.915
  },
  {
    "codeTitle": "Handling Errors in Crawl4AI Extraction Process in Python",
    "codeDescription": "This example demonstrates error handling in the Crawl4AI extraction process. It shows how to catch exceptions and check for successful extraction results.",
    "codeLanguage": "python",
    "codeTokens": 102,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_10",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "try:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        extraction_strategy=strategy\n    )\n    if result.success:\n        content = json.loads(result.extracted_content)\nexcept Exception as e:\n    print(f\"Extraction failed: {e}\")"
      }
    ],
    "relevance": 0.913
  },
  {
    "codeTitle": "Implementing Rotating Proxies in Crawl4AI",
    "codeDescription": "This snippet illustrates how to use rotating proxies with Crawl4AI. It demonstrates creating a new run configuration for each URL with a different proxy, obtained from a custom proxy rotation function.",
    "codeLanguage": "python",
    "codeTokens": 221,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/proxy-security.md#2025-04-11_snippet_2",
    "pageTitle": "Proxy Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def get_next_proxy():\n    # Your proxy rotation logic here\n    return {\"server\": \"http://next.proxy.com:8080\"}\n\nasync def main():\n    browser_config = BrowserConfig()\n    run_config = CrawlerRunConfig()\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        # For each URL, create a new run config with different proxy\n        for url in urls:\n            proxy = await get_next_proxy()\n            # Clone the config and update proxy - this creates a new browser context\n            current_config = run_config.clone(proxy_config=proxy)\n            result = await crawler.arun(url=url, config=current_config)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.912
  },
  {
    "codeTitle": "Configuring Basic Proxy in Crawl4AI with BrowserConfig",
    "codeDescription": "This snippet shows how to set up a basic proxy using BrowserConfig in Crawl4AI. It demonstrates both HTTP and SOCKS5 proxy configurations.",
    "codeLanguage": "python",
    "codeTokens": 154,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/proxy-security.md#2025-04-11_snippet_0",
    "pageTitle": "Proxy Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_configs import BrowserConfig\n\n# Using proxy URL\nbrowser_config = BrowserConfig(proxy=\"http://proxy.example.com:8080\")\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")\n\n# Using SOCKS proxy\nbrowser_config = BrowserConfig(proxy=\"socks5://proxy.example.com:1080\")\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Initializing AsyncWebCrawler with CrawlerRunConfig in Python",
    "codeDescription": "Demonstrates how to set up a basic CrawlerRunConfig and use it with AsyncWebCrawler's arun() method. It includes settings for verbose logging, cache mode, and robots.txt checking.",
    "codeLanguage": "python",
    "codeTokens": 207,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI Parameter Guide for arun() Method",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    run_config = CrawlerRunConfig(\n        verbose=True,            # Detailed logging\n        cache_mode=CacheMode.ENABLED,  # Use normal read/write cache\n        check_robots_txt=True,   # Respect robots.txt rules\n        # ... other parameters\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\",\n            config=run_config\n        )\n        \n        # Check if blocked by robots.txt\n        if not result.success and result.status_code == 403:\n            print(f\"Error: {result.error_message}\")"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Error handling in Crawl4AI web crawling",
    "codeDescription": "This code demonstrates how to handle errors when performing a web crawl with Crawl4AI. It checks the success status of the crawl and prints error information if the crawl fails.",
    "codeLanguage": "python",
    "codeTokens": 105,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-11_snippet_3",
    "pageTitle": "Simple Web Crawling with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig()\nresult = await crawler.arun(url=\"https://example.com\", config=run_config)\n\nif not result.success:\n    print(f\"Crawl failed: {result.error_message}\")\n    print(f\"Status code: {result.status_code}\")"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "CSS-Based Selection using css_selector in Crawl4AI",
    "codeDescription": "This snippet demonstrates how to use the css_selector parameter in CrawlerRunConfig to limit crawl results to a specific region of the page. It targets the first 30 items from Hacker News.",
    "codeLanguage": "python",
    "codeTokens": 174,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_0",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # e.g., first 30 items from Hacker News\n        css_selector=\".athing:nth-child(-n+30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com/newest\", \n            config=config\n        )\n        print(\"Partial HTML length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Exact Content Preservation with LLMContentFilter",
    "codeDescription": "Example configuration for LLMContentFilter that focuses on preserving original wording and substance while removing only irrelevant elements. This approach maintains the exact language and structure of important content.",
    "codeLanguage": "python",
    "codeTokens": 133,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_5",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "filter = LLMContentFilter(\n    instruction=\"\"\"\n    Extract the main educational content while preserving its original wording and substance completely.\n    1. Maintain the exact language and terminology\n    2. Keep all technical explanations and examples intact\n    3. Preserve the original flow and structure\n    4. Remove only clearly irrelevant elements like navigation menus and ads\n    \"\"\",\n    chunk_token_threshold=4096\n)"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Implementing AsyncHTTPCrawlerStrategy in Python",
    "codeDescription": "This snippet shows how to use the AsyncHTTPCrawlerStrategy, a lightweight and fast HTTP-only crawler. It demonstrates configuring the crawler with custom headers and other HTTP-specific options.",
    "codeLanguage": "python",
    "codeTokens": 214,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, HTTPCrawlerConfig\nfrom crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy\nimport asyncio\n\n# Use the HTTP crawler strategy\nhttp_crawler_config = HTTPCrawlerConfig(\n        method=\"GET\",\n        headers={\"User-Agent\": \"MyCustomBot/1.0\"},\n        follow_redirects=True,\n        verify_ssl=True\n)\n\nasync def main():\n    async with AsyncWebCrawler(crawler_strategy=AsyncHTTPCrawlerStrategy(browser_config =http_crawler_config)) as crawler:\n        result = await crawler.arun(\"https://example.com\")\n        print(f\"Status code: {result.status_code}\")\n        print(f\"Content length: {len(result.html)}\")\n\nasyncio.run(main())"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Using Your Own Browser with Custom User Profile in Python",
    "codeDescription": "This example shows how to use Crawl4AI with a custom browser profile, allowing for persistent user data and settings. It demonstrates crawling a challenging website with a persistent browser context.",
    "codeLanguage": "python",
    "codeTokens": 272,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_11",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "python",
        "code": "import os, sys\nfrom pathlib import Path\nimport asyncio, time\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n\nasync def test_news_crawl():\n    # Create a persistent user data directory\n    user_data_dir = os.path.join(Path.home(), \".crawl4ai\", \"browser_profile\")\n    os.makedirs(user_data_dir, exist_ok=True)\n\n    browser_config = BrowserConfig(\n        verbose=True,\n        headless=True,\n        user_data_dir=user_data_dir,\n        use_persistent_context=True,\n    )\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.BYPASS\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        url = \"ADDRESS_OF_A_CHALLENGING_WEBSITE\"\n        \n        result = await crawler.arun(\n            url,\n            config=run_config,\n            magic=True,\n        )\n        \n        print(f\"Successfully crawled {url}\")\n        print(f\"Content length: {len(result.markdown)}\")"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Enabling Downloads with BrowserConfig in Python",
    "codeDescription": "Demonstrates how to enable download functionality in Crawl4AI by configuring the BrowserConfig object with accept_downloads parameter.",
    "codeLanguage": "python",
    "codeTokens": 99,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-11_snippet_0",
    "pageTitle": "Download Handling Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_configs import BrowserConfig, AsyncWebCrawler\n\nasync def main():\n    config = BrowserConfig(accept_downloads=True)  # Enable downloads globally\n    async with AsyncWebCrawler(config=config) as crawler:\n        # ... your crawling logic ...\n\nasyncio.run(main())"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Form Interaction with JavaScript in Crawl4AI",
    "codeDescription": "Shows how to interact with forms by filling in fields and submitting them using JavaScript code, then waiting for the results to appear using CSS selectors.",
    "codeLanguage": "python",
    "codeTokens": 109,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-11_snippet_4",
    "pageTitle": "Dynamic Page Interaction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "js_form_interaction = \"\"\"\ndocument.querySelector('#your-search').value = 'TypeScript commits';\ndocument.querySelector('form').submit();\n\"\"\"\n\nconfig = CrawlerRunConfig(\n    js_code=js_form_interaction,\n    wait_for=\"css:.commit\"\n)\nresult = await crawler.arun(url=\"https://github.com/search\", config=config)"
      }
    ],
    "relevance": 0.91
  },
  {
    "codeTitle": "Initializing CrawlerRunConfig Class in Python",
    "codeDescription": "The CrawlerRunConfig class constructor with its parameters that control crawler behavior during each run. It includes settings for content extraction, caching, JavaScript execution, rate limiting, and resource management.",
    "codeLanguage": "python",
    "codeTokens": 179,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-11_snippet_4",
    "pageTitle": "Browser, Crawler & LLM Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "class CrawlerRunConfig:\n    def __init__(\n        word_count_threshold=200,\n        extraction_strategy=None,\n        markdown_generator=None,\n        cache_mode=None,\n        js_code=None,\n        wait_for=None,\n        screenshot=False,\n        pdf=False,\n        enable_rate_limiting=False,\n        rate_limit_config=None,\n        memory_threshold_percent=70.0,\n        check_interval=1.0,\n        max_session_permit=20,\n        display_mode=None,\n        verbose=True,\n        stream=False,  # Enable streaming for arun_many()\n        # ... other advanced parameters omitted\n    ):\n        ..."
      }
    ],
    "relevance": 0.908
  },
  {
    "codeTitle": "Content Filtering and Exclusions Configuration in Crawl4AI",
    "codeDescription": "This snippet demonstrates how to configure various content filtering and exclusion options in CrawlerRunConfig, including word count thresholds, tag exclusions, link filtering, and media filtering.",
    "codeLanguage": "python",
    "codeTokens": 161,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_2",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    # Content thresholds\n    word_count_threshold=10,        # Minimum words per block\n\n    # Tag exclusions\n    excluded_tags=['form', 'header', 'footer', 'nav'],\n\n    # Link filtering\n    exclude_external_links=True,    \n    exclude_social_media_links=True,\n    # Block entire domains\n    exclude_domains=[\"adtrackers.com\", \"spammynews.org\"],    \n    exclude_social_media_domains=[\"facebook.com\", \"twitter.com\"],\n\n    # Media filtering\n    exclude_external_images=True\n)"
      }
    ],
    "relevance": 0.908
  },
  {
    "codeTitle": "Accessing Basic Crawl Information in Python",
    "codeDescription": "Examples of accessing the basic crawl information fields from a CrawlResult object, including URL, success status, status code, error messages, session ID, response headers, and SSL certificate details.",
    "codeLanguage": "python",
    "codeTokens": 64,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-11_snippet_1",
    "pageTitle": "CrawlResult Class Reference",
    "codeList": [
      {
        "language": "python",
        "code": "print(result.url)  # e.g., \"https://example.com/\""
      },
      {
        "language": "python",
        "code": "if not result.success:\n    print(f\"Crawl failed: {result.error_message}\")"
      },
      {
        "language": "python",
        "code": "if result.status_code == 404:\n    print(\"Page not found!\")"
      },
      {
        "language": "python",
        "code": "if not result.success:\n    print(\"Error:\", result.error_message)"
      },
      {
        "language": "python",
        "code": "# If you used session_id=\"login_session\" in CrawlerRunConfig, see it here:\nprint(\"Session:\", result.session_id)"
      },
      {
        "language": "python",
        "code": "if result.response_headers:\n    print(\"Server:\", result.response_headers.get(\"Server\", \"Unknown\"))"
      },
      {
        "language": "python",
        "code": "if result.ssl_certificate:\n    print(\"Issuer:\", result.ssl_certificate.issuer)"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Setting Content Processing Parameters in CrawlerRunConfig for Python",
    "codeDescription": "Demonstrates how to configure text processing, content selection, link handling, and media filtering options in CrawlerRunConfig. These settings control how the crawler processes and filters the content of crawled pages.",
    "codeLanguage": "python",
    "codeTokens": 111,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI Parameter Guide for arun() Method",
    "codeList": [
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    word_count_threshold=10,   # Ignore text blocks <10 words\n    only_text=False,           # If True, tries to remove non-text elements\n    keep_data_attributes=False # Keep or discard data-* attributes\n)"
      },
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    css_selector=\".main-content\",  # Focus on .main-content region only\n    excluded_tags=[\"form\", \"nav\"], # Remove entire tag blocks\n    remove_forms=True,             # Specifically strip <form> elements\n    remove_overlay_elements=True,  # Attempt to remove modals/popups\n)"
      },
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    exclude_external_links=True,         # Remove external links from final content\n    exclude_social_media_links=True,     # Remove links to known social sites\n    exclude_domains=[\"ads.example.com\"], # Exclude links to these domains\n    exclude_social_media_domains=[\"facebook.com\",\"twitter.com\"], # Extend the default list\n)"
      },
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    exclude_external_images=True  # Strip images from other domains\n)"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Initializing LLMConfig with Provider and API Token",
    "codeDescription": "Example showing how to create an LLMConfig with provider and API token. The configuration specifies which LLM provider to use (in this case OpenAI's GPT-4o-mini) and retrieves the API key from environment variables.",
    "codeLanguage": "python",
    "codeTokens": 94,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-11_snippet_6",
    "pageTitle": "Browser, Crawler & LLM Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "llm_config = LLMConfig(provider=\"openai/gpt-4o-mini\", api_token=os.getenv(\"OPENAI_API_KEY\"))"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Managing Session State in Crawl4AI",
    "codeDescription": "Shows how to implement authenticated crawling by importing a saved storage state from a JSON file. This allows for maintaining session persistence across crawler runs.",
    "codeLanguage": "python",
    "codeTokens": 72,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.2.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI 0.4.2 Update Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "result = await crawler.arun(\n    url=\"https://example.com/protected\",\n    storage_state=\"my_storage_state.json\"\n)"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "BM25 Filter Configuration Pattern",
    "codeDescription": "A concise pattern for configuring a BM25ContentFilter with a specific user query and threshold, then integrating it into the crawler configuration. Shows the basic setup for query-based content filtering.",
    "codeLanguage": "python",
    "codeTokens": 104,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-11_snippet_3",
    "pageTitle": "Fit Markdown with Pruning & BM25 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "bm25_filter = BM25ContentFilter(\n    user_query=\"health benefits fruit\",\n    bm25_threshold=1.2\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Cloning BrowserConfig with Helper Methods in Python",
    "codeDescription": "Example demonstrating how to create a base browser configuration and clone it with modified parameters. This approach allows reusing common settings while changing only specific parameters for different use cases.",
    "codeLanguage": "python",
    "codeTokens": 109,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-11_snippet_2",
    "pageTitle": "Browser, Crawler & LLM Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "# Create a base browser config\nbase_browser = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    text_mode=True\n)\n\n# Create a visible browser config for debugging\ndebug_browser = base_browser.clone(\n    headless=False,\n    verbose=True\n)"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Using Streaming Support for Real-Time URL Processing in Crawl4AI",
    "codeDescription": "Shows how to enable streaming support to process crawled URLs in real-time instead of waiting for all results. This feature helps reduce memory footprint by processing each result as it becomes available.",
    "codeLanguage": "python",
    "codeTokens": 104,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI 0.4.3 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(stream=True)\n\nasync with AsyncWebCrawler() as crawler:\n    async for result in await crawler.arun_many(urls, config=config):\n        print(f\"Got result for {result.url}\")\n        # Process each result immediately"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Setting Custom Headers in Crawl4AI (Python)",
    "codeDescription": "Demonstrates two methods for setting custom HTTP headers in Crawl4AI: at the crawler strategy level or directly in the arun() method.",
    "codeLanguage": "python",
    "codeTokens": 274,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-11_snippet_3",
    "pageTitle": "Advanced Features in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    # Option 1: Set headers at the crawler strategy level\n    crawler1 = AsyncWebCrawler(\n        # The underlying strategy can accept headers in its constructor\n        crawler_strategy=None  # We'll override below for clarity\n    )\n    crawler1.crawler_strategy.update_user_agent(\"MyCustomUA/1.0\")\n    crawler1.crawler_strategy.set_custom_headers({\n        \"Accept-Language\": \"fr-FR,fr;q=0.9\"\n    })\n    result1 = await crawler1.arun(\"https://www.example.com\")\n    print(\"Example 1 result success:\", result1.success)\n\n    # Option 2: Pass headers directly to `arun()`\n    crawler2 = AsyncWebCrawler()\n    result2 = await crawler2.arun(\n        url=\"https://www.example.com\",\n        headers={\"Accept-Language\": \"es-ES,es;q=0.9\"}\n    )\n    print(\"Example 2 result success:\", result2.success)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Implementing Semaphore-based Web Crawler with Rate Limiting",
    "codeDescription": "Shows implementation of a web crawler using SemaphoreDispatcher for controlled concurrency and rate limiting. Includes configuration for request pacing and detailed monitoring.",
    "codeLanguage": "python",
    "codeTokens": 199,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_7",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "async def crawl_with_semaphore(urls):\n    browser_config = BrowserConfig(headless=True, verbose=False)\n    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n    \n    dispatcher = SemaphoreDispatcher(\n        semaphore_count=5,\n        rate_limiter=RateLimiter(\n            base_delay=(0.5, 1.0),\n            max_delay=10.0\n        ),\n        monitor=CrawlerMonitor(\n            max_visible_rows=15,\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        results = await crawler.arun_many(\n            urls, \n            config=run_config,\n            dispatcher=dispatcher\n        )\n        return results"
      }
    ],
    "relevance": 0.905
  },
  {
    "codeTitle": "Implementing Sliding Window Chunking in Python",
    "codeDescription": "A class that generates overlapping chunks for better contextual coherence. It takes a window_size parameter for chunk length and a step parameter to control the overlap between consecutive chunks.",
    "codeLanguage": "python",
    "codeTokens": 180,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-11_snippet_4",
    "pageTitle": "Chunking Strategies for Text Processing",
    "codeList": [
      {
        "language": "python",
        "code": "class SlidingWindowChunking:\n    def __init__(self, window_size=100, step=50):\n        self.window_size = window_size\n        self.step = step\n\n    def chunk(self, text):\n        words = text.split()\n        chunks = []\n        for i in range(0, len(words) - self.window_size + 1, self.step):\n            chunks.append(' '.join(words[i:i + self.window_size]))\n        return chunks\n\n# Example Usage\ntext = \"This is a long text to demonstrate sliding window chunking.\"\nchunker = SlidingWindowChunking(window_size=5, step=2)\nprint(chunker.chunk(text))"
      }
    ],
    "relevance": 0.903
  },
  {
    "codeTitle": "Configuring Page Navigation and Timing in CrawlerRunConfig for Python",
    "codeDescription": "Shows how to set parameters for controlling page navigation, timing, and JavaScript execution in CrawlerRunConfig. These settings affect how the crawler interacts with the page and handles dynamic content.",
    "codeLanguage": "python",
    "codeTokens": 115,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI Parameter Guide for arun() Method",
    "codeList": [
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    wait_for=\"css:.dynamic-content\", # Wait for .dynamic-content\n    delay_before_return_html=2.0,    # Wait 2s before capturing final HTML\n    page_timeout=60000,             # Navigation & script timeout (ms)\n)"
      },
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    js_code=[\n        \"window.scrollTo(0, document.body.scrollHeight);\",\n        \"document.querySelector('.load-more')?.click();\"\n    ],\n    js_only=False\n)"
      },
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    magic=True,\n    simulate_user=True,\n    override_navigator=True\n)"
      }
    ],
    "relevance": 0.903
  },
  {
    "codeTitle": "Working with Raw and Cleaned HTML Content in Python",
    "codeDescription": "Examples showing how to access and use the raw HTML and cleaned HTML content from a CrawlResult object, including checking content length and viewing snippets.",
    "codeLanguage": "python",
    "codeTokens": 49,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-11_snippet_2",
    "pageTitle": "CrawlResult Class Reference",
    "codeList": [
      {
        "language": "python",
        "code": "# Possibly large\nprint(len(result.html))"
      },
      {
        "language": "python",
        "code": "print(result.cleaned_html[:500])  # Show a snippet"
      },
      {
        "language": "python",
        "code": "if result.markdown.fit_html:\n    print(\"High-value HTML content:\", result.markdown.fit_html[:300])"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Input Format Selection for LLMExtractionStrategy in Crawl4AI",
    "codeDescription": "Example showing how to set the input format for LLMExtractionStrategy. This parameter determines which version of the crawler's content (HTML, markdown, or filtered markdown) is passed to the LLM for processing.",
    "codeLanguage": "python",
    "codeTokens": 88,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-11_snippet_2",
    "pageTitle": "LLM-Based JSON Extraction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "LLMExtractionStrategy(\n    # ...\n    input_format=\"html\",  # Instead of \"markdown\" or \"fit_markdown\"\n)"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Crawling a Web URL with Crawl4AI in Python",
    "codeDescription": "This snippet demonstrates how to use AsyncWebCrawler to crawl a live web page (Wikipedia) using a URL. It utilizes CrawlerRunConfig for configuration and prints the resulting markdown content.",
    "codeLanguage": "python",
    "codeTokens": 173,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-11_snippet_0",
    "pageTitle": "Prefix-Based Input Handling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_web():\n    config = CrawlerRunConfig(bypass_cache=True)\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://en.wikipedia.org/wiki/apple\", \n            config=config\n        )\n        if result.success:\n            print(\"Markdown Content:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl: {result.error_message}\")\n\nasyncio.run(crawl_web())"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Reusing Saved State in AsyncWebCrawler",
    "codeDescription": "This Python script demonstrates how to reuse a previously saved storage_state file in AsyncWebCrawler. It loads the session data from a file, allowing the crawler to start in an authenticated state without repeating the login process.",
    "codeLanguage": "python",
    "codeTokens": 270,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-11_snippet_4",
    "pageTitle": "Using storage_state for Session Management in Crawl4ai",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    # Second run: no need to hook on_browser_created this time.\n    # Just provide the previously saved storage state.\n    async with AsyncWebCrawler(\n        headless=True,\n        verbose=True,\n        use_persistent_context=True,\n        user_data_dir=\"./my_user_data\",\n        storage_state=\"my_storage_state.json\"  # Reuse previously exported state\n    ) as crawler:\n        \n        # Now the crawler starts already logged in\n        result = await crawler.arun(\n            url='https://example.com/protected-page',\n            cache_mode=CacheMode.BYPASS,\n            markdown_generator=DefaultMarkdownGenerator(options={\"ignore_links\": True}),\n        )\n        print(\"Second run result success:\", result.success)\n        if result.success:\n            print(\"Protected page HTML length:\", len(result.html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Combined Filtering Configuration in Crawl4AI",
    "codeDescription": "Example showing how to combine multiple filtering approaches including tag exclusion, external link exclusion, word count thresholds, and content filtering in a single CrawlerRunConfig. Demonstrates multi-level filtering strategy.",
    "codeLanguage": "python",
    "codeTokens": 111,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-11_snippet_4",
    "pageTitle": "Fit Markdown with Pruning & BM25 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    word_count_threshold=10,\n    excluded_tags=[\"nav\", \"footer\", \"header\"],\n    exclude_external_links=True,\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=PruningContentFilter(threshold=0.5)\n    )\n)"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Initializing OverlappingWindowChunking in Python for Text Chunking",
    "codeDescription": "This code snippet shows the initialization of OverlappingWindowChunking class, which creates chunks with specified overlap. It takes window size and overlap size as parameters.",
    "codeLanguage": "python",
    "codeTokens": 88,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_5",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "OverlappingWindowChunking(\n    window_size: int = 1000,   # Chunk size in words\n    overlap: int = 100         # Overlap size in words\n)"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Tracking URL Redirections in Crawl4AI",
    "codeDescription": "Demonstrates how to track final URLs after redirects. This feature allows applications to know both the initial URL requested and the final URL after any redirections have occurred.",
    "codeLanguage": "python",
    "codeTokens": 79,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-11_snippet_7",
    "pageTitle": "Crawl4AI 0.4.3 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "result = await crawler.arun(url)\nprint(f\"Initial URL: {url}\")\nprint(f\"Final URL: {result.redirected_url}\")"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Configuring Screenshot, PDF, and Media Options in CrawlerRunConfig for Python",
    "codeDescription": "Shows how to set parameters for capturing screenshots, generating PDFs, and handling media content in CrawlerRunConfig. These options allow for additional data collection beyond HTML content.",
    "codeLanguage": "python",
    "codeTokens": 133,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI Parameter Guide for arun() Method",
    "codeList": [
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    screenshot=True,             # Grab a screenshot as base64\n    screenshot_wait_for=1.0,     # Wait 1s before capturing\n    pdf=True,                    # Also produce a PDF\n    image_description_min_word_threshold=5,  # If analyzing alt text\n    image_score_threshold=3,                # Filter out low-score images\n)"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Using Custom Dispatcher with arun_many in Python",
    "codeDescription": "An example demonstrating how to use a custom dispatcher with arun_many for advanced concurrency control. The MemoryAdaptiveDispatcher dynamically manages concurrency based on system memory usage.",
    "codeLanguage": "python",
    "codeTokens": 111,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-11_snippet_3",
    "pageTitle": "arun_many Function Reference",
    "codeList": [
      {
        "language": "python",
        "code": "dispatcher = MemoryAdaptiveDispatcher(\n    memory_threshold_percent=70.0,\n    max_session_permit=10\n)\nresults = await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"],\n    config=my_run_config,\n    dispatcher=dispatcher\n)"
      }
    ],
    "relevance": 0.9
  },
  {
    "codeTitle": "Performance Optimization Configuration in Python",
    "codeDescription": "Shows how to optimize CosineStrategy performance by early filtering with word count threshold, limiting results with top_k, and enabling verbose mode for monitoring extraction performance during development.",
    "codeLanguage": "python",
    "codeTokens": 86,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_10",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "strategy = CosineStrategy(\n    word_count_threshold=10,  # Filter early\n    top_k=5,                 # Limit results\n    verbose=True             # Monitor performance\n)"
      }
    ],
    "relevance": 0.898
  },
  {
    "codeTitle": "Using Storage State for Session Persistence in Crawl4AI (Python)",
    "codeDescription": "Shows how to maintain browser state across sessions using storage_state, which preserves cookies and localStorage data for authentication and session continuity.",
    "codeLanguage": "python",
    "codeTokens": 277,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-11_snippet_4",
    "pageTitle": "Advanced Features in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    storage_dict = {\n        \"cookies\": [\n            {\n                \"name\": \"session\",\n                \"value\": \"abcd1234\",\n                \"domain\": \"example.com\",\n                \"path\": \"/\",\n                \"expires\": 1699999999.0,\n                \"httpOnly\": False,\n                \"secure\": False,\n                \"sameSite\": \"None\"\n            }\n        ],\n        \"origins\": [\n            {\n                \"origin\": \"https://example.com\",\n                \"localStorage\": [\n                    {\"name\": \"token\", \"value\": \"my_auth_token\"}\n                ]\n            }\n        ]\n    }\n\n    # Provide the storage state as a dictionary to start \"already logged in\"\n    async with AsyncWebCrawler(\n        headless=True,\n        storage_state=storage_dict\n    ) as crawler:\n        result = await crawler.arun(\"https://example.com/protected\")\n        if result.success:\n            print(\"Protected page content length:\", len(result.html))\n        else:\n            print(\"Failed to crawl protected page\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.897
  },
  {
    "codeTitle": "Accessing Internal Links from CrawlResult in Python",
    "codeDescription": "Code snippet demonstrating how to access and print the first 3 internal links from a crawl result's links dictionary. This shows the structure of link data available in the result.",
    "codeLanguage": "python",
    "codeTokens": 69,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-11_snippet_4",
    "pageTitle": "CrawlResult Object and Output Structure in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "print(result.links[\"internal\"][:3])  # Show first 3 internal links"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Configuring Browser and Crawler Settings in Python with Crawl4AI",
    "codeDescription": "Demonstrates how to use the new BrowserConfig and CrawlerRunConfig objects to configure browser and crawler behavior. Shows initialization of an AsyncWebCrawler with custom viewport settings and cache mode.",
    "codeLanguage": "python",
    "codeTokens": 150,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.2.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI 0.4.2 Update Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import BrowserConfig, CrawlerRunConfig, AsyncWebCrawler\n\nbrowser_config = BrowserConfig(headless=True, viewport_width=1920, viewport_height=1080)\ncrawler_config = CrawlerRunConfig(cache_mode=\"BYPASS\")\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\", config=crawler_config)\n    print(result.markdown[:500])"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Implementing Regex-Based Chunking in Python",
    "codeDescription": "A class that splits text into chunks based on regular expression patterns. By default, it uses paragraph breaks (\\n\\n) as the splitting pattern, but custom patterns can be provided.",
    "codeLanguage": "python",
    "codeTokens": 158,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-11_snippet_0",
    "pageTitle": "Chunking Strategies for Text Processing",
    "codeList": [
      {
        "language": "python",
        "code": "class RegexChunking:\n    def __init__(self, patterns=None):\n        self.patterns = patterns or [r'\\n\\n']  # Default pattern for paragraphs\n\n    def chunk(self, text):\n        paragraphs = [text]\n        for pattern in self.patterns:\n            paragraphs = [seg for p in paragraphs for seg in re.split(pattern, p)]\n        return paragraphs\n\n# Example Usage\ntext = \"\"\"This is the first paragraph.\n\nThis is the second paragraph.\"\"\"\nchunker = RegexChunking()\nprint(chunker.chunk(text))"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "New CacheMode-based Caching Method in Crawl4AI (Python)",
    "codeDescription": "This code snippet showcases the new recommended way of controlling caching in Crawl4AI using the CacheMode enum. It uses CrawlerRunConfig to set the cache mode to BYPASS, which is equivalent to the old 'bypass_cache=True' flag.",
    "codeLanguage": "python",
    "codeTokens": 200,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cache-modes.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI Cache System Migration Guide",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def use_proxy():\n    # Use CacheMode in CrawlerRunConfig\n    config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)  \n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            config=config  # Pass the configuration object\n        )\n        print(len(result.markdown))\n\nasync def main():\n    await use_proxy()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "LLM Schema Generation for CSS Extraction",
    "codeDescription": "Demonstrates the use of LLM-powered schema generation for CSS-based data extraction. The code shows how to generate extraction schemas automatically from HTML content using LLM.",
    "codeLanguage": "python",
    "codeTokens": 164,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_7",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\nfrom crawl4ai import LLMConfig\n\nllm_config = LLMConfig(provider=\"gemini/gemini-1.5-pro\", api_token=\"env:GEMINI_API_KEY\")\n\nschema = JsonCssExtractionStrategy.generate_schema(\n    html=\"<div class='product'><h2>Product Name</h2><span class='price'>$99</span></div>\",\n    llm_config = llm_config,\n    query=\"Extract product name and price\"\n)\nprint(schema)"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Enabling Text-Only Mode for Fast Crawling",
    "codeDescription": "Shows how to configure text-only mode for faster crawling by disabling images, JavaScript, and other heavy resources. This can make crawling 3-4 times faster when only text data is needed.",
    "codeLanguage": "python",
    "codeTokens": 82,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI Release 0.4.1 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "crawler = AsyncPlaywrightCrawlerStrategy(\n    text_mode=True  # Set this to True to enable text-only crawling\n)"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Using LXML Web Scraping Strategy with Crawl4AI in Python",
    "codeDescription": "This snippet demonstrates how to use the LXML-based scraping strategy in Crawl4AI, which offers better performance for large HTML documents compared to the default BeautifulSoup-based strategy.",
    "codeLanguage": "python",
    "codeTokens": 137,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_8",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LXMLWebScrapingStrategy\n\nasync def main():\n    config = CrawlerRunConfig(\n        scraping_strategy=LXMLWebScrapingStrategy()  # Faster alternative to default BeautifulSoup\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com\", \n            config=config\n        )"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Configuring Proxy Support and Rotation in Crawl4AI",
    "codeDescription": "Demonstrates how to set up integrated proxy support with authentication. This feature allows using proxies for web crawling to avoid IP-based rate limiting and restrictions.",
    "codeLanguage": "python",
    "codeTokens": 87,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI 0.4.3 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    proxy_config={\n        \"server\": \"http://proxy:8080\",\n        \"username\": \"user\",\n        \"password\": \"pass\"\n    }\n)"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "Implementing KeywordRelevanceScorer for Prioritized Crawling in Python",
    "codeDescription": "This snippet demonstrates how to use KeywordRelevanceScorer with BestFirstCrawlingStrategy in Crawl4AI. It shows how to prioritize crawling based on keyword relevance, allowing for more focused exploration of relevant content.",
    "codeLanguage": "python",
    "codeTokens": 269,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_8",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\n\n# Create a keyword relevance scorer\nkeyword_scorer = KeywordRelevanceScorer(\n    keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n    weight=0.7  # Importance of this scorer (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BestFirstCrawlingStrategy(\n        max_depth=2,\n        url_scorer=keyword_scorer\n    ),\n    stream=True  # Recommended with BestFirstCrawling\n)\n\n# Results will come in order of relevance score\nasync with AsyncWebCrawler() as crawler:\n    async for result in await crawler.arun(\"https://example.com\", config=config):\n        score = result.metadata.get(\"score\", 0)\n        print(f\"Score: {score:.2f} | {result.url}\")"
      }
    ],
    "relevance": 0.895
  },
  {
    "codeTitle": "PruningContentFilter Setup for General Content Cleanup",
    "codeDescription": "Configures a PruningContentFilter to remove boilerplate and low-value content. This filter uses text density, link density, and other heuristics to identify and remove non-essential content from web pages.",
    "codeLanguage": "python",
    "codeTokens": 104,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_3",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.content_filter_strategy import PruningContentFilter\n\nprune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",  # or \"dynamic\"\n    min_word_threshold=50\n)"
      }
    ],
    "relevance": 0.892
  },
  {
    "codeTitle": "Processing Dispatch Results from Web Crawler",
    "codeDescription": "Shows how to access and process dispatch results from completed crawl operations. Demonstrates accessing memory usage and duration metrics.",
    "codeLanguage": "python",
    "codeTokens": 100,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_10",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "for result in results:\n    if result.success:\n        dr = result.dispatch_result\n        print(f\"URL: {result.url}\")\n        print(f\"Memory: {dr.memory_usage:.1f}MB\")\n        print(f\"Duration: {dr.end_time - dr.start_time}\")"
      }
    ],
    "relevance": 0.892
  },
  {
    "codeTitle": "Screenshot Configuration with Custom Timing in Python",
    "codeDescription": "This example shows how to configure Crawl4AI to take screenshots with custom timing. It includes settings for headless mode, waiting for specific elements, and delay before capturing the screenshot.",
    "codeLanguage": "python",
    "codeTokens": 117,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_16",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "request = {\n    \"urls\": \"https://example.com\",\n    \"screenshot\": True,\n    \"crawler_params\": {\n        \"headless\": True,\n        \"screenshot_wait_for\": \".main-content\"\n    },\n    \"extra\": {\n        \"delay_before_return_html\": 3.0\n    }\n}"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Handling Media in Web Crawling",
    "codeDescription": "Demonstrates how to extract and process media content like images during web crawling, including options to include external images and take screenshots of the page.",
    "codeLanguage": "python",
    "codeTokens": 143,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_6",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "async def media_handling():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\", \n            bypass_cache=True,\n            exclude_external_images=False,\n            screenshot=True\n        )\n        for img in result.media['images'][:5]:\n            print(f\"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}\")\n        \nasyncio.run(media_handling())"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Configuring CosineStrategy for Performance Monitoring in Python",
    "codeDescription": "This snippet shows how to configure CosineStrategy for performance monitoring by enabling verbose logging, setting a word count threshold, and limiting the number of results.",
    "codeLanguage": "python",
    "codeTokens": 87,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_11",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "strategy = CosineStrategy(\n    verbose=True,  # Enable logging\n    word_count_threshold=20,  # Filter short content\n    top_k=5  # Limit results\n)"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Mixed Content Handling Configuration in Python",
    "codeDescription": "Demonstrates how to configure CosineStrategy for pages with mixed content types by using more flexible matching parameters, larger clusters, and retrieving multiple relevant content sections.",
    "codeLanguage": "python",
    "codeTokens": 104,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_11",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "# For mixed content pages\nstrategy = CosineStrategy(\n    semantic_filter=\"product features\",\n    sim_threshold=0.4,      # More flexible matching\n    max_dist=0.3,          # Larger clusters\n    top_k=3                # Multiple relevant sections\n)"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Accessing Media Data in Crawl4AI Result",
    "codeDescription": "This snippet demonstrates how to access and process media data (images and tables) from the crawl result. It shows how to iterate through image information and table data, printing relevant details for each.",
    "codeLanguage": "python",
    "codeTokens": 268,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#2025-04-11_snippet_2",
    "pageTitle": "Link and Media Extraction in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "if result.success:\n    # Get images\n    images_info = result.media.get(\"images\", [])\n    print(f\"Found {len(images_info)} images in total.\")\n    for i, img in enumerate(images_info[:3]):  # Inspect just the first 3\n        print(f\"[Image {i}] URL: {img['src']}\")\n        print(f\"           Alt text: {img.get('alt', '')}\")\n        print(f\"           Score: {img.get('score')}\")\n        print(f\"           Description: {img.get('desc', '')}\\n\")\n    \n    # Get tables\n    tables = result.media.get(\"tables\", [])\n    print(f\"Found {len(tables)} data tables in total.\")\n    for i, table in enumerate(tables):\n        print(f\"[Table {i}] Caption: {table.get('caption', 'No caption')}\")\n        print(f\"           Columns: {len(table.get('headers', []))}\")\n        print(f\"           Rows: {len(table.get('rows', []))}\")"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Installing Crawl4AI Package",
    "codeDescription": "Instructions for installing Crawl4AI, running post-installation setup, and verifying installation. Includes an optional step for manually installing browser dependencies if needed.",
    "codeLanguage": "bash",
    "codeTokens": 101,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "bash",
        "code": "# Install the package\npip install -U crawl4ai\n\n# For pre release versions\npip install crawl4ai --pre\n\n# Run post-installation setup\ncrawl4ai-setup\n\n# Verify your installation\ncrawl4ai-doctor"
      },
      {
        "language": "bash",
        "code": "python -m playwright install --with-deps chromium"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Configuring Dynamic Viewport Adjustment",
    "codeDescription": "Demonstrates how to enable dynamic viewport size adjustment to match page content dimensions, useful for responsive layouts and ensuring proper content loading.",
    "codeLanguage": "python",
    "codeTokens": 72,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI Release 0.4.1 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "await crawler.crawl(\n    url=\"https://example.com\",\n    adjust_viewport_to_content=True  # Dynamically adjusts the viewport\n)"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Demonstrating Fit Markdown Feature in Crawl4AI using Python",
    "codeDescription": "This snippet demonstrates the Fit Markdown feature of Crawl4AI, which extracts the main content from a webpage. It uses AsyncWebCrawler to fetch and process the content from a specified URL.",
    "codeLanguage": "python",
    "codeTokens": 147,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI v0.3.72 Release Announcement and Feature Demonstration",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def fit_markdown_demo():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=\"https://janineintheworld.com/places-to-visit-in-central-mexico\")\n        print(result.fit_markdown)  # Shows main content in Markdown format\n\n# Run the demo\nawait fit_markdown_demo()"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Performing Login and Saving State in AsyncWebCrawler",
    "codeDescription": "This Python script demonstrates how to perform a login operation and save the resulting storage_state to a file. It uses a custom hook to handle the login process and exports the session state for future use.",
    "codeLanguage": "python",
    "codeTokens": 424,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-11_snippet_3",
    "pageTitle": "Using storage_state for Session Management in Crawl4ai",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CacheMode\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def on_browser_created_hook(browser):\n    # Access the default context and create a page\n    context = browser.contexts[0]\n    page = await context.new_page()\n    \n    # Navigate to the login page\n    await page.goto(\"https://example.com/login\", wait_until=\"domcontentloaded\")\n    \n    # Fill in credentials and submit\n    await page.fill(\"input[name='username']\", \"myuser\")\n    await page.fill(\"input[name='password']\", \"mypassword\")\n    await page.click(\"button[type='submit']\")\n    await page.wait_for_load_state(\"networkidle\")\n    \n    # Now the site sets tokens in localStorage and cookies\n    # Export this state to a file so we can reuse it\n    await context.storage_state(path=\"my_storage_state.json\")\n    await page.close()\n\nasync def main():\n    # First run: perform login and export the storage_state\n    async with AsyncWebCrawler(\n        headless=True,\n        verbose=True,\n        hooks={\"on_browser_created\": on_browser_created_hook},\n        use_persistent_context=True,\n        user_data_dir=\"./my_user_data\"\n    ) as crawler:\n        \n        # After on_browser_created_hook runs, we have storage_state saved to my_storage_state.json\n        result = await crawler.arun(\n            url='https://example.com/protected-page',\n            cache_mode=CacheMode.BYPASS,\n            markdown_generator=DefaultMarkdownGenerator(options={\"ignore_links\": True}),\n        )\n        print(\"First run result success:\", result.success)\n        if result.success:\n            print(\"Protected page HTML length:\", len(result.html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.89
  },
  {
    "codeTitle": "Similarity Threshold Configuration Examples in Python",
    "codeDescription": "Demonstrates how to configure similarity thresholds in CosineStrategy for strict and loose matching. Higher threshold values enforce stricter matching while lower values allow more variation in content matching.",
    "codeLanguage": "python",
    "codeTokens": 80,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_2",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "# Strict matching\nstrategy = CosineStrategy(sim_threshold=0.8)\n\n# Loose matching\nstrategy = CosineStrategy(sim_threshold=0.3)"
      }
    ],
    "relevance": 0.888
  },
  {
    "codeTitle": "Using Additional CrawlResult Fields in Python",
    "codeDescription": "Examples demonstrating how to work with additional fields from a CrawlResult, including extracted content, downloaded files, screenshots, PDFs, and metadata.",
    "codeLanguage": "python",
    "codeTokens": 60,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-11_snippet_5",
    "pageTitle": "CrawlResult Class Reference",
    "codeList": [
      {
        "language": "python",
        "code": "if result.extracted_content:\n    data = json.loads(result.extracted_content)\n    print(data)"
      },
      {
        "language": "python",
        "code": "if result.downloaded_files:\n    for file_path in result.downloaded_files:\n        print(\"Downloaded:\", file_path)"
      },
      {
        "language": "python",
        "code": "import base64\nif result.screenshot:\n    with open(\"page.png\", \"wb\") as f:\n        f.write(base64.b64decode(result.screenshot))"
      },
      {
        "language": "python",
        "code": "if result.pdf:\n    with open(\"page.pdf\", \"wb\") as f:\n        f.write(result.pdf)"
      },
      {
        "language": "python",
        "code": "if result.metadata:\n    print(\"Title:\", result.metadata.get(\"title\"))\n    print(\"Author:\", result.metadata.get(\"author\"))"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Cleaning and Fitting Content in Web Crawling",
    "codeDescription": "Demonstrates content cleaning features like excluding specific HTML tags, removing overlay elements, and applying word count thresholds to get more relevant and concise content.",
    "codeLanguage": "python",
    "codeTokens": 194,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "async def clean_content():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://janineintheworld.com/places-to-visit-in-central-mexico\",\n            excluded_tags=['nav', 'footer', 'aside'],\n            remove_overlay_elements=True,\n            word_count_threshold=10,\n            bypass_cache=True\n        )\n        full_markdown_length = len(result.markdown.raw_markdown)\n        fit_markdown_length = len(result.markdown.fit_markdown)\n        print(f\"Full Markdown Length: {full_markdown_length}\")\n        print(f\"Fit Markdown Length: {fit_markdown_length}\")\n        print(result.markdown.fit_markdown[:1000])\n        \n\nasyncio.run(clean_content())"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "LLMConfig Implementation Example",
    "codeDescription": "Shows how to use the new LLMConfig parameter for configuring LLM-based extraction and filtering tasks.",
    "codeLanguage": "python",
    "codeTokens": 179,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_11",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\n# Example of using LLMConfig with LLMExtractionStrategy\nllm_config = LLMConfig(provider=\"openai/gpt-4o\", api_token=\"YOUR_API_KEY\")\nstrategy = LLMExtractionStrategy(llm_config=llm_config, schema=...)\n\n# Example usage within a crawler\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        config=CrawlerRunConfig(extraction_strategy=strategy)\n    )"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Minimal AsyncWebCrawler Implementation with BrowserConfig",
    "codeDescription": "A minimal example showing how to initialize an AsyncWebCrawler with a custom BrowserConfig and run a simple crawl. The example demonstrates using a Firefox browser in visible mode with text-only fetching.",
    "codeLanguage": "python",
    "codeTokens": 128,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-11_snippet_3",
    "pageTitle": "Browser, Crawler & LLM Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_conf = BrowserConfig(\n    browser_type=\"firefox\",\n    headless=False,\n    text_mode=True\n)\n\nasync with AsyncWebCrawler(config=browser_conf) as crawler:\n    result = await crawler.arun(\"https://example.com\")\n    print(result.markdown[:300])"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Using arun() Method with CrawlerRunConfig in Python",
    "codeDescription": "Demonstrates how to use the arun() method with a CrawlerRunConfig object. This approach allows for detailed configuration of crawl parameters, including caching, content filtering, and screenshot options.",
    "codeLanguage": "python",
    "codeTokens": 169,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-11_snippet_4",
    "pageTitle": "AsyncWebCrawler Class Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import CrawlerRunConfig, CacheMode\n\nrun_cfg = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS,\n    css_selector=\"main.article\",\n    word_count_threshold=10,\n    screenshot=True\n)\n\nasync with AsyncWebCrawler(config=browser_cfg) as crawler:\n    result = await crawler.arun(\"https://example.com/news\", config=run_cfg)\n    print(\"Crawled HTML length:\", len(result.cleaned_html))\n    if result.screenshot:\n        print(\"Screenshot base64 length:\", len(result.screenshot))"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Installing Crawl4AI and Dependencies in Python",
    "codeDescription": "This snippet installs Crawl4AI, Playwright, and nest_asyncio using pip. It's necessary to set up the environment for using Crawl4AI and ensure compatibility with Colab's asynchronous environment.",
    "codeLanguage": "python",
    "codeTokens": 91,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI v0.3.72 Release Announcement and Feature Demonstration",
    "codeList": [
      {
        "language": "python",
        "code": "# Install Crawl4AI and dependencies\n!pip install crawl4ai\n!playwright install\n!pip install nest_asyncio"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "LLM Q&A Command Examples",
    "codeDescription": "Examples showing how to use the Q&A feature to ask questions about crawled content, including combined configurations with advanced crawling options.",
    "codeLanguage": "bash",
    "codeTokens": 154,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_10",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "# Simple question\ncrwl https://example.com -q \"What is the main topic discussed?\"\n\n# View content then ask questions\ncrwl https://example.com -o markdown  # See content first\ncrwl https://example.com -q \"Summarize the key points\"\ncrwl https://example.com -q \"What are the conclusions?\"\n\n# Combined with advanced crawling\ncrwl https://example.com \\\n    -B browser.yml \\\n    -c \"css_selector=article,scan_full_page=true\" \\\n    -q \"What are the pros and cons mentioned?\""
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Downloading Files with AsyncWebCrawler in Python",
    "codeDescription": "Demonstrates how to use the new file download capabilities of AsyncWebCrawler, including specifying a download path and using JavaScript to trigger downloads.",
    "codeLanguage": "python",
    "codeTokens": 231,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_7",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nimport os\nfrom pathlib import Path\nfrom crawl4ai import AsyncWebCrawler\n\nasync def download_example():\n    downloads_path = os.path.join(Path.home(), \".crawl4ai\", \"downloads\")\n    os.makedirs(downloads_path, exist_ok=True)\n\n    async with AsyncWebCrawler(\n        accept_downloads=True, \n        downloads_path=downloads_path, \n        verbose=True\n    ) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.python.org/downloads/\",\n            js_code=\"\"\"\n                const downloadLink = document.querySelector('a[href$=\".exe\"]');\n                if (downloadLink) { downloadLink.click(); }\n            \"\"\",\n            wait_for=5 # To ensure download has started\n        )\n\n        if result.downloaded_files:\n            print(\"Downloaded files:\")\n            for file in result.downloaded_files:\n                print(f\"- {file}\")\n\nasyncio.run(download_example())"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Article Content Extraction Use Case in Python",
    "codeDescription": "Example of using CosineStrategy for extracting main article content from blog posts. It configures a higher word count threshold suitable for articles and targets only the single main content section.",
    "codeLanguage": "python",
    "codeTokens": 116,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_5",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "strategy = CosineStrategy(\n    semantic_filter=\"main article content\",\n    word_count_threshold=100,  # Longer blocks for articles\n    top_k=1                   # Usually want single main content\n)\n\nresult = await crawler.arun(\n    url=\"https://example.com/blog/post\",\n    extraction_strategy=strategy\n)"
      }
    ],
    "relevance": 0.885
  },
  {
    "codeTitle": "Configuring LXML-Based Scraping in Crawl4AI",
    "codeDescription": "Shows how to enable the new LXML scraping strategy that offers up to 20x faster parsing compared to previous methods. The example also demonstrates enabling cache mode for improved performance.",
    "codeLanguage": "python",
    "codeTokens": 82,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI 0.4.3 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    scraping_strategy=LXMLWebScrapingStrategy(),\n    cache_mode=CacheMode.ENABLED\n)"
      }
    ],
    "relevance": 0.882
  },
  {
    "codeTitle": "Accessing Different Markdown Formats from MarkdownGenerationResult in Python",
    "codeDescription": "This snippet shows how to access various markdown formats stored in the MarkdownGenerationResult object, including raw markdown, markdown with citations, references, and filtered markdown. It's useful for debugging or advanced usage scenarios.",
    "codeLanguage": "python",
    "codeTokens": 120,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_8",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "md_obj = result.markdown  # your library's naming may vary\nprint(\"RAW:\\n\", md_obj.raw_markdown)\nprint(\"CITED:\\n\", md_obj.markdown_with_citations)\nprint(\"REFERENCES:\\n\", md_obj.references_markdown)\nprint(\"FIT:\\n\", md_obj.fit_markdown)"
      }
    ],
    "relevance": 0.882
  },
  {
    "codeTitle": "Configuring Custom Download Location in Python",
    "codeDescription": "Shows how to specify a custom download directory for Crawl4AI using the downloads_path parameter in BrowserConfig.",
    "codeLanguage": "python",
    "codeTokens": 129,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-11_snippet_1",
    "pageTitle": "Download Handling Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_configs import BrowserConfig\nimport os\n\ndownloads_path = os.path.join(os.getcwd(), \"my_downloads\")  # Custom download path\nos.makedirs(downloads_path, exist_ok=True)\n\nconfig = BrowserConfig(accept_downloads=True, downloads_path=downloads_path)\n\nasync def main():\n    async with AsyncWebCrawler(config=config) as crawler:\n        result = await crawler.arun(url=\"https://example.com\")\n        # ..."
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Implementing RateLimiter Class in Python for Crawl4AI",
    "codeDescription": "Code snippet for the RateLimiter class constructor which handles delays between requests and implements exponential backoff for rate limit responses. It manages request pacing to prevent server overload.",
    "codeLanguage": "python",
    "codeTokens": 159,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_0",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "class RateLimiter:\n    def __init__(\n        # Random delay range between requests\n        base_delay: Tuple[float, float] = (1.0, 3.0),  \n        \n        # Maximum backoff delay\n        max_delay: float = 60.0,                        \n        \n        # Retries before giving up\n        max_retries: int = 3,                          \n        \n        # Status codes triggering backoff\n        rate_limit_codes: List[int] = [429, 503]        \n    )"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Configuring Robots.txt Compliance in Python",
    "codeDescription": "Example of setting up crawler compliance settings using CrawlerRunConfig. Enables robots.txt checking and sets a custom user agent for crawler identification.",
    "codeLanguage": "python",
    "codeTokens": 83,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-11_snippet_4",
    "pageTitle": "Browser and Crawler Configuration in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    check_robots_txt=True,  # Enable robots.txt compliance\n    user_agent=\"MyBot/1.0\"  # Identify your crawler\n)"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "CSS-Based Waiting in Crawl4AI",
    "codeDescription": "Shows how to use the wait_for parameter with CSS selectors to make the crawler wait for specific elements to appear on the page before continuing execution.",
    "codeLanguage": "python",
    "codeTokens": 183,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-11_snippet_1",
    "pageTitle": "Dynamic Page Interaction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Wait for at least 30 items on Hacker News\n        wait_for=\"css:.athing:nth-child(30)\"  \n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://news.ycombinator.com\",\n            config=config\n        )\n        print(\"We have at least 30 items loaded!\")\n        # Rough check\n        print(\"Total items in HTML:\", result.cleaned_html.count(\"athing\"))  \n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Configuring Managed Browser Profile in Python",
    "codeDescription": "Python code demonstrating how to set up and use a managed browser profile with Crawl4AI for identity-based crawling",
    "codeLanguage": "python",
    "codeTokens": 238,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-11_snippet_2",
    "pageTitle": "Setting up Browser Profiles with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    # 1) Reference your persistent data directory\n    browser_config = BrowserConfig(\n        headless=True,             # 'True' for automated runs\n        verbose=True,\n        use_managed_browser=True,  # Enables persistent browser strategy\n        browser_type=\"chromium\",\n        user_data_dir=\"/path/to/my-chrome-profile\"\n    )\n\n    # 2) Standard crawl config\n    crawl_config = CrawlerRunConfig(\n        wait_for=\"css:.logged-in-content\"\n    )\n\n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(url=\"https://example.com/private\", config=crawl_config)\n        if result.success:\n            print(\"Successfully accessed private data with your identity!\")\n        else:\n            print(\"Error:\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Accessing Downloaded Files in Python",
    "codeDescription": "Shows how to access and verify downloaded files using the CrawlResult object's downloaded_files attribute.",
    "codeLanguage": "python",
    "codeTokens": 97,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-11_snippet_3",
    "pageTitle": "Download Handling Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "if result.downloaded_files:\n    print(\"Downloaded files:\")\n    for file_path in result.downloaded_files:\n        print(f\"- {file_path}\")\n        file_size = os.path.getsize(file_path)\n        print(f\"- File size: {file_size} bytes\")\nelse:\n    print(\"No files downloaded.\")"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Defining storage_state Structure in JSON",
    "codeDescription": "This snippet shows the structure of a storage_state object in JSON format. It includes cookie and localStorage data for a specific domain.",
    "codeLanguage": "json",
    "codeTokens": 183,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-11_snippet_0",
    "pageTitle": "Using storage_state for Session Management in Crawl4ai",
    "codeList": [
      {
        "language": "json",
        "code": "{\n  \"cookies\": [\n    {\n      \"name\": \"session\",\n      \"value\": \"abcd1234\",\n      \"domain\": \"example.com\",\n      \"path\": \"/\",\n      \"expires\": 1675363572.037711,\n      \"httpOnly\": false,\n      \"secure\": false,\n      \"sameSite\": \"None\"\n    }\n  ],\n  \"origins\": [\n    {\n      \"origin\": \"https://example.com\",\n      \"localStorage\": [\n        { \"name\": \"token\", \"value\": \"my_auth_token\" },\n        { \"name\": \"refreshToken\", \"value\": \"my_refresh_token\" }\n      ]\n    }\n  ]\n}"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Processing Markdown Content from CrawlResult in Python",
    "codeDescription": "Examples demonstrating how to work with the markdown field of a CrawlResult, which can contain raw markdown, markdown with citations, references, and filtered content when content filtering is applied.",
    "codeLanguage": "python",
    "codeTokens": 125,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-11_snippet_3",
    "pageTitle": "CrawlResult Class Reference",
    "codeList": [
      {
        "language": "python",
        "code": "if result.markdown:\n    md_res = result.markdown\n    print(\"Raw MD:\", md_res.raw_markdown[:300])\n    print(\"Citations MD:\", md_res.markdown_with_citations[:300])\n    print(\"References:\", md_res.references_markdown)\n    if md_res.fit_markdown:\n        print(\"Pruned text:\", md_res.fit_markdown[:300])"
      },
      {
        "language": "python",
        "code": "print(result.markdown.raw_markdown[:200])\nprint(result.markdown.fit_markdown)\nprint(result.markdown.fit_html)"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Typical Initialization of AsyncWebCrawler in Python",
    "codeDescription": "Example of how to initialize an AsyncWebCrawler instance with a BrowserConfig object. This demonstrates the recommended way to set up the crawler with specific browser settings.",
    "codeLanguage": "python",
    "codeTokens": 91,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-11_snippet_1",
    "pageTitle": "AsyncWebCrawler Class Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, BrowserConfig\n\nbrowser_cfg = BrowserConfig(\n    browser_type=\"chromium\",\n    headless=True,\n    verbose=True\n)\n\ncrawler = AsyncWebCrawler(config=browser_cfg)"
      }
    ],
    "relevance": 0.88
  },
  {
    "codeTitle": "Crawling Raw HTML Content with Crawl4AI in Python",
    "codeDescription": "This snippet illustrates how to use AsyncWebCrawler to crawl raw HTML content by prefixing the HTML string with 'raw:'. It employs CrawlerRunConfig for configuration and prints the resulting markdown content.",
    "codeLanguage": "python",
    "codeTokens": 207,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-11_snippet_2",
    "pageTitle": "Prefix-Based Input Handling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_raw_html():\n    raw_html = \"<html><body><h1>Hello, World!</h1></body></html>\"\n    raw_html_url = f\"raw:{raw_html}\"\n    config = CrawlerRunConfig(bypass_cache=True)\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=raw_html_url, config=config)\n        if result.success:\n            print(\"Markdown Content from Raw HTML:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl raw HTML: {result.error_message}\")\n\nasyncio.run(crawl_raw_html())"
      }
    ],
    "relevance": 0.878
  },
  {
    "codeTitle": "Defining CrawlResult and MarkdownGenerationResult Models in Python",
    "codeDescription": "Core schema definitions for the CrawlResult and MarkdownGenerationResult classes using Python BaseModel. The CrawlResult contains all crawl outputs including HTML, content extractions, metadata and media references.",
    "codeLanguage": "python",
    "codeTokens": 290,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-11_snippet_0",
    "pageTitle": "CrawlResult Object and Output Structure in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "class MarkdownGenerationResult(BaseModel):\n    raw_markdown: str\n    markdown_with_citations: str\n    references_markdown: str\n    fit_markdown: Optional[str] = None\n    fit_html: Optional[str] = None\n\nclass CrawlResult(BaseModel):\n    url: str\n    html: str\n    success: bool\n    cleaned_html: Optional[str] = None\n    media: Dict[str, List[Dict]] = {}\n    links: Dict[str, List[Dict]] = {}\n    downloaded_files: Optional[List[str]] = None\n    screenshot: Optional[str] = None\n    pdf : Optional[bytes] = None\n    markdown: Optional[Union[str, MarkdownGenerationResult]] = None\n    extracted_content: Optional[str] = None\n    metadata: Optional[dict] = None\n    error_message: Optional[str] = None\n    session_id: Optional[str] = None\n    response_headers: Optional[dict] = None\n    status_code: Optional[int] = None\n    ssl_certificate: Optional[SSLCertificate] = None\n    class Config:\n        arbitrary_types_allowed = True"
      }
    ],
    "relevance": 0.878
  },
  {
    "codeTitle": "LLM Extraction with Filtering Example",
    "codeDescription": "Complete command example showing LLM-based extraction with BM25 filtering, using browser configuration and JSON output.",
    "codeLanguage": "bash",
    "codeTokens": 73,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_17",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crwl https://example.com \\\n    -B browser.yml \\\n    -e extract_llm.yml \\\n    -s llm_schema.json \\\n    -f filter_bm25.yml \\\n    -o json"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Configuring LLM-Powered Markdown Generation in Crawl4AI",
    "codeDescription": "Demonstrates how to set up smart content filtering and organization using LLMs. This feature uses an LLM provider to extract specific content types from crawled pages according to custom instructions.",
    "codeLanguage": "python",
    "codeTokens": 104,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI 0.4.3 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    markdown_generator=DefaultMarkdownGenerator(\n        content_filter=LLMContentFilter(\n            provider=\"openai/gpt-4o\",\n            instruction=\"Extract technical documentation and code examples\"\n        )\n    )\n)"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Creating and Cloning Crawler Configurations in Python",
    "codeDescription": "Demonstrates how to create base crawler configurations and create variations using the clone() method. Shows pattern for modifying configurations without affecting the original.",
    "codeLanguage": "python",
    "codeTokens": 113,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-11_snippet_2",
    "pageTitle": "Browser and Crawler Configuration in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "# Create a base configuration\nbase_config = CrawlerRunConfig(\n    cache_mode=CacheMode.ENABLED,\n    word_count_threshold=200\n)\n\n# Create variations using clone()\nstream_config = base_config.clone(stream=True)\nno_cache_config = base_config.clone(\n    cache_mode=CacheMode.BYPASS,\n    stream=True\n)"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Implementing Full-Page Scrolling Simulation",
    "codeDescription": "Shows how to enable full-page scanning with simulated scrolling to capture dynamically loaded content. Includes optional scroll delay configuration.",
    "codeLanguage": "python",
    "codeTokens": 84,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI Release 0.4.1 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "await crawler.crawl(\n    url=\"https://example.com\",\n    scan_full_page=True,   # Enables scrolling\n    scroll_delay=0.2       # Waits 200ms between scrolls (optional)\n)"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Verifying Crawl4AI Installation",
    "codeDescription": "Python script to verify successful installation by crawling a sample website and displaying extracted content.",
    "codeLanguage": "python",
    "codeTokens": 102,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-11_snippet_7",
    "pageTitle": "Crawl4AI Installation Guide",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(url=\"https://www.example.com\")\n        print(result.markdown[:500])  # Print first 500 characters\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Word Count Threshold Configuration in Python",
    "codeDescription": "Shows how to set the word count threshold to filter out short content blocks in CosineStrategy. This helps eliminate noise and irrelevant content by only considering substantial paragraphs.",
    "codeLanguage": "python",
    "codeTokens": 64,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_3",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "# Only consider substantial paragraphs\nstrategy = CosineStrategy(word_count_threshold=50)"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Working with Dispatch Results in Python",
    "codeDescription": "Example showing how to access and use the dispatch_result field from a CrawlResult object, which provides information about concurrency and resource usage when crawling URLs in parallel.",
    "codeLanguage": "python",
    "codeTokens": 130,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#2025-04-11_snippet_6",
    "pageTitle": "CrawlResult Class Reference",
    "codeList": [
      {
        "language": "python",
        "code": "# Example usage:\nfor result in results:\n    if result.success and result.dispatch_result:\n        dr = result.dispatch_result\n        print(f\"URL: {result.url}, Task ID: {dr.task_id}\")\n        print(f\"Memory: {dr.memory_usage:.1f} MB (Peak: {dr.peak_memory:.1f} MB)\")\n        print(f\"Duration: {dr.end_time - dr.start_time}\")"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Configuring SEO Filter with Crawl4AI",
    "codeDescription": "Sets up an SEO filter to search for specific keywords in page metadata with a configurable threshold score.",
    "codeLanguage": "python",
    "codeTokens": 112,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_9",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "seo_filter = SEOFilter(\n    threshold=0.5,  # Minimum score (0.0 to 1.0)\n    keywords=[\"tutorial\", \"guide\", \"documentation\"]\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([seo_filter])\n    )\n)"
      }
    ],
    "relevance": 0.875
  },
  {
    "codeTitle": "Handling Iframes in Crawl4AI",
    "codeDescription": "This snippet demonstrates how to configure CrawlerRunConfig to process iframes and remove overlay elements, allowing for inline inclusion of iframe content in the final output.",
    "codeLanguage": "python",
    "codeTokens": 152,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_4",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        process_iframes=True,\n        remove_overlay_elements=True\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.org/iframe-demo\", \n            config=config\n        )\n        print(\"Iframe-merged length:\", len(result.cleaned_html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.873
  },
  {
    "codeTitle": "Implementing Sentence-Based Chunking with NLTK",
    "codeDescription": "A class that divides text into sentences using NLTK's sent_tokenize function. This method is ideal for extracting meaningful statements from text.",
    "codeLanguage": "python",
    "codeTokens": 109,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-11_snippet_1",
    "pageTitle": "Chunking Strategies for Text Processing",
    "codeList": [
      {
        "language": "python",
        "code": "from nltk.tokenize import sent_tokenize\n\nclass NlpSentenceChunking:\n    def chunk(self, text):\n        sentences = sent_tokenize(text)\n        return [sentence.strip() for sentence in sentences]\n\n# Example Usage\ntext = \"This is sentence one. This is sentence two.\"\nchunker = NlpSentenceChunking()\nprint(chunker.chunk(text))"
      }
    ],
    "relevance": 0.872
  },
  {
    "codeTitle": "Installing Crawl4AI and Dependencies",
    "codeDescription": "Installs the Crawl4AI package and its dependencies including nest_asyncio for handling asyncio in notebooks and Playwright for browser automation.",
    "codeLanguage": "python",
    "codeTokens": 62,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "# %%capture\n!pip install crawl4ai\n!pip install nest_asyncio\n!playwright install"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Content Selection using target_elements in Crawl4AI",
    "codeDescription": "This snippet shows how to use the target_elements parameter in CrawlerRunConfig to focus on multiple specific elements while preserving the entire page context for other features.",
    "codeLanguage": "python",
    "codeTokens": 178,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-11_snippet_1",
    "pageTitle": "Content Selection in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    config = CrawlerRunConfig(\n        # Target article body and sidebar, but not other content\n        target_elements=[\"article.main-content\", \"aside.sidebar\"]\n    )\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://example.com/blog-post\", \n            config=config\n        )\n        print(\"Markdown focused on target elements\")\n        print(\"Links from entire page still available:\", len(result.links.get(\"internal\", [])))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Advanced Usage Example with JSON-CSS Schema",
    "codeDescription": "Example showing how to extract content from a webpage using a CSS extraction configuration file and a JSON schema definition.",
    "codeLanguage": "bash",
    "codeTokens": 70,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crwl \"https://www.infoq.com/ai-ml-data-eng/\" -e docs/examples/cli/extract_css.yml -s docs/examples/cli/css_schema.json -o json;"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Capturing PDFs and Screenshots with Crawl4AI (Python)",
    "codeDescription": "Shows how to capture both PDF exports and screenshots of web pages in a single crawl operation, saving them as files.",
    "codeLanguage": "python",
    "codeTokens": 233,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-11_snippet_1",
    "pageTitle": "Advanced Features in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import os, asyncio\nfrom base64 import b64decode\nfrom crawl4ai import AsyncWebCrawler, CacheMode\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\",\n            cache_mode=CacheMode.BYPASS,\n            pdf=True,\n            screenshot=True\n        )\n        \n        if result.success:\n            # Save screenshot\n            if result.screenshot:\n                with open(\"wikipedia_screenshot.png\", \"wb\") as f:\n                    f.write(b64decode(result.screenshot))\n            \n            # Save PDF\n            if result.pdf:\n                with open(\"wikipedia_page.pdf\", \"wb\") as f:\n                    f.write(result.pdf)\n            \n            print(\"[OK] PDF & screenshot captured.\")\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Using storage_state as a Dictionary in AsyncWebCrawler",
    "codeDescription": "This Python script demonstrates how to use storage_state as a dictionary when initializing AsyncWebCrawler. It sets up session data and performs a crawl on a protected page.",
    "codeLanguage": "python",
    "codeTokens": 291,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-11_snippet_1",
    "pageTitle": "Using storage_state for Session Management in Crawl4ai",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    storage_dict = {\n        \"cookies\": [\n            {\n                \"name\": \"session\",\n                \"value\": \"abcd1234\",\n                \"domain\": \"example.com\",\n                \"path\": \"/\",\n                \"expires\": 1675363572.037711,\n                \"httpOnly\": False,\n                \"secure\": False,\n                \"sameSite\": \"None\"\n            }\n        ],\n        \"origins\": [\n            {\n                \"origin\": \"https://example.com\",\n                \"localStorage\": [\n                    {\"name\": \"token\", \"value\": \"my_auth_token\"},\n                    {\"name\": \"refreshToken\", \"value\": \"my_refresh_token\"}\n                ]\n            }\n        ]\n    }\n\n    async with AsyncWebCrawler(\n        headless=True,\n        storage_state=storage_dict\n    ) as crawler:\n        result = await crawler.arun(url='https://example.com/protected')\n        if result.success:\n            print(\"Crawl succeeded with pre-loaded session data!\")\n            print(\"Page HTML length:\", len(result.html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Installing Crawl4AI Package Update",
    "codeDescription": "Shows how to install or upgrade to version 0.4.1 of Crawl4AI using pip package manager.",
    "codeLanguage": "bash",
    "codeTokens": 49,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI Release 0.4.1 Documentation",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai --upgrade"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Implementing Fixed-Length Word Chunking in Python",
    "codeDescription": "A class that splits text into chunks of a fixed word count. The chunk_size parameter determines how many words each chunk should contain.",
    "codeLanguage": "python",
    "codeTokens": 147,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-11_snippet_3",
    "pageTitle": "Chunking Strategies for Text Processing",
    "codeList": [
      {
        "language": "python",
        "code": "class FixedLengthWordChunking:\n    def __init__(self, chunk_size=100):\n        self.chunk_size = chunk_size\n\n    def chunk(self, text):\n        words = text.split()\n        return [' '.join(words[i:i + self.chunk_size]) for i in range(0, len(words), self.chunk_size)]\n\n# Example Usage\ntext = \"This is a long text with many words to be chunked into fixed sizes.\"\nchunker = FixedLengthWordChunking(chunk_size=5)\nprint(chunker.chunk(text))"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Setting Up Authenticated Proxy in Crawl4AI",
    "codeDescription": "This example demonstrates how to configure an authenticated proxy using BrowserConfig in Crawl4AI. It includes server, username, and password in the proxy configuration.",
    "codeLanguage": "python",
    "codeTokens": 126,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/proxy-security.md#2025-04-11_snippet_1",
    "pageTitle": "Proxy Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.async_configs import BrowserConfig\n\nproxy_config = {\n    \"server\": \"http://proxy.example.com:8080\",\n    \"username\": \"user\",\n    \"password\": \"pass\"\n}\n\nbrowser_config = BrowserConfig(proxy_config=proxy_config)\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    result = await crawler.arun(url=\"https://example.com\")"
      }
    ],
    "relevance": 0.87
  },
  {
    "codeTitle": "Top-K Configuration for Content Clusters in Python",
    "codeDescription": "Demonstrates how to configure the number of top content clusters to return using the top_k parameter in CosineStrategy. Higher values return more diverse content clusters.",
    "codeLanguage": "python",
    "codeTokens": 66,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_4",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "# Get top 5 most relevant content clusters\nstrategy = CosineStrategy(top_k=5)"
      }
    ],
    "relevance": 0.868
  },
  {
    "codeTitle": "Exporting SSL Certificate in PEM Format in Python",
    "codeDescription": "Method to export a certificate in PEM format (commonly used for web servers), returning a string or saving to a file.",
    "codeLanguage": "python",
    "codeTokens": 67,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-11_snippet_6",
    "pageTitle": "SSLCertificate Class Reference Guide",
    "codeList": [
      {
        "language": "python",
        "code": "pem_str = cert.to_pem()              # in-memory PEM string\ncert.to_pem(\"/path/to/cert.pem\")     # saved to file"
      }
    ],
    "relevance": 0.865
  },
  {
    "codeTitle": "Structured Data Extraction with CSS Selectors",
    "codeDescription": "Command example showing how to extract structured data from a webpage using CSS selectors with a configuration file and schema definition.",
    "codeLanguage": "bash",
    "codeTokens": 59,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_11",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crwl https://example.com \\\n    -e extract_css.yml \\\n    -s css_schema.json \\\n    -o json"
      }
    ],
    "relevance": 0.865
  },
  {
    "codeTitle": "LLM-Based Extraction Configuration",
    "codeDescription": "YAML configuration for using a language model (LLM) for content extraction, specifying the provider, instruction, and API token.",
    "codeLanguage": "yaml",
    "codeTokens": 95,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_8",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "yaml",
        "code": "# extract_llm.yml\ntype: \"llm\"\nprovider: \"openai/gpt-4\"\ninstruction: \"Extract all articles with their titles and links\"\napi_token: \"your-token\"\nparams:\n  temperature: 0.3\n  max_tokens: 1000"
      }
    ],
    "relevance": 0.865
  },
  {
    "codeTitle": "PDF Processing Implementation",
    "codeDescription": "Demonstrates how to use PDF processing capabilities for extracting text, images, and metadata from PDF files using PDFCrawlerStrategy and PDFContentScrapingStrategy.",
    "codeLanguage": "python",
    "codeTokens": 191,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_10",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.processors.pdf import PDFCrawlerStrategy, PDFContentScrapingStrategy\nimport asyncio\n\nasync def main():\n    async with AsyncWebCrawler(crawler_strategy=PDFCrawlerStrategy()) as crawler:\n        result = await crawler.arun(\n            \"https://arxiv.org/pdf/2310.06825.pdf\",\n            config=CrawlerRunConfig(\n                scraping_strategy=PDFContentScrapingStrategy()\n            )\n        )\n        print(result.markdown)  # Access extracted text\n        print(result.metadata)  # Access PDF metadata (title, author, etc.)\n\nasyncio.run(main())"
      }
    ],
    "relevance": 0.865
  },
  {
    "codeTitle": "Deprecated Caching Method in Crawl4AI (Python)",
    "codeDescription": "This code snippet demonstrates the old way of controlling caching in Crawl4AI using boolean flags. It uses the 'bypass_cache' parameter to skip the cache entirely when crawling a web page.",
    "codeLanguage": "python",
    "codeTokens": 144,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cache-modes.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI Cache System Migration Guide",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def use_proxy():\n    async with AsyncWebCrawler(verbose=True) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True  # Old way\n        )\n        print(len(result.markdown))\n\nasync def main():\n    await use_proxy()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.865
  },
  {
    "codeTitle": "Implementing Web Embedding Index",
    "codeDescription": "Implementation of a semantic search infrastructure for crawled content using vector embeddings, including automatic embedding generation and efficient storage.",
    "codeLanguage": "python",
    "codeTokens": 230,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-11_snippet_6",
    "pageTitle": "Crawl4AI Project Strategic Roadmap and Implementation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.indexing import WebIndex\n\n# Initialize and build index\nindex = WebIndex(model=\"efficient-mini\")\n\nasync with AsyncWebCrawler() as crawler:\n    # Crawl and index content\n    await index.build(\n        urls=[\"https://docs.example.com\"],\n        crawler=crawler,\n        options={\n            \"chunk_method\": \"semantic\",\n            \"update_policy\": \"incremental\",\n            \"embedding_batch_size\": 100\n        }\n    )\n\n    # Search through indexed content\n    results = await index.search(\n        query=\"How to implement OAuth authentication?\",\n        filters={\n            \"content_type\": \"technical\",\n            \"recency\": \"6months\"\n        },\n        top_k=5\n    )\n\n    # Get similar content\n    similar = await index.find_similar(\n        url=\"https://docs.example.com/auth/oauth\",\n        threshold=0.85\n    )"
      }
    ],
    "relevance": 0.865
  },
  {
    "codeTitle": "Managing Browser Profiles with BrowserProfiler",
    "codeDescription": "Demonstrates the use of BrowserProfiler class for creating and managing browser profiles",
    "codeLanguage": "python",
    "codeTokens": 240,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-11_snippet_4",
    "pageTitle": "Setting up Browser Profiles with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import BrowserProfiler\n\nasync def manage_profiles():\n    # Create a profiler instance\n    profiler = BrowserProfiler()\n    \n    # Create a profile interactively - opens a browser window\n    profile_path = await profiler.create_profile(\n        profile_name=\"my-login-profile\"  # Optional: name your profile\n    )\n    \n    print(f\"Profile saved at: {profile_path}\")\n    \n    # List all available profiles\n    profiles = profiler.list_profiles()\n    \n    for profile in profiles:\n        print(f\"Profile: {profile['name']}\")\n        print(f\"  Path: {profile['path']}\")\n        print(f\"  Created: {profile['created']}\")\n        print(f\"  Browser type: {profile['type']}\")\n    \n    # Get a specific profile path by name\n    specific_profile = profiler.get_profile_path(\"my-login-profile\")\n    \n    # Delete a profile when no longer needed\n    success = profiler.delete_profile(\"old-profile-name\")\n    \nasyncio.run(manage_profiles())"
      }
    ],
    "relevance": 0.865
  },
  {
    "codeTitle": "Configuring Cache Mode in CrawlerRunConfig for Python",
    "codeDescription": "Shows how to set the cache_mode parameter in CrawlerRunConfig using the CacheMode enum. This determines how the crawler interacts with the local cache for reading and writing crawled data.",
    "codeLanguage": "python",
    "codeTokens": 72,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI Parameter Guide for arun() Method",
    "codeList": [
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    cache_mode=CacheMode.BYPASS\n)"
      }
    ],
    "relevance": 0.865
  },
  {
    "codeTitle": "Crawling Local Files and Raw HTML with AsyncWebCrawler in Python",
    "codeDescription": "Illustrates how to use AsyncWebCrawler to process local files and raw HTML content directly, using the 'file://' and 'raw:' prefixes.",
    "codeLanguage": "python",
    "codeTokens": 213,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_9",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "async def crawl_local_or_raw(crawler, content, content_type):\n    prefix = \"file://\" if content_type == \"local\" else \"raw:\"\n    url = f\"{prefix}{content}\"\n    result = await crawler.arun(url=url)\n    if result.success:\n        print(f\"Markdown Content from {content_type.title()} Source:\")\n        print(result.markdown)\n\n# Example usage with local file and raw HTML\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        # Local File\n        await crawl_local_or_raw(\n            crawler, os.path.abspath('tests/async/sample_wikipedia.html'), \"local\"\n        )\n        # Raw HTML\n        await crawl_raw_html(crawler, \"<h1>Raw Test</h1><p>This is raw HTML.</p>\")\n        \n\nasyncio.run(main())"
      }
    ],
    "relevance": 0.862
  },
  {
    "codeTitle": "LLM Schema Definition for Structured Extraction",
    "codeDescription": "JSON schema definition for LLM-based extraction, defining the object structure for extracting article titles and links.",
    "codeLanguage": "json",
    "codeTokens": 111,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_9",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "json",
        "code": "// llm_schema.json\n{\n  \"title\": \"Article\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"title\": {\n      \"type\": \"string\",\n      \"description\": \"The title of the article\"\n    },\n    \"link\": {\n      \"type\": \"string\",\n      \"description\": \"URL to the full article\"\n    }\n  }\n}"
      }
    ],
    "relevance": 0.862
  },
  {
    "codeTitle": "Exporting SSL Certificate as JSON in Python",
    "codeDescription": "Method to export a certificate as JSON data, either returning a string or saving to a file depending on whether a filepath is provided.",
    "codeLanguage": "python",
    "codeTokens": 64,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-11_snippet_5",
    "pageTitle": "SSLCertificate Class Reference Guide",
    "codeList": [
      {
        "language": "python",
        "code": "json_data = cert.to_json()  # returns JSON string\ncert.to_json(\"certificate.json\")  # writes file, returns None"
      }
    ],
    "relevance": 0.86
  },
  {
    "codeTitle": "Configuring Extra Parameters for Crawl4AI in Python",
    "codeDescription": "This code snippet shows how to set additional parameters for the Crawl4AI crawler's 'arun' function. It includes options for content processing and caching control.",
    "codeLanguage": "python",
    "codeTokens": 125,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_11",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "request = {\n    \"urls\": \"https://example.com\",\n    \"extra\": {\n        \"word_count_threshold\": 10,          # Min words per block\n        \"only_text\": True,                   # Extract only text\n        \"bypass_cache\": True,                # Force fresh crawl\n        \"process_iframes\": True,             # Include iframe content\n    }\n}"
      }
    ],
    "relevance": 0.86
  },
  {
    "codeTitle": "Accessing and Processing Table Data in Crawl4AI",
    "codeDescription": "This example shows how to access and process table data extracted by Crawl4AI. It demonstrates printing table information including caption, headers, and the first few rows of data.",
    "codeLanguage": "python",
    "codeTokens": 174,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#2025-04-11_snippet_3",
    "pageTitle": "Link and Media Extraction in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "if result.success:\n    tables = result.media.get(\"tables\", [])\n    print(f\"Found {len(tables)} data tables on the page\")\n    \n    if tables:\n        # Access the first table\n        first_table = tables[0]\n        print(f\"Table caption: {first_table.get('caption', 'No caption')}\")\n        print(f\"Headers: {first_table.get('headers', [])}\")\n        \n        # Print the first 3 rows\n        for i, row in enumerate(first_table.get('rows', [])[:3]):\n            print(f\"Row {i+1}: {row}\")"
      }
    ],
    "relevance": 0.86
  },
  {
    "codeTitle": "Using Multi-Browser Support in Crawl4AI with Python",
    "codeDescription": "This snippet showcases the multi-browser support feature of Crawl4AI. It demonstrates how to use Firefox instead of the default Chromium browser for web crawling.",
    "codeLanguage": "python",
    "codeTokens": 122,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI v0.3.72 Release Announcement and Feature Demonstration",
    "codeList": [
      {
        "language": "python",
        "code": "async def multi_browser_demo():\n    async with AsyncWebCrawler(browser_type=\"firefox\") as crawler:  # Using Firefox instead of default Chromium\n        result = await crawler.arun(url=\"https://crawl4i.com\")\n        print(result.markdown)  # Shows content extracted using Firefox\n\n# Run the demo\nawait multi_browser_demo()"
      }
    ],
    "relevance": 0.86
  },
  {
    "codeTitle": "Installing Full Crawl4AI Package",
    "codeDescription": "Installation command for Crawl4AI with all available features and dependencies.",
    "codeLanguage": "bash",
    "codeTokens": 34,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI Installation Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai[all]"
      }
    ],
    "relevance": 0.86
  },
  {
    "codeTitle": "Managing Builtin Browser via CLI in Crawl4AI",
    "codeDescription": "This snippet shows various CLI commands for managing the builtin browser in Crawl4AI. It includes commands for starting, checking status, opening a visible window, stopping, and restarting the browser with different settings.",
    "codeLanguage": "bash",
    "codeTokens": 127,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-11_snippet_1",
    "pageTitle": "Builtin Browser in Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "# Start the builtin browser\ncrwl browser start\n\n# Check its status\ncrwl browser status\n\n# Open a visible window to see what the browser is doing\ncrwl browser view --url https://example.com\n\n# Stop it when no longer needed\ncrwl browser stop\n\n# Restart with different settings\ncrwl browser restart --no-headless"
      }
    ],
    "relevance": 0.86
  },
  {
    "codeTitle": "Streaming Mode Example with arun_many in Python",
    "codeDescription": "An example showing how to use arun_many in streaming mode to process results as they become available. This is ideal for processing large numbers of URLs without waiting for all to complete.",
    "codeLanguage": "python",
    "codeTokens": 147,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-11_snippet_2",
    "pageTitle": "arun_many Function Reference",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    stream=True,  # Enable streaming mode\n    cache_mode=CacheMode.BYPASS\n)\n\n# Process results as they complete\nasync for result in await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\", \"https://site3.com\"],\n    config=config\n):\n    if result.success:\n        print(f\"Just completed: {result.url}\")\n        # Process each result immediately\n        process_result(result)"
      }
    ],
    "relevance": 0.86
  },
  {
    "codeTitle": "CSS Schema Definition for Structured Extraction",
    "codeDescription": "JSON schema definition for CSS-based extraction, specifying the selectors and field types for extracting article titles and links.",
    "codeLanguage": "json",
    "codeTokens": 124,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_7",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "json",
        "code": "// css_schema.json\n{\n  \"name\": \"ArticleExtractor\",\n  \"baseSelector\": \".article\",\n  \"fields\": [\n    {\n      \"name\": \"title\",\n      \"selector\": \"h1.title\",\n      \"type\": \"text\"\n    },\n    {\n      \"name\": \"link\",\n      \"selector\": \"a.read-more\",\n      \"type\": \"attribute\",\n      \"attribute\": \"href\"\n    }\n  ]\n}"
      }
    ],
    "relevance": 0.858
  },
  {
    "codeTitle": "Implementing Agentic Crawler",
    "codeDescription": "Example of an autonomous crawling system capable of understanding complex goals and executing multi-step crawling operations with automatic planning and error recovery.",
    "codeLanguage": "python",
    "codeTokens": 247,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI Project Strategic Roadmap and Implementation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.agents import CrawlerAgent\n\nasync with AsyncWebCrawler() as crawler:\n    agent = CrawlerAgent(crawler)\n    \n    # Automatic planning and execution\n    result = await agent.arun(\n        goal=\"Find research papers about quantum computing published in 2023 with more than 50 citations\",\n        auto_retry=True\n    )\n    print(\"Generated Plan:\", result.executed_steps)\n    print(\"Extracted Data:\", result.data)\n    \n    # Using custom steps with automatic execution\n    result = await agent.arun(\n        goal=\"Extract conference deadlines from ML conferences\",\n        custom_plan=[\n            \"Navigate to conference page\",\n            \"Find important dates section\",\n            \"Extract submission deadlines\",\n            \"Verify dates are for 2024\"\n        ]\n    )\n    \n    # Monitoring execution\n    print(\"Step Completion:\", result.step_status)\n    print(\"Execution Time:\", result.execution_time)\n    print(\"Success Rate:\", result.success_rate)"
      }
    ],
    "relevance": 0.855
  },
  {
    "codeTitle": "Loading SSL Certificate from File in Python",
    "codeDescription": "Method to load a certificate from a local file containing certificate data in ASN.1 or DER format, useful when working with locally stored certificates.",
    "codeLanguage": "python",
    "codeTokens": 52,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-11_snippet_3",
    "pageTitle": "SSLCertificate Class Reference Guide",
    "codeList": [
      {
        "language": "python",
        "code": "cert = SSLCertificate.from_file(\"/path/to/cert.der\")"
      }
    ],
    "relevance": 0.855
  },
  {
    "codeTitle": "Product Review Analysis Use Case in Python",
    "codeDescription": "Shows how to configure CosineStrategy for extracting customer reviews and ratings from product pages. Uses a lower word count threshold suitable for shorter review content and retrieves multiple review clusters.",
    "codeLanguage": "python",
    "codeTokens": 106,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-11_snippet_6",
    "pageTitle": "Cosine Strategy in Crawl4AI Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "strategy = CosineStrategy(\n    semantic_filter=\"customer reviews and ratings\",\n    word_count_threshold=20,   # Reviews can be shorter\n    top_k=10,                 # Get multiple reviews\n    sim_threshold=0.4         # Allow variety in review content\n)"
      }
    ],
    "relevance": 0.855
  },
  {
    "codeTitle": "Complete SSL Certificate Handling Example with Crawl4AI in Python",
    "codeDescription": "A comprehensive example demonstrating how to configure Crawl4AI to fetch SSL certificates, access certificate properties, and export the certificate in multiple formats. Shows the entire workflow from crawling to certificate processing.",
    "codeLanguage": "python",
    "codeTokens": 270,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-11_snippet_8",
    "pageTitle": "SSLCertificate Class Reference Guide",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nimport os\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n\nasync def main():\n    tmp_dir = \"tmp\"\n    os.makedirs(tmp_dir, exist_ok=True)\n\n    config = CrawlerRunConfig(\n        fetch_ssl_certificate=True,\n        cache_mode=CacheMode.BYPASS\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com\", config=config)\n        if result.success and result.ssl_certificate:\n            cert = result.ssl_certificate\n            # 1. Basic Info\n            print(\"Issuer CN:\", cert.issuer.get(\"CN\", \"\"))\n            print(\"Valid until:\", cert.valid_until)\n            print(\"Fingerprint:\", cert.fingerprint)\n            \n            # 2. Export\n            cert.to_json(os.path.join(tmp_dir, \"certificate.json\"))\n            cert.to_pem(os.path.join(tmp_dir, \"certificate.pem\"))\n            cert.to_der(os.path.join(tmp_dir, \"certificate.der\"))\n    \nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.855
  },
  {
    "codeTitle": "Installing Crawl4AI Versions using pip in Bash",
    "codeDescription": "These commands demonstrate how to install different versions of Crawl4AI using pip. It includes options for installing the stable version, pre-release versions, and specific versions.",
    "codeLanguage": "bash",
    "codeTokens": 102,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_12",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "bash",
        "code": "# Regular installation (stable version):\npip install -U crawl4ai\n\n# Install pre-release versions:\npip install crawl4ai --pre\n\n# Install specific version:\npip install crawl4ai==0.4.3b1"
      }
    ],
    "relevance": 0.855
  },
  {
    "codeTitle": "Troubleshooting Builtin Browser in Crawl4AI via CLI",
    "codeDescription": "This snippet provides CLI commands for troubleshooting issues with the builtin browser in Crawl4AI. It includes commands to check the browser status, restart it, and stop it to allow Crawl4AI to start a fresh instance.",
    "codeLanguage": "bash",
    "codeTokens": 102,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-11_snippet_4",
    "pageTitle": "Builtin Browser in Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "# Check the browser status:\ncrwl browser status\n\n# Try restarting it:\ncrwl browser restart\n\n# If problems persist, stop it and let Crawl4AI start a fresh one:\ncrwl browser stop"
      }
    ],
    "relevance": 0.855
  },
  {
    "codeTitle": "Timing Control Parameters in CrawlerRunConfig",
    "codeDescription": "Demonstrates how to control timing aspects of the crawling process, including page timeout and delay before returning HTML, which are useful for ensuring complete page loading.",
    "codeLanguage": "python",
    "codeTokens": 76,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-11_snippet_5",
    "pageTitle": "Dynamic Page Interaction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    page_timeout=60000,  # 60s limit\n    delay_before_return_html=2.5\n)"
      }
    ],
    "relevance": 0.855
  },
  {
    "codeTitle": "Pruning Filter Configuration Pattern",
    "codeDescription": "A concise pattern for configuring a PruningContentFilter with specific threshold settings and integrating it into the crawler configuration. Shows the basic setup necessary for content pruning.",
    "codeLanguage": "python",
    "codeTokens": 101,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-11_snippet_2",
    "pageTitle": "Fit Markdown with Pruning & BM25 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "prune_filter = PruningContentFilter(\n    threshold=0.5,\n    threshold_type=\"fixed\",\n    min_word_threshold=10\n)\nmd_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\nconfig = CrawlerRunConfig(markdown_generator=md_generator)"
      }
    ],
    "relevance": 0.85
  },
  {
    "codeTitle": "Python Package Dependencies List",
    "codeDescription": "Comprehensive list of Python packages required for the crawl4ai project, including web scraping tools (playwright, beautifulsoup4), data processing libraries (numpy, pillow), and utility packages. Version constraints are specified using pip's version specifier syntax (~=, >=, <).",
    "codeLanguage": "plaintext",
    "codeTokens": 256,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/requirements.txt#2025-04-11_snippet_0",
    "pageTitle": "Python Dependencies Configuration",
    "codeList": [
      {
        "language": "plaintext",
        "code": "aiosqlite~=0.20\nlxml~=5.3\nlitellm>=1.53.1\nnumpy>=1.26.0,<3\npillow~=10.4\nplaywright>=1.49.0\npython-dotenv~=1.0\nrequests~=2.26\nbeautifulsoup4~=4.12\ntf-playwright-stealth>=1.1.0\nxxhash~=3.4\nrank-bm25~=0.2\naiofiles>=24.1.0\ncolorama~=0.4\nsnowballstemmer~=2.2\npydantic>=2.10\npyOpenSSL>=24.3.0\npsutil>=6.1.1\nnltk>=3.9.1\nrich>=13.9.4\ncssselect>=1.2.0\nfaust-cchardet>=2.1.19"
      }
    ],
    "relevance": 0.85
  },
  {
    "codeTitle": "Processing Images from CrawlResult.media in Python",
    "codeDescription": "Example showing how to access and iterate through image data from the media dictionary in a crawl result. This demonstrates retrieving image URLs and alt text attributes.",
    "codeLanguage": "python",
    "codeTokens": 79,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-11_snippet_5",
    "pageTitle": "CrawlResult Object and Output Structure in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "images = result.media.get(\"images\", [])\nfor img in images:\n    print(\"Image URL:\", img[\"src\"], \"Alt:\", img.get(\"alt\"))"
      }
    ],
    "relevance": 0.85
  },
  {
    "codeTitle": "Implementing Magic Mode Crawling",
    "codeDescription": "Example showing how to use Magic Mode for simplified automation without persistent profiles",
    "codeLanguage": "python",
    "codeTokens": 106,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-11_snippet_3",
    "pageTitle": "Setting up Browser Profiles with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://example.com\",\n        config=CrawlerRunConfig(\n            magic=True,  # Simplifies a lot of interaction\n            remove_overlay_elements=True,\n            page_timeout=60000\n        )\n    )"
      }
    ],
    "relevance": 0.85
  },
  {
    "codeTitle": "Setting Up Asyncio with Nest Asyncio",
    "codeDescription": "Imports and applies nest_asyncio to make asyncio work properly in notebook environments by patching the event loop.",
    "codeLanguage": "python",
    "codeTokens": 47,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nimport nest_asyncio\nnest_asyncio.apply()"
      }
    ],
    "relevance": 0.85
  },
  {
    "codeTitle": "Loading SSL Certificate from URL in Python",
    "codeDescription": "Method to manually fetch an SSL certificate from a specified URL and access its properties. This is used internally by Crawl4AI but can also be called directly.",
    "codeLanguage": "python",
    "codeTokens": 68,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-11_snippet_2",
    "pageTitle": "SSLCertificate Class Reference Guide",
    "codeList": [
      {
        "language": "python",
        "code": "cert = SSLCertificate.from_url(\"https://example.com\")\nif cert:\n    print(\"Fingerprint:\", cert.fingerprint)"
      }
    ],
    "relevance": 0.85
  },
  {
    "codeTitle": "Configuring Proxy Settings in Crawl4AI (Python)",
    "codeDescription": "Demonstrates how to route crawl traffic through a proxy server with optional authentication credentials using BrowserConfig.proxy_config.",
    "codeLanguage": "python",
    "codeTokens": 219,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-11_snippet_0",
    "pageTitle": "Advanced Features in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    browser_cfg = BrowserConfig(\n        proxy_config={\n            \"server\": \"http://proxy.example.com:8080\",\n            \"username\": \"myuser\",\n            \"password\": \"mypass\",\n        },\n        headless=True\n    )\n    crawler_cfg = CrawlerRunConfig(\n        verbose=True\n    )\n\n    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n        result = await crawler.arun(\n            url=\"https://www.whatismyip.com/\",\n            config=crawler_cfg\n        )\n        if result.success:\n            print(\"[OK] Page fetched via proxy.\")\n            print(\"Page HTML snippet:\", result.html[:200])\n        else:\n            print(\"[ERROR]\", result.error_message)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.85
  },
  {
    "codeTitle": "Implementing PruningContentFilter in Python",
    "codeDescription": "Adds a new content filtering strategy that removes less relevant nodes based on metrics like text and link density.",
    "codeLanguage": "diff",
    "codeTokens": 42,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_2",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "diff",
        "code": "Implemented effective pruning algorithm with comprehensive scoring."
      }
    ],
    "relevance": 0.845
  },
  {
    "codeTitle": "Using DefaultMarkdownGenerator with Options in Python",
    "codeDescription": "Example of configuring the Markdown generator with specific options including citations and body width. The code shows how to access different parts of the markdown result after crawling a page.",
    "codeLanguage": "python",
    "codeTokens": 179,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-11_snippet_2",
    "pageTitle": "CrawlResult Object and Output Structure in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nconfig = CrawlerRunConfig(\n    markdown_generator=DefaultMarkdownGenerator(\n        options={\"citations\": True, \"body_width\": 80}  # e.g. pass html2text style options\n    )\n)\nresult = await crawler.arun(url=\"https://example.com\", config=config)\n\nmd_res = result.markdown  # or eventually 'result.markdown'\nprint(md_res.raw_markdown[:500])\nprint(md_res.markdown_with_citations)\nprint(md_res.references_markdown)"
      }
    ],
    "relevance": 0.845
  },
  {
    "codeTitle": "Defining DispatchResult Data Structure",
    "codeDescription": "Defines the data structure for crawl dispatch results using Python dataclass. Includes fields for tracking task execution metrics and errors.",
    "codeLanguage": "python",
    "codeTokens": 88,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_9",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "@dataclass\nclass DispatchResult:\n    task_id: str\n    memory_usage: float\n    peak_memory: float\n    start_time: datetime\n    end_time: datetime\n    error_message: str = \"\""
      }
    ],
    "relevance": 0.845
  },
  {
    "codeTitle": "Basic Docker Commands for Crawl4AI",
    "codeDescription": "Basic Docker commands to pull and run Crawl4AI containers with and without API security.",
    "codeLanguage": "bash",
    "codeTokens": 106,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_0",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "# Basic run without security\ndocker pull unclecode/crawl4ai:basic\ndocker run -p 11235:11235 unclecode/crawl4ai:basic\n\n# Run with API security enabled\ndocker run -p 11235:11235 -e CRAWL4AI_API_TOKEN=your_secret_token unclecode/crawl4ai:basic"
      }
    ],
    "relevance": 0.845
  },
  {
    "codeTitle": "Legacy Profile Listing in Python using ManagedBrowser",
    "codeDescription": "Demonstrates the backward-compatible method to list browser profiles using ManagedBrowser class, which internally delegates to BrowserProfiler.",
    "codeLanguage": "python",
    "codeTokens": 65,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-11_snippet_6",
    "pageTitle": "Setting up Browser Profiles with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.browser_manager import ManagedBrowser\n\n# These methods still work but use BrowserProfiler internally\nprofiles = ManagedBrowser.list_profiles()"
      }
    ],
    "relevance": 0.845
  },
  {
    "codeTitle": "Analyzing and Filtering Links in Web Crawling",
    "codeDescription": "Shows how to analyze links found during crawling, including filtering options to exclude external links, social media links, or specific domains while printing information about the discovered links.",
    "codeLanguage": "python",
    "codeTokens": 187,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "async def link_analysis():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n            bypass_cache=True,\n            exclude_external_links=True,\n            exclude_social_media_links=True,\n            # exclude_domains=[\"facebook.com\", \"twitter.com\"]\n        )\n        print(f\"Found {len(result.links['internal'])} internal links\")\n        print(f\"Found {len(result.links['external'])} external links\")\n\n        for link in result.links['internal'][:5]:\n            print(f\"Href: {link['href']}\\nText: {link['text']}\\n\")\n                \n\nasyncio.run(link_analysis())"
      }
    ],
    "relevance": 0.845
  },
  {
    "codeTitle": "Installing Crawl4AI with PyTorch Support",
    "codeDescription": "Command to install Crawl4AI with PyTorch-based features for text clustering and semantic chunking.",
    "codeLanguage": "bash",
    "codeTokens": 47,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI Installation and Setup Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai[torch]\ncrawl4ai-setup"
      }
    ],
    "relevance": 0.842
  },
  {
    "codeTitle": "Crawler Configuration in YAML Format",
    "codeDescription": "YAML configuration for crawler behavior, including cache settings, page timeout, scroll behavior, and content processing options.",
    "codeLanguage": "yaml",
    "codeTokens": 108,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "yaml",
        "code": "# crawler.yml\ncache_mode: \"bypass\"\nwait_until: \"networkidle\"\npage_timeout: 30000\ndelay_before_return_html: 0.5\nword_count_threshold: 100\nscan_full_page: true\nscroll_delay: 0.3\nprocess_iframes: false\nremove_overlay_elements: true\nmagic: true\nverbose: true"
      }
    ],
    "relevance": 0.84
  },
  {
    "codeTitle": "Configuring HTML Cleanup Settings with CrawlerRunConfig in Python",
    "codeDescription": "Example showing how to use CrawlerRunConfig to specify HTML elements to exclude from the cleaned_html output. This creates a sanitized version of the HTML without forms, headers, footers, and data attributes.",
    "codeLanguage": "python",
    "codeTokens": 118,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-11_snippet_1",
    "pageTitle": "CrawlResult Object and Output Structure in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(\n    excluded_tags=[\"form\", \"header\", \"footer\"],\n    keep_data_attributes=False\n)\nresult = await crawler.arun(\"https://example.com\", config=config)\nprint(result.cleaned_html)  # Freed of forms, header, footer, data-* attributes"
      }
    ],
    "relevance": 0.84
  },
  {
    "codeTitle": "Basic Usage of Crawl4AI CLI",
    "codeDescription": "Examples of basic command-line usage for the Crawl4AI CLI tool, showing how to crawl a website with different output options and cache settings.",
    "codeLanguage": "bash",
    "codeTokens": 101,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "# Basic crawling\ncrwl https://example.com\n\n# Get markdown output\ncrwl https://example.com -o markdown\n\n# Verbose JSON output with cache bypass\ncrwl https://example.com -o json -v --bypass-cache\n\n# See usage examples\ncrwl --example"
      }
    ],
    "relevance": 0.84
  },
  {
    "codeTitle": "Installing Ubuntu Dependencies for Playwright",
    "codeDescription": "Commands to install required system dependencies for Playwright on Ubuntu systems.",
    "codeLanguage": "bash",
    "codeTokens": 321,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-11_snippet_6",
    "pageTitle": "Crawl4AI Installation Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "sudo apt-get install -y \\\n    libwoff1 \\\n    libopus0 \\\n    libwebp7 \\\n    libwebpdemux2 \\\n    libenchant-2-2 \\\n    libgudev-1.0-0 \\\n    libsecret-1-0 \\\n    libhyphen0 \\\n    libgdk-pixbuf2.0-0 \\\n    libegl1 \\\n    libnotify4 \\\n    libxslt1.1 \\\n    libevent-2.1-7 \\\n    libgles2 \\\n    libxcomposite1 \\\n    libatk1.0-0 \\\n    libatk-bridge2.0-0 \\\n    libepoxy0 \\\n    libgtk-3-0 \\\n    libharfbuzz-icu0 \\\n    libgstreamer-gl1.0-0 \\\n    libgstreamer-plugins-bad1.0-0 \\\n    gstreamer1.0-plugins-good \\\n    gstreamer1.0-plugins-bad \\\n    libxt6 \\\n    libxaw7 \\\n    xvfb \\\n    fonts-noto-color-emoji \\\n    libfontconfig \\\n    libfreetype6 \\\n    xfonts-cyrillic \\\n    xfonts-scalable \\\n    fonts-liberation \\\n    fonts-ipafont-gothic \\\n    fonts-wqy-zenhei \\\n    fonts-tlwg-loma-otf \\\n    fonts-freefont-ttf"
      }
    ],
    "relevance": 0.84
  },
  {
    "codeTitle": "Running Crawl4AI Docker Container",
    "codeDescription": "Commands to pull and run the experimental Docker container for Crawl4AI.",
    "codeLanguage": "bash",
    "codeTokens": 58,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-11_snippet_8",
    "pageTitle": "Crawl4AI Installation and Setup Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "docker pull unclecode/crawl4ai:basic\ndocker run -p 11235:11235 unclecode/crawl4ai:basic"
      }
    ],
    "relevance": 0.84
  },
  {
    "codeTitle": "Setting Session Management in CrawlerRunConfig for Python",
    "codeDescription": "Demonstrates how to use the session_id parameter in CrawlerRunConfig to maintain session continuity across multiple arun() calls. This is useful for multi-step tasks or stateful browsing.",
    "codeLanguage": "python",
    "codeTokens": 71,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI Parameter Guide for arun() Method",
    "codeList": [
      {
        "language": "python",
        "code": "run_config = CrawlerRunConfig(\n    session_id=\"my_session123\"\n)"
      }
    ],
    "relevance": 0.84
  },
  {
    "codeTitle": "Downloading Crawl4AI Models",
    "codeDescription": "Command to pre-fetch and cache large models locally for offline use.",
    "codeLanguage": "bash",
    "codeTokens": 33,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-11_snippet_7",
    "pageTitle": "Crawl4AI Installation and Setup Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crawl4ai-download-models"
      }
    ],
    "relevance": 0.838
  },
  {
    "codeTitle": "Combining Crawl4AI Features for Advanced Web Scraping and Knowledge Graph Extraction in Python",
    "codeDescription": "This comprehensive snippet combines multiple Crawl4AI features including Fit Markdown, Magic Mode, multi-browser support, and knowledge graph extraction. It defines a schema for travel information, uses LLMExtractionStrategy for content processing, and demonstrates how to extract structured data from a travel blog.",
    "codeLanguage": "python",
    "codeTokens": 530,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI v0.3.72 Release Announcement and Feature Demonstration",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel\nimport json, os\nfrom typing import List\n\n# Define classes for the knowledge graph structure\nclass Landmark(BaseModel):\n    name: str\n    description: str\n    activities: list[str]  # E.g., visiting, sightseeing, relaxing\n\nclass City(BaseModel):\n    name: str\n    description: str\n    landmarks: list[Landmark]\n    cultural_highlights: list[str]  # E.g., food, music, traditional crafts\n\nclass TravelKnowledgeGraph(BaseModel):\n    cities: list[City]  # Central Mexican cities to visit\n\nasync def combined_demo():\n    # Define the knowledge graph extraction strategy\n    strategy = LLMExtractionStrategy(\n        # provider=\"ollama/nemotron\",\n        provider='openai/gpt-4o-mini', # Or any other provider, including Ollama and open source models\n        pi_token=os.getenv('OPENAI_API_KEY'), # In case of Ollama just pass \"no-token\"\n        schema=TravelKnowledgeGraph.schema(),\n        instruction=(\n            \"Extract cities, landmarks, and cultural highlights for places to visit in Central Mexico. \"\n            \"For each city, list main landmarks with descriptions and activities, as well as cultural highlights.\"\n        )\n    )\n\n    # Set up the AsyncWebCrawler with multi-browser support, Magic Mode, and Fit Markdown\n    async with AsyncWebCrawler(browser_type=\"firefox\") as crawler:\n        result = await crawler.arun(\n            url=\"https://janineintheworld.com/places-to-visit-in-central-mexico\",\n            extraction_strategy=strategy,\n            bypass_cache=True,\n            magic=True\n        )\n        \n        # Display main article content in Fit Markdown format\n        print(\"Extracted Main Content:\\n\", result.fit_markdown)\n        \n        # Display extracted knowledge graph of cities, landmarks, and cultural highlights\n        if result.extracted_content:\n            travel_graph = json.loads(result.extracted_content)\n            print(\"\\nExtracted Knowledge Graph:\\n\", json.dumps(travel_graph, indent=2))\n\n# Run the combined demo\nawait combined_demo()"
      }
    ],
    "relevance": 0.835
  },
  {
    "codeTitle": "Customizing DefaultMarkdownGenerator with Options in Crawl4AI",
    "codeDescription": "Configures a DefaultMarkdownGenerator with specific options to customize markdown output. This example demonstrates ignoring links, disabling HTML escaping, and setting text wrapping width.",
    "codeLanguage": "python",
    "codeTokens": 233,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_1",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n\nasync def main():\n    # Example: ignore all links, don't escape HTML, and wrap text at 80 characters\n    md_generator = DefaultMarkdownGenerator(\n        options={\n            \"ignore_links\": True,\n            \"escape_html\": False,\n            \"body_width\": 80\n        }\n    )\n\n    config = CrawlerRunConfig(\n        markdown_generator=md_generator\n    )\n\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\"https://example.com/docs\", config=config)\n        if result.success:\n            print(\"Markdown:\\n\", result.markdown[:500])  # Just a snippet\n        else:\n            print(\"Crawl failed:\", result.error_message)\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.835
  },
  {
    "codeTitle": "Content Filtering Configuration",
    "codeDescription": "YAML configurations for BM25 and pruning-based content filtering, specifying query terms and relevance thresholds.",
    "codeLanguage": "yaml",
    "codeTokens": 79,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_13",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "yaml",
        "code": "# filter_bm25.yml\ntype: \"bm25\"\nquery: \"target content\"\nthreshold: 1.0\n\n# filter_pruning.yml\ntype: \"pruning\"\nquery: \"focus topic\"\nthreshold: 0.48"
      }
    ],
    "relevance": 0.835
  },
  {
    "codeTitle": "Docker Compose Configuration",
    "codeDescription": "YAML configuration for Docker Compose deployment with environment variables and resource limits.",
    "codeLanguage": "yaml",
    "codeTokens": 72,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_5",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "yaml",
        "code": "services:\n  crawl4ai:\n    image: unclecode/crawl4ai:all\n    environment:\n      - CRAWL4AI_API_TOKEN=${CRAWL4AI_API_TOKEN:-}  # Optional"
      }
    ],
    "relevance": 0.835
  },
  {
    "codeTitle": "Running Builtin Browser Example in Python with Crawl4AI",
    "codeDescription": "This snippet shows the command to run a complete example of using the builtin browser in Crawl4AI. It references an example file and provides the command to execute it.",
    "codeLanguage": "bash",
    "codeTokens": 53,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-11_snippet_3",
    "pageTitle": "Builtin Browser in Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "python builtin_browser_example.py"
      }
    ],
    "relevance": 0.835
  },
  {
    "codeTitle": "Basic LLMExtractionStrategy Configuration Example in Crawl4AI",
    "codeDescription": "Example showing the essential parameters for configuring the LLMExtractionStrategy. This includes provider selection, schema definition, chunking settings, and input format options for processing webpage content with an LLM.",
    "codeLanguage": "python",
    "codeTokens": 177,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-11_snippet_1",
    "pageTitle": "LLM-Based JSON Extraction with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "extraction_strategy = LLMExtractionStrategy(\n    llm_config = LLMConfig(provider=\"openai/gpt-4\", api_token=\"YOUR_OPENAI_KEY\"),\n    schema=MyModel.model_json_schema(),\n    extraction_type=\"schema\",\n    instruction=\"Extract a list of items from the text with 'name' and 'price' fields.\",\n    chunk_token_threshold=1200,\n    overlap_rate=0.1,\n    apply_chunking=True,\n    input_format=\"html\",\n    extra_args={\"temperature\": 0.1, \"max_tokens\": 1000},\n    verbose=True\n)"
      }
    ],
    "relevance": 0.835
  },
  {
    "codeTitle": "Using Crawl4AI Command-Line Interface (CLI)",
    "codeDescription": "This set of bash commands showcases various ways to use the new Crawl4AI CLI (crwl). It includes examples of basic crawling, output formatting, configuration file usage, LLM-based extraction, and asking questions about crawled content.",
    "codeLanguage": "bash",
    "codeTokens": 167,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "bash",
        "code": "# Basic crawl\ncrwl https://example.com\n\n# Get markdown output\ncrwl https://example.com -o markdown\n\n# Use a configuration file\ncrwl https://example.com -B browser.yml -C crawler.yml\n\n# Use LLM-based extraction\ncrwl https://example.com -e extract.yml -s schema.json\n\n# Ask a question about the crawled content\ncrwl https://example.com -q \"What is the main topic?\"\n\n# See usage examples\ncrwl --example"
      }
    ],
    "relevance": 0.83
  },
  {
    "codeTitle": "Modifying Configuration Settings in Python",
    "codeDescription": "Demonstrates how to create a modified copy of existing configuration using the clone() method. Updates stream settings and cache mode while preserving other settings.",
    "codeLanguage": "python",
    "codeTokens": 66,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-11_snippet_6",
    "pageTitle": "Browser and Crawler Configuration in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "stream_cfg = run_cfg.clone(\n    stream=True,\n    cache_mode=CacheMode.BYPASS\n)"
      }
    ],
    "relevance": 0.83
  },
  {
    "codeTitle": "Browser Configuration in YAML Format",
    "codeDescription": "Configuration for browser settings in YAML format, controlling parameters like headless mode, viewport size, and user agent settings.",
    "codeLanguage": "yaml",
    "codeTokens": 67,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "yaml",
        "code": "# browser.yml\nheadless: true\nviewport_width: 1280\nuser_agent_mode: \"random\"\nverbose: true\nignore_https_errors: true"
      }
    ],
    "relevance": 0.83
  },
  {
    "codeTitle": "Docker Container Management Commands",
    "codeDescription": "Commands for stopping and managing Docker containers and networks.",
    "codeLanguage": "bash",
    "codeTokens": 72,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_3",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "docker ps\ndocker stop <CONTAINER_ID>\ndocker-compose -f docker-compose.local.yml logs -f\ndocker-compose -f docker-compose.local.yml down --remove-orphans\ndocker network ls\ndocker network rm crawl4ai_default"
      }
    ],
    "relevance": 0.83
  },
  {
    "codeTitle": "Loading SSL Certificate from Binary Data in Python",
    "codeDescription": "Method to create an SSLCertificate object from raw binary data, useful when certificate data is captured from network sockets or other binary sources.",
    "codeLanguage": "python",
    "codeTokens": 46,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-11_snippet_4",
    "pageTitle": "SSLCertificate Class Reference Guide",
    "codeList": [
      {
        "language": "python",
        "code": "cert = SSLCertificate.from_binary(raw_bytes)"
      }
    ],
    "relevance": 0.83
  },
  {
    "codeTitle": "Crawling a Local HTML File with Crawl4AI in Python",
    "codeDescription": "This code snippet shows how to use AsyncWebCrawler to crawl a local HTML file by prefixing the file path with 'file://'. It uses CrawlerRunConfig for configuration and prints the resulting markdown content.",
    "codeLanguage": "python",
    "codeTokens": 204,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-11_snippet_1",
    "pageTitle": "Prefix-Based Input Handling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\nfrom crawl4ai.async_configs import CrawlerRunConfig\n\nasync def crawl_local_file():\n    local_file_path = \"/path/to/apple.html\"  # Replace with your file path\n    file_url = f\"file://{local_file_path}\"\n    config = CrawlerRunConfig(bypass_cache=True)\n    \n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(url=file_url, config=config)\n        if result.success:\n            print(\"Markdown Content from Local File:\")\n            print(result.markdown)\n        else:\n            print(f\"Failed to crawl local file: {result.error_message}\")\n\nasyncio.run(crawl_local_file())"
      }
    ],
    "relevance": 0.83
  },
  {
    "codeTitle": "Basic Extraction Example",
    "codeDescription": "Complete command example showing basic extraction using browser and crawler configurations with JSON output.",
    "codeLanguage": "bash",
    "codeTokens": 50,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_15",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crwl https://example.com \\\n    -B browser.yml \\\n    -C crawler.yml \\\n    -o json"
      }
    ],
    "relevance": 0.825
  },
  {
    "codeTitle": "Submitting a Crawl Task to Crawl4AI API",
    "codeDescription": "This HTTP request shows how to submit a crawl task to the Crawl4AI API, including the structure of the JSON payload with various configuration options.",
    "codeLanguage": "http",
    "codeTokens": 112,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_19",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "http",
        "code": "POST /crawl\nContent-Type: application/json\n\n{\n    \"urls\": \"string or array\",\n    \"extraction_config\": {\n        \"type\": \"basic|llm|cosine|json_css\",\n        \"params\": {}\n    },\n    \"priority\": 1-10,\n    \"ttl\": 3600\n}"
      }
    ],
    "relevance": 0.825
  },
  {
    "codeTitle": "Running Crawler in Python with Crawl4AI",
    "codeDescription": "Shows how to execute the crawler asynchronously with a URL and configuration.",
    "codeLanguage": "python",
    "codeTokens": 49,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-11_snippet_7",
    "pageTitle": "CrawlResult Object and Output Structure in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "result = await crawler.arun(url=\"https://example.com\", config=some_config)"
      }
    ],
    "relevance": 0.825
  },
  {
    "codeTitle": "Enhancing BM25ContentFilter Tests in Python",
    "codeDescription": "Extends testing to cover additional edge cases and performance metrics for the BM25ContentFilter.",
    "codeLanguage": "diff",
    "codeTokens": 42,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_6",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "diff",
        "code": "Added tests for new extraction scenarios including malformed HTML."
      }
    ],
    "relevance": 0.825
  },
  {
    "codeTitle": "Exporting SSL Certificate in DER Format in Python",
    "codeDescription": "Method to export a certificate in DER (binary ASN.1) format, either returning bytes or saving to a file.",
    "codeLanguage": "python",
    "codeTokens": 49,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-11_snippet_7",
    "pageTitle": "SSLCertificate Class Reference Guide",
    "codeList": [
      {
        "language": "python",
        "code": "der_bytes = cert.to_der()\ncert.to_der(\"certificate.der\")"
      }
    ],
    "relevance": 0.825
  },
  {
    "codeTitle": "Manual Start and Close of AsyncWebCrawler in Python",
    "codeDescription": "Shows how to manually control the lifecycle of AsyncWebCrawler by explicitly calling start() and close() methods. This is useful for long-running applications or when full control over the crawler's lifecycle is needed.",
    "codeLanguage": "python",
    "codeTokens": 98,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-11_snippet_3",
    "pageTitle": "AsyncWebCrawler Class Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "crawler = AsyncWebCrawler(config=browser_cfg)\nawait crawler.start()\n\nresult1 = await crawler.arun(\"https://example.com\")\nresult2 = await crawler.arun(\"https://another.com\")\n\nawait crawler.close()"
      }
    ],
    "relevance": 0.825
  },
  {
    "codeTitle": "Enabling SSL Certificate Fetching in Crawl4AI",
    "codeDescription": "Code snippet showing how to enable SSL certificate fetching in a Crawl4AI configuration, which is required to obtain SSLCertificate objects in crawl results.",
    "codeLanguage": "python",
    "codeTokens": 50,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-11_snippet_1",
    "pageTitle": "SSLCertificate Class Reference Guide",
    "codeList": [
      {
        "language": "python",
        "code": "CrawlerRunConfig(fetch_ssl_certificate=True, ...)"
      }
    ],
    "relevance": 0.82
  },
  {
    "codeTitle": "Structured Data Extraction with LLM",
    "codeDescription": "Command example showing how to extract structured data from a webpage using a language model (LLM) with a configuration file and schema definition.",
    "codeLanguage": "bash",
    "codeTokens": 66,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_12",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crwl https://example.com \\\n    -e extract_llm.yml \\\n    -s llm_schema.json \\\n    -o json"
      }
    ],
    "relevance": 0.82
  },
  {
    "codeTitle": "Saving PDF Output from CrawlResult in Python",
    "codeDescription": "Simple code snippet showing how to save the PDF bytes from a crawl result to a file. This works when pdf=True is set in the CrawlerRunConfig.",
    "codeLanguage": "python",
    "codeTokens": 68,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-11_snippet_6",
    "pageTitle": "CrawlResult Object and Output Structure in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "with open(\"page.pdf\", \"wb\") as f:\n    f.write(result.pdf)"
      }
    ],
    "relevance": 0.82
  },
  {
    "codeTitle": "Python API Client Implementation",
    "codeDescription": "Example Python code for making authenticated API requests to Crawl4AI.",
    "codeLanguage": "python",
    "codeTokens": 165,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_4",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import requests\n\n# Setup headers if token is being used\napi_token = \"your_secret_token\"  # Same token set in CRAWL4AI_API_TOKEN\nheaders = {\"Authorization\": f\"Bearer {api_token}\"} if api_token else {}\n\n# Making authenticated requests\nresponse = requests.post(\n    \"http://localhost:11235/crawl\",\n    headers=headers,\n    json={\n        \"urls\": \"https://example.com\",\n        \"priority\": 10\n    }\n)\n\n# Checking task status\ntask_id = response.json()[\"task_id\"]\nstatus = requests.get(\n    f\"http://localhost:11235/task/{task_id}\",\n    headers=headers\n)"
      }
    ],
    "relevance": 0.82
  },
  {
    "codeTitle": "Initializing and Using Builtin Browser in Python with Crawl4AI",
    "codeDescription": "This snippet demonstrates how to create a browser config with builtin mode, initialize an AsyncWebCrawler, and use it to crawl a website. It highlights the key settings and explains that explicit start() and close() calls are not needed.",
    "codeLanguage": "python",
    "codeTokens": 159,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-11_snippet_0",
    "pageTitle": "Builtin Browser in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\n# Create browser config with builtin mode\nbrowser_config = BrowserConfig(\n    browser_mode=\"builtin\",  # This is the key setting!\n    headless=True            # Can be headless or not\n)\n\n# Create the crawler\ncrawler = AsyncWebCrawler(config=browser_config)\n\n# Use it - no need to explicitly start()\nresult = await crawler.arun(\"https://example.com\")"
      }
    ],
    "relevance": 0.82
  },
  {
    "codeTitle": "CSS-Based Extraction Configuration",
    "codeDescription": "YAML configuration for CSS/XPath-based content extraction, demonstrating the JSON-CSS extraction type with verbosity parameter.",
    "codeLanguage": "yaml",
    "codeTokens": 50,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_6",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "yaml",
        "code": "# extract_css.yml\ntype: \"json-css\"\nparams:\n  verbose: true"
      }
    ],
    "relevance": 0.815
  },
  {
    "codeTitle": "Crawler Configuration via Command Line",
    "codeDescription": "Examples showing how to apply crawler configuration either through a YAML file or directly as command-line parameters.",
    "codeLanguage": "bash",
    "codeTokens": 78,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "# Using config file\ncrwl https://example.com -C crawler.yml\n\n# Using direct parameters\ncrwl https://example.com -c \"css_selector=#main,delay_before_return_html=2,scan_full_page=true\""
      }
    ],
    "relevance": 0.815
  },
  {
    "codeTitle": "Debugging Crawl4AI Container with Bash",
    "codeDescription": "These Bash commands demonstrate how to access a Crawl4AI container for debugging purposes and view container logs.",
    "codeLanguage": "bash",
    "codeTokens": 53,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_17",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "docker run -it --entrypoint /bin/bash unclecode/crawl4ai:all"
      },
      {
        "language": "bash",
        "code": "docker logs [container_id]"
      }
    ],
    "relevance": 0.815
  },
  {
    "codeTitle": "Creating a CrawlerMonitor in Python for Crawl4AI",
    "codeDescription": "Code for initializing a CrawlerMonitor instance to provide real-time visibility into crawling operations. Configures the maximum visible rows and display mode (detailed or aggregated view).",
    "codeLanguage": "python",
    "codeTokens": 111,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-11_snippet_2",
    "pageTitle": "Advanced Multi-URL Crawling with Dispatchers in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import CrawlerMonitor, DisplayMode\nmonitor = CrawlerMonitor(\n    # Maximum rows in live display\n    max_visible_rows=15,          \n\n    # DETAILED or AGGREGATED view\n    display_mode=DisplayMode.DETAILED  \n)"
      }
    ],
    "relevance": 0.815
  },
  {
    "codeTitle": "Content Filtering Command Example",
    "codeDescription": "Command example showing how to apply content filtering to focus on relevant content using a filter configuration file.",
    "codeLanguage": "bash",
    "codeTokens": 47,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_14",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crwl https://example.com -f filter_bm25.yml -o markdown-fit"
      }
    ],
    "relevance": 0.81
  },
  {
    "codeTitle": "Browser Configuration via Command Line",
    "codeDescription": "Examples showing how to apply browser configuration either through a YAML file or directly as command-line parameters.",
    "codeLanguage": "bash",
    "codeTokens": 75,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI CLI Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "# Using config file\ncrwl https://example.com -B browser.yml\n\n# Using direct parameters\ncrwl https://example.com -b \"headless=true,viewport_width=1280,user_agent_mode=random\""
      }
    ],
    "relevance": 0.81
  },
  {
    "codeTitle": "Initializing and Using Performance Monitoring in Crawl4AI",
    "codeDescription": "This snippet demonstrates how to initialize and start the CrawlMonitor for real-time monitoring of crawler operations, resource usage, and system health. It shows configuration of monitoring mode, refresh rate, and specific metrics to track.",
    "codeLanguage": "python",
    "codeTokens": 164,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-11_snippet_7",
    "pageTitle": "Crawl4AI Project Strategic Roadmap and Implementation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.monitor import CrawlMonitor\n\n# Initialize monitoring\nmonitor = CrawlMonitor()\n\n# Start monitoring with CLI interface\nawait monitor.start(\n    mode=\"cli\",  # or \"gui\"\n    refresh_rate=\"1s\",\n    metrics={\n        \"resources\": [\"cpu\", \"memory\", \"network\"],\n        \"crawls\": [\"active\", \"queued\", \"completed\"],\n        \"performance\": [\"success_rate\", \"response_times\"]\n    }\n)"
      }
    ],
    "relevance": 0.81
  },
  {
    "codeTitle": "Finding Playwright Chromium Binary Location (Bash)",
    "codeDescription": "Command to locate the Playwright-managed Chromium binary installation on the system",
    "codeLanguage": "bash",
    "codeTokens": 37,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-11_snippet_0",
    "pageTitle": "Setting up Browser Profiles with Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "python -m playwright install --dry-run"
      },
      {
        "language": "bash",
        "code": "playwright install --dry-run"
      }
    ],
    "relevance": 0.81
  },
  {
    "codeTitle": "Full Docker Compose Configuration",
    "codeDescription": "Complete Docker Compose configuration with all available options and resource limits.",
    "codeLanguage": "yaml",
    "codeTokens": 179,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_7",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "yaml",
        "code": "version: '3.8'\n\nservices:\n  crawl4ai:\n    image: unclecode/crawl4ai:all\n    ports:\n      - \"11235:11235\"\n    environment:\n      - CRAWL4AI_API_TOKEN=${CRAWL4AI_API_TOKEN:-}  # Optional API security\n      - MAX_CONCURRENT_TASKS=5\n      # LLM Provider Keys\n      - OPENAI_API_KEY=${OPENAI_API_KEY:-}\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}\n    volumes:\n      - /dev/shm:/dev/shm\n    deploy:\n      resources:\n        limits:\n          memory: 4G\n        reservations:\n          memory: 1G"
      }
    ],
    "relevance": 0.81
  },
  {
    "codeTitle": "Launching Chromium with Custom Profile (Multi-Platform)",
    "codeDescription": "Platform-specific commands to launch Playwright's Chromium with a custom user data directory",
    "codeLanguage": "bash",
    "codeTokens": 60,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-11_snippet_1",
    "pageTitle": "Setting up Browser Profiles with Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "~/.cache/ms-playwright/chromium-1234/chrome-linux/chrome \\\n    --user-data-dir=/home/<you>/my_chrome_profile"
      },
      {
        "language": "bash",
        "code": "~/Library/Caches/ms-playwright/chromium-1234/chrome-mac/Chromium.app/Contents/MacOS/Chromium \\\n    --user-data-dir=/Users/<you>/my_chrome_profile"
      },
      {
        "language": "powershell",
        "code": "\"C:\\Users\\<you>\\AppData\\Local\\ms-playwright\\chromium-1234\\chrome-win\\chrome.exe\" ^\n    --user-data-dir=\"C:\\Users\\<you>\\my_chrome_profile\""
      }
    ],
    "relevance": 0.805
  },
  {
    "codeTitle": "Using Crawl4AI Command-Line Interface",
    "codeDescription": "Examples of using the Crawl4AI command-line interface (CLI) for different use cases including basic crawling, deep crawling with BFS strategy, and extracting specific information using LLM.",
    "codeLanguage": "bash",
    "codeTokens": 138,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "bash",
        "code": "# Basic crawl with markdown output\ncrwl https://www.nbcnews.com/business -o markdown\n\n# Deep crawl with BFS strategy, max 10 pages\ncrwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10\n\n# Use LLM extraction with a specific question\ncrwl https://www.example.com/products -q \"Extract all product prices\""
      }
    ],
    "relevance": 0.805
  },
  {
    "codeTitle": "Environment Configuration",
    "codeDescription": "Environment variable configuration for API tokens and settings.",
    "codeLanguage": "env",
    "codeTokens": 34,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_6",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "env",
        "code": "CRAWL4AI_API_TOKEN=your_secret_token"
      }
    ],
    "relevance": 0.805
  },
  {
    "codeTitle": "Adding PruningContentFilter Documentation in Markdown",
    "codeDescription": "Expands documentation to include detailed explanation of the PruningContentFilter for users.",
    "codeLanguage": "diff",
    "codeTokens": 40,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_4",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "diff",
        "code": "Added detailed section explaining the PruningContentFilter."
      }
    ],
    "relevance": 0.805
  },
  {
    "codeTitle": "Python Testing Implementation",
    "codeDescription": "Python code for testing both secured and unsecured Crawl4AI deployments.",
    "codeLanguage": "python",
    "codeTokens": 243,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_8",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "import requests\n\n# For unsecured instances\ndef test_unsecured():\n    # Health check\n    health = requests.get(\"http://localhost:11235/health\")\n    print(\"Health check:\", health.json())\n\n    # Basic crawl\n    response = requests.post(\n        \"http://localhost:11235/crawl\",\n        json={\n            \"urls\": \"https://www.nbcnews.com/business\",\n            \"priority\": 10\n        }\n    )\n    task_id = response.json()[\"task_id\"]\n    print(\"Task ID:\", task_id)\n\n# For secured instances\ndef test_secured(api_token):\n    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n    \n    # Basic crawl with authentication\n    response = requests.post(\n        \"http://localhost:11235/crawl\",\n        headers=headers,\n        json={\n            \"urls\": \"https://www.nbcnews.com/business\",\n            \"priority\": 10\n        }\n    )\n    task_id = response.json()[\"task_id\"]\n    print(\"Task ID:\", task_id)"
      }
    ],
    "relevance": 0.805
  },
  {
    "codeTitle": "Using Builtin Browser Mode in CLI Crawling with Crawl4AI",
    "codeDescription": "This snippet demonstrates how to use the builtin browser mode when crawling via the CLI in Crawl4AI. It shows the command to crawl a website using the builtin browser mode.",
    "codeLanguage": "bash",
    "codeTokens": 63,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-11_snippet_2",
    "pageTitle": "Builtin Browser in Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "crwl https://example.com -b \"browser_mode=builtin\""
      }
    ],
    "relevance": 0.8
  },
  {
    "codeTitle": "Basic Crawl4AI Usage Example",
    "codeDescription": "Minimal Python script demonstrating basic web crawling functionality using AsyncWebCrawler.",
    "codeLanguage": "python",
    "codeTokens": 116,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI Installation and Setup Guide",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.example.com\",\n        )\n        print(result.markdown[:300])  # Show the first 300 characters of extracted text\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.8
  },
  {
    "codeTitle": "Installing Basic Crawl4AI Package with Playwright",
    "codeDescription": "Basic installation command for Crawl4AI with Playwright dependencies for web crawling and scraping tasks.",
    "codeLanguage": "bash",
    "codeTokens": 45,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI Installation Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai\nplaywright install # Install Playwright dependencies"
      }
    ],
    "relevance": 0.8
  },
  {
    "codeTitle": "Quick Test API Example",
    "codeDescription": "Python code example demonstrating how to submit a crawl job and poll for results using the API.",
    "codeLanguage": "python",
    "codeTokens": 121,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_7",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "python",
        "code": "import requests\n\n# Submit a crawl job\nresponse = requests.post(\n    \"http://localhost:11235/crawl\",\n    json={\"urls\": \"https://example.com\", \"priority\": 10}\n)\ntask_id = response.json()[\"task_id\"]\n\n# Continue polling until the task is complete (status=\"completed\")\nresult = requests.get(f\"http://localhost:11235/task/{task_id}\")"
      }
    ],
    "relevance": 0.8
  },
  {
    "codeTitle": "Running Crawl4AI Diagnostics",
    "codeDescription": "Command to run diagnostic checks for Python compatibility and Playwright installation.",
    "codeLanguage": "bash",
    "codeTokens": 32,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI Installation and Setup Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crawl4ai-doctor"
      }
    ],
    "relevance": 0.795
  },
  {
    "codeTitle": "Browser Management with AsyncWebCrawler in Python",
    "codeDescription": "Demonstrates the use of managed browser sessions and persistent contexts in AsyncWebCrawler for improved performance and state management across requests.",
    "codeLanguage": "python",
    "codeTokens": 181,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_10",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "async def browser_management_demo():\n    user_data_dir = os.path.join(Path.home(), \".crawl4ai\", \"user-data-dir\")\n    os.makedirs(user_data_dir, exist_ok=True)  # Ensure directory exists\n    async with AsyncWebCrawler(\n        use_managed_browser=True,\n        user_data_dir=user_data_dir,\n        use_persistent_context=True,\n        verbose=True\n    ) as crawler:\n        result1 = await crawler.arun(\n            url=\"https://example.com\", session_id=\"my_session\"\n        )\n        result2 = await crawler.arun(\n            url=\"https://example.com/anotherpage\", session_id=\"my_session\"\n        )\n\nasyncio.run(browser_management_demo())"
      }
    ],
    "relevance": 0.795
  },
  {
    "codeTitle": "BM25ContentFilter Configuration for Targeted Content Extraction",
    "codeDescription": "Sets up a BM25ContentFilter to focus on content related to a specific search query. This filter is used with DefaultMarkdownGenerator to produce more relevant markdown output by filtering out irrelevant content.",
    "codeLanguage": "python",
    "codeTokens": 157,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-11_snippet_2",
    "pageTitle": "Markdown Generation with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\nfrom crawl4ai import CrawlerRunConfig\n\nbm25_filter = BM25ContentFilter(\n    user_query=\"machine learning\",\n    bm25_threshold=1.2,\n    use_stemming=True\n)\n\nmd_generator = DefaultMarkdownGenerator(\n    content_filter=bm25_filter,\n    options={\"ignore_links\": True}\n)\n\nconfig = CrawlerRunConfig(markdown_generator=md_generator)"
      }
    ],
    "relevance": 0.795
  },
  {
    "codeTitle": "Docker Deployment Commands for Crawl4AI",
    "codeDescription": "These bash commands demonstrate how to build and run a Docker container for Crawl4AI. It includes steps to build the image and run the container, exposing the necessary port for the FastAPI server.",
    "codeLanguage": "bash",
    "codeTokens": 101,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "bash",
        "code": "# Build the image (from the project root)\ndocker build -t crawl4ai .\n\n# Run the container\ndocker run -d -p 8000:8000 --name crawl4ai crawl4ai"
      }
    ],
    "relevance": 0.79
  },
  {
    "codeTitle": "Extracting Structured Data with LLMs in Python",
    "codeDescription": "This snippet demonstrates how to extract structured data using Language Models (LLMs) with Crawl4AI. It configures an LLM extraction strategy to extract OpenAI model fees from a pricing page.",
    "codeLanguage": "python",
    "codeTokens": 478,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_10",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "python",
        "code": "import os\nimport asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig\nfrom crawl4ai.extraction_strategy import LLMExtractionStrategy\nfrom pydantic import BaseModel, Field\n\nclass OpenAIModelFee(BaseModel):\n    model_name: str = Field(..., description=\"Name of the OpenAI model.\")\n    input_fee: str = Field(..., description=\"Fee for input token for the OpenAI model.\")\n    output_fee: str = Field(..., description=\"Fee for output token for the OpenAI model.\")\n\nasync def main():\n    browser_config = BrowserConfig(verbose=True)\n    run_config = CrawlerRunConfig(\n        word_count_threshold=1,\n        extraction_strategy=LLMExtractionStrategy(\n            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2\n            # provider=\"ollama/qwen2\", api_token=\"no-token\", \n            llm_config = LLMConfig(provider=\"openai/gpt-4o\", api_token=os.getenv('OPENAI_API_KEY')), \n            schema=OpenAIModelFee.schema(),\n            extraction_type=\"schema\",\n            instruction=\"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. \n            Do not miss any models in the entire content. One extracted model JSON format should look like this: \n            {\"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\"}.\"\"\"\n        ),            \n        cache_mode=CacheMode.BYPASS,\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url='https://openai.com/api/pricing/',\n            config=run_config\n        )\n        print(result.extracted_content)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.79
  },
  {
    "codeTitle": "Development Installation of Crawl4AI",
    "codeDescription": "Commands for installing Crawl4AI from source code for development purposes.",
    "codeLanguage": "bash",
    "codeTokens": 63,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI Installation Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "git clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai\npip install -e \".[all]\"\nplaywright install # Install Playwright dependencies"
      }
    ],
    "relevance": 0.79
  },
  {
    "codeTitle": "Implementing Domain Specific Scrapers",
    "codeDescription": "Implementation of specialized extraction strategies optimized for common website types and platforms, including academic and e-commerce sites.",
    "codeLanguage": "python",
    "codeTokens": 207,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI Project Strategic Roadmap and Implementation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.extractors import AcademicExtractor, EcommerceExtractor\n\nasync with AsyncWebCrawler() as crawler:\n    # Academic paper extraction\n    papers = await crawler.arun(\n        url=\"https://arxiv.org/list/cs.AI/recent\",\n        extractor=\"academic\",  # Built-in extractor type\n        site_type=\"arxiv\",     # Specific site optimization\n        extract_fields=[\n            \"title\", \n            \"authors\", \n            \"abstract\", \n            \"citations\"\n        ]\n    )\n    \n    # E-commerce product data\n    products = await crawler.arun(\n        url=\"https://store.example.com/products\",\n        extractor=\"ecommerce\",\n        extract_fields=[\n            \"name\",\n            \"price\",\n            \"availability\",\n            \"reviews\"\n        ]\n    )"
      }
    ],
    "relevance": 0.79
  },
  {
    "codeTitle": "Implementing Attribution Badges in HTML",
    "codeDescription": "HTML code snippets for adding different themed attribution badges to reference Crawl4AI usage in projects. Includes animated, dark, light, and neon themed options.",
    "codeLanguage": "html",
    "codeTokens": 382,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_13",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "html",
        "code": "<!-- Disco Theme (Animated) -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-disco.svg\" alt=\"Powered by Crawl4AI\" width=\"200\"/>\n</a>\n\n<!-- Night Theme (Dark with Neon) -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-night.svg\" alt=\"Powered by Crawl4AI\" width=\"200\"/>\n</a>\n\n<!-- Dark Theme (Classic) -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-dark.svg\" alt=\"Powered by Crawl4AI\" width=\"200\"/>\n</a>\n\n<!-- Light Theme (Classic) -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://raw.githubusercontent.com/unclecode/crawl4ai/main/docs/assets/powered-by-light.svg\" alt=\"Powered by Crawl4AI\" width=\"200\"/>\n</a>\n\n<!-- Simple Shield Badge -->\n<a href=\"https://github.com/unclecode/crawl4ai\">\n  <img src=\"https://img.shields.io/badge/Powered%20by-Crawl4AI-blue?style=flat-square\" alt=\"Powered by Crawl4AI\"/>\n</a>"
      }
    ],
    "relevance": 0.79
  },
  {
    "codeTitle": "Using storage_state as a File in AsyncWebCrawler",
    "codeDescription": "This Python script shows how to use storage_state as a file path when initializing AsyncWebCrawler. It loads session data from a JSON file and performs a crawl on a protected page.",
    "codeLanguage": "python",
    "codeTokens": 165,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-11_snippet_2",
    "pageTitle": "Using storage_state for Session Management in Crawl4ai",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler\n\nasync def main():\n    async with AsyncWebCrawler(\n        headless=True,\n        storage_state=\"mystate.json\"  # Uses a JSON file instead of a dictionary\n    ) as crawler:\n        result = await crawler.arun(url='https://example.com/protected')\n        if result.success:\n            print(\"Crawl succeeded with pre-loaded session data!\")\n            print(\"Page HTML length:\", len(result.html))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.79
  },
  {
    "codeTitle": "Initializing SlidingWindowChunking in Python for Overlapping Text Chunks",
    "codeDescription": "This snippet demonstrates the initialization of SlidingWindowChunking class, which creates overlapping chunks with a sliding window approach. It takes window size and step size as parameters.",
    "codeLanguage": "python",
    "codeTokens": 88,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_4",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "SlidingWindowChunking(\n    window_size: int = 100,    # Window size in words\n    step: int = 50             # Step size between windows\n)"
      }
    ],
    "relevance": 0.785
  },
  {
    "codeTitle": "Development Installation Commands",
    "codeDescription": "Commands for installing Crawl4AI for development purposes with various optional features.",
    "codeLanguage": "bash",
    "codeTokens": 105,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_6",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "bash",
        "code": "git clone https://github.com/unclecode/crawl4ai.git\ncd crawl4ai\npip install -e .\npip install -e \".[torch]\"\npip install -e \".[transformer]\"\npip install -e \".[cosine]\"\npip install -e \".[sync]\"\npip install -e \".[all]\""
      }
    ],
    "relevance": 0.785
  },
  {
    "codeTitle": "Using New CacheMode in AsyncWebCrawler in Python",
    "codeDescription": "Shows how to use the new CacheMode enum for fine-grained cache control in AsyncWebCrawler, replacing deprecated cache control flags.",
    "codeLanguage": "python",
    "codeTokens": 83,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_11",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import CacheMode\n\ncrawler = AsyncWebCrawler(always_bypass_cache=True)\nresult = await crawler.arun(url=\"https://example.com\", cache_mode=CacheMode.BYPASS)"
      }
    ],
    "relevance": 0.785
  },
  {
    "codeTitle": "Updating Schema Reference in Python",
    "codeDescription": "Changes the schema reference for LLMExtractionStrategy from OpenAIModelFee.schema() to OpenAIModelFee.model_json_schema() for better compatibility.",
    "codeLanguage": "python",
    "codeTokens": 64,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_1",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "# Old\nOpenAIModelFee.schema()\n\n# New\nOpenAIModelFee.model_json_schema()"
      }
    ],
    "relevance": 0.78
  },
  {
    "codeTitle": "Initializing RegexChunking in Python for Text Splitting",
    "codeDescription": "This code snippet shows the initialization of RegexChunking class, which is used to split text based on regex patterns. It takes a list of regex patterns as an optional parameter.",
    "codeLanguage": "python",
    "codeTokens": 83,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#2025-04-11_snippet_3",
    "pageTitle": "Extraction & Chunking Strategies API Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "RegexChunking(\n    patterns: List[str] = None  # Regex patterns for splitting\n                               # Default: [r'\\n\\n']\n)"
      }
    ],
    "relevance": 0.78
  },
  {
    "codeTitle": "Enabling verbose logging in Crawl4AI",
    "codeDescription": "This snippet shows how to enable verbose logging in the BrowserConfig for debugging purposes when using Crawl4AI.",
    "codeLanguage": "python",
    "codeTokens": 87,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-11_snippet_4",
    "pageTitle": "Simple Web Crawling with Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "browser_config = BrowserConfig(verbose=True)\n\nasync with AsyncWebCrawler(config=browser_config) as crawler:\n    run_config = CrawlerRunConfig()\n    result = await crawler.arun(url=\"https://example.com\", config=run_config)"
      }
    ],
    "relevance": 0.78
  },
  {
    "codeTitle": "Citation Format in BibTeX",
    "codeDescription": "Academic citation format in BibTeX for referencing Crawl4AI in research papers and publications.",
    "codeLanguage": "bibtex",
    "codeTokens": 133,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_15",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "bibtex",
        "code": "@software{crawl4ai2024,\n  author = {UncleCode},\n  title = {Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper},\n  year = {2024},\n  publisher = {GitHub},\n  journal = {GitHub Repository},\n  howpublished = {\\url{https://github.com/unclecode/crawl4ai}},\n  commit = {Please use the commit hash you're working with}\n}"
      }
    ],
    "relevance": 0.78
  },
  {
    "codeTitle": "Basic Batch Mode Example with arun_many in Python",
    "codeDescription": "A minimal example showing how to use arun_many in batch mode to crawl multiple URLs. The function returns a list of CrawlResult objects that can be processed after all crawls complete.",
    "codeLanguage": "python",
    "codeTokens": 136,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-11_snippet_1",
    "pageTitle": "arun_many Function Reference",
    "codeList": [
      {
        "language": "python",
        "code": "# Minimal usage: The default dispatcher will be used\nresults = await crawler.arun_many(\n    urls=[\"https://site1.com\", \"https://site2.com\"],\n    config=CrawlerRunConfig(stream=False)  # Default behavior\n)\n\nfor res in results:\n    if res.success:\n        print(res.url, \"crawled OK!\")\n    else:\n        print(\"Failed:\", res.url, \"-\", res.error_message)"
      }
    ],
    "relevance": 0.78
  },
  {
    "codeTitle": "Implementing Automated Schema Generator",
    "codeDescription": "Example of using the Automated Schema Generator to create JsonCssExtractionStrategy schemas from natural language descriptions for structured data extraction.",
    "codeLanguage": "python",
    "codeTokens": 155,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI Project Strategic Roadmap and Implementation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.schema import SchemaGenerator\n\n# Generate schema from natural language description\ngenerator = SchemaGenerator()\nschema = await generator.generate(\n    url=\"https://news-website.com\",\n    description=\"For each news article on the page, I need the headline, publication date, and main image\"\n)\n\n# Use generated schema with crawler\nasync with AsyncWebCrawler() as crawler:\n    result = await crawler.arun(\n        url=\"https://news-website.com\",\n        extraction_strategy=schema\n    )"
      }
    ],
    "relevance": 0.775
  },
  {
    "codeTitle": "Installation Command for Crawl4AI",
    "codeDescription": "Simple pip installation command showing how to install Crawl4AI with all optional dependencies.",
    "codeLanguage": "bash",
    "codeTokens": 38,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/index.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI Blog Documentation",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install \"crawl4ai[all]\""
      }
    ],
    "relevance": 0.775
  },
  {
    "codeTitle": "Configuring Page Limits in Crawl4AI",
    "codeDescription": "Demonstrates how to set maximum page limits for crawling to control execution scope and resources.",
    "codeLanguage": "python",
    "codeTokens": 65,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_12",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "# Limit to exactly 20 pages regardless of depth\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=3,\n    max_pages=20\n)"
      }
    ],
    "relevance": 0.775
  },
  {
    "codeTitle": "Retrieving Task Status from Crawl4AI API",
    "codeDescription": "This HTTP request demonstrates how to retrieve the status of a specific task from the Crawl4AI API using the task ID.",
    "codeLanguage": "http",
    "codeTokens": 46,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_20",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "http",
        "code": "GET /task/{task_id}"
      }
    ],
    "relevance": 0.77
  },
  {
    "codeTitle": "Visualizing Crawl4AI Roadmap with Mermaid",
    "codeDescription": "A mermaid diagram showing the strategic roadmap of Crawl4AI project divided into four main sections: Advanced Crawling Systems, Specialized Features, Development Tools, and Community & Growth.",
    "codeLanguage": "mermaid",
    "codeTokens": 344,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI Project Strategic Roadmap and Implementation",
    "codeList": [
      {
        "language": "mermaid",
        "code": "%%{init: {'themeVariables': { 'fontSize': '14px'}}}%%\ngraph TD\n    subgraph A1[Advanced Crawling Systems ]\n        A[\"`\n         Graph Crawler \n         Question-Based Crawler\n         Knowledge-Optimal Crawler\n         Agentic Crawler\n        `\"]\n    end\n\n    subgraph A2[Specialized Features ]\n        B[\"`\n         Automated Schema Generator\n         Domain-Specific Scrapers\n         \n         \n        `\"]\n    end\n\n    subgraph A3[Development Tools ]\n        C[\"`\n         Interactive Playground\n         Performance Monitor\n         Cloud Integration\n         \n        `\"]\n    end\n\n    subgraph A4[Community & Growth ]\n        D[\"`\n         Sponsorship Program\n         Educational Content\n         \n         \n        `\"]\n    end\n\n    classDef default fill:#f9f9f9,stroke:#333,stroke-width:2px\n    classDef section fill:#f0f0f0,stroke:#333,stroke-width:4px,rx:10\n    class A1,A2,A3,A4 section\n\n    %% Layout hints\n    A1 --> A2[\" \"]\n    A3 --> A4[\" \"]\n    linkStyle 0,1 stroke:none"
      }
    ],
    "relevance": 0.77
  },
  {
    "codeTitle": "Creating Unit Tests for PruningContentFilter in Python",
    "codeDescription": "Adds comprehensive test cases for various scenarios using the PruningContentFilter.",
    "codeLanguage": "diff",
    "codeTokens": 41,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_5",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "diff",
        "code": "Created test cases for various scenarios using the PruningContentFilter."
      }
    ],
    "relevance": 0.77
  },
  {
    "codeTitle": "Implementing Knowledge-Optimal Crawler",
    "codeDescription": "Implementation of an intelligent crawling system that optimizes data extraction while maximizing knowledge acquisition for specific objectives.",
    "codeLanguage": "python",
    "codeTokens": 224,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI Project Strategic Roadmap and Implementation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.optimization import KnowledgeOptimizer\n\nasync with AsyncWebCrawler() as crawler:\n    optimizer = KnowledgeOptimizer(\n        objective=\"Understand GPU instance pricing and limitations across cloud providers\",\n        required_knowledge=[\n            \"pricing structure\",\n            \"GPU specifications\",\n            \"usage limits\",\n            \"availability zones\"\n        ],\n        confidence_threshold=0.85\n    )\n    \n    result = await crawler.arun(\n        urls=[\n            \"https://aws.amazon.com/ec2/pricing/\",\n            \"https://cloud.google.com/gpu\",\n            \"https://azure.microsoft.com/pricing/\"\n        ],\n        optimizer=optimizer,\n        optimization_mode=\"minimal_extraction\"\n    )\n    \n    print(f\"Knowledge Coverage: {result.knowledge_coverage}\")\n    print(f\"Data Efficiency: {result.efficiency_ratio}\")\n    print(f\"Extracted Content: {result.optimal_content}\")"
      }
    ],
    "relevance": 0.77
  },
  {
    "codeTitle": "Crawl4AI API Health Check Endpoint",
    "codeDescription": "This HTTP request demonstrates how to perform a health check on the Crawl4AI API.",
    "codeLanguage": "http",
    "codeTokens": 35,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_18",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "http",
        "code": "GET /health"
      }
    ],
    "relevance": 0.765
  },
  {
    "codeTitle": "Proxy Configuration in JSON Format",
    "codeDescription": "JSON structure for proxy configuration to be used with BrowserConfig. It includes server URL, username, and password fields for authenticating with a proxy server.",
    "codeLanguage": "json",
    "codeTokens": 79,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-11_snippet_1",
    "pageTitle": "Browser, Crawler & LLM Configuration for Crawl4AI",
    "codeList": [
      {
        "language": "json",
        "code": "{\n    \"server\": \"http://proxy.example.com:8080\", \n    \"username\": \"...\", \n    \"password\": \"...\"\n}"
      }
    ],
    "relevance": 0.765
  },
  {
    "codeTitle": "Applying nest_asyncio for Asyncio Compatibility in Python",
    "codeDescription": "This code imports and applies nest_asyncio to allow asyncio operations in Colab. It's a prerequisite for running asynchronous code in notebook environments.",
    "codeLanguage": "python",
    "codeTokens": 81,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI v0.3.72 Release Announcement and Feature Demonstration",
    "codeList": [
      {
        "language": "python",
        "code": "# Import nest_asyncio and apply it to allow asyncio in Colab\nimport nest_asyncio\nnest_asyncio.apply()\n\nprint('Setup complete!')"
      }
    ],
    "relevance": 0.765
  },
  {
    "codeTitle": "Deploying Crawl4AI to Cloud Platforms",
    "codeDescription": "This code snippet illustrates the process of deploying Crawl4AI to cloud platforms using the CloudDeployer. It demonstrates setting up a crawler cluster with auto-scaling, region selection, and monitoring integration on AWS (with options for GCP and Azure).",
    "codeLanguage": "python",
    "codeTokens": 237,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-11_snippet_8",
    "pageTitle": "Crawl4AI Project Strategic Roadmap and Implementation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.deploy import CloudDeployer\n\n# Initialize deployer\ndeployer = CloudDeployer()\n\n# Deploy crawler service\ndeployment = await deployer.deploy(\n    service_name=\"crawler-cluster\",\n    platform=\"aws\",  # or \"gcp\", \"azure\"\n    config={\n        \"instance_type\": \"compute-optimized\",\n        \"auto_scaling\": {\n            \"min_instances\": 2,\n            \"max_instances\": 10,\n            \"scale_based_on\": \"cpu_usage\"\n        },\n        \"region\": \"us-east-1\",\n        \"monitoring\": True\n    }\n)\n\n# Get deployment status and endpoints\nprint(f\"Service Status: {deployment.status}\")\nprint(f\"API Endpoint: {deployment.endpoint}\")\nprint(f\"Monitor URL: {deployment.monitor_url}\")"
      }
    ],
    "relevance": 0.765
  },
  {
    "codeTitle": "Enabling robots.txt Compliance",
    "codeDescription": "Shows how to enable robots.txt compliance in the crawler configuration.",
    "codeLanguage": "python",
    "codeTokens": 42,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_9",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "config = CrawlerRunConfig(check_robots_txt=True)"
      }
    ],
    "relevance": 0.76
  },
  {
    "codeTitle": "Defining the SSLCertificate Class with Key Methods and Properties in Python",
    "codeDescription": "Class definition showing the main structure of the SSLCertificate class, including its primary methods for loading certificates from different sources and exporting them in various formats, along with common properties like issuer, subject, and validity information.",
    "codeLanguage": "python",
    "codeTokens": 152,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/ssl-certificate.md#2025-04-11_snippet_0",
    "pageTitle": "SSLCertificate Class Reference Guide",
    "codeList": [
      {
        "language": "python",
        "code": "class SSLCertificate:\n    \"\"\"\n    Represents an SSL certificate with methods to export in various formats.\n\n    Main Methods:\n    - from_url(url, timeout=10)\n    - from_file(file_path)\n    - from_binary(binary_data)\n    - to_json(filepath=None)\n    - to_pem(filepath=None)\n    - to_der(filepath=None)\n    ...\n\n    Common Properties:\n    - issuer\n    - subject\n    - valid_from\n    - valid_until\n    - fingerprint\n    \"\"\"\n"
      }
    ],
    "relevance": 0.76
  },
  {
    "codeTitle": "Setting Score Threshold in Crawl4AI",
    "codeDescription": "Shows how to implement score-based filtering to only crawl high-quality pages that meet minimum relevance criteria.",
    "codeLanguage": "python",
    "codeTokens": 101,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#2025-04-11_snippet_13",
    "pageTitle": "Configuring Deep Crawling in Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "# Only follow links with scores above 0.4\nstrategy = DFSDeepCrawlStrategy(\n    max_depth=2,\n    url_scorer=KeywordRelevanceScorer(keywords=[\"api\", \"guide\", \"reference\"]),\n    score_threshold=0.4  # Skip URLs with scores below this value\n)"
      }
    ],
    "relevance": 0.755
  },
  {
    "codeTitle": "Docker Compose Hub Image Commands",
    "codeDescription": "Commands for running Crawl4AI using pre-built images from Docker Hub.",
    "codeLanguage": "bash",
    "codeTokens": 41,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_2",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "docker-compose -f docker-compose.hub.yml up -d"
      }
    ],
    "relevance": 0.755
  },
  {
    "codeTitle": "Installing All Crawl4AI Features",
    "codeDescription": "Command to install Crawl4AI with all optional features including PyTorch and transformers.",
    "codeLanguage": "bash",
    "codeTokens": 44,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-11_snippet_6",
    "pageTitle": "Crawl4AI Installation and Setup Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai[all]\ncrawl4ai-setup"
      }
    ],
    "relevance": 0.75
  },
  {
    "codeTitle": "Example Schema Output for CSS Extraction",
    "codeDescription": "Shows the expected JSON output format for the LLM-generated schema, including field definitions for product extraction.",
    "codeLanguage": "json",
    "codeTokens": 103,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-11_snippet_8",
    "pageTitle": "Crawl4AI v0.5.0 Release Notes",
    "codeList": [
      {
        "language": "json",
        "code": "{\n  \"name\": \"ProductExtractor\",\n  \"baseSelector\": \"div.product\",\n  \"fields\": [\n      {\"name\": \"name\", \"selector\": \"h2\", \"type\": \"text\"},\n      {\"name\": \"price\", \"selector\": \".price\", \"type\": \"text\"}\n    ]\n }"
      }
    ],
    "relevance": 0.75
  },
  {
    "codeTitle": "Text Citation Format",
    "codeDescription": "Plain text citation format for referencing Crawl4AI in academic works.",
    "codeLanguage": "text",
    "codeTokens": 78,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_16",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "text",
        "code": "UncleCode. (2024). Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper [Computer software]. \nGitHub. https://github.com/unclecode/crawl4ai"
      }
    ],
    "relevance": 0.75
  },
  {
    "codeTitle": "Installing Crawl4AI Basic Package",
    "codeDescription": "Basic installation commands for Crawl4AI using pip, including the browser setup command. Includes manual Playwright installation commands for troubleshooting.",
    "codeLanguage": "bash",
    "codeTokens": 58,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_3",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai\ncrawl4ai-setup"
      }
    ],
    "relevance": 0.75
  },
  {
    "codeTitle": "Installing Crawl4AI with Transformer Support",
    "codeDescription": "Installation command for Crawl4AI with Transformer dependencies for text summarization and Hugging Face models.",
    "codeLanguage": "bash",
    "codeTokens": 41,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-11_snippet_2",
    "pageTitle": "Crawl4AI Installation Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai[transformer]"
      }
    ],
    "relevance": 0.745
  },
  {
    "codeTitle": "Visualizing Event-Driven Crawl Process Flow with Mermaid",
    "codeDescription": "A sequence diagram illustrating the interaction between Client, Server, and Crawler in the proposed event-driven architecture. The diagram shows the flow of starting a crawl request, receiving streamed events, sending instructions based on process ID, and completing the crawl.",
    "codeLanguage": "mermaid",
    "codeTokens": 176,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/articles/dockerize_hooks.md#2025-04-11_snippet_0",
    "pageTitle": "Event Streams and Interactive Hooks in Crawl4AI",
    "codeList": [
      {
        "language": "mermaid",
        "code": "sequenceDiagram\n    participant Client\n    participant Server\n    participant Crawler\n\n    Client->>Server: Start crawl request\n    Server->>Crawler: Initiate crawl with Process ID\n    Crawler-->>Server: Event: Page hit\n    Server-->>Client: Stream: Page hit event\n    Client->>Server: Instruction for Process ID\n    Server->>Crawler: Update crawl with new instructions\n    Crawler-->>Server: Event: Crawl completed\n    Server-->>Client: Stream: Crawl completed"
      }
    ],
    "relevance": 0.745
  },
  {
    "codeTitle": "Defining API Keys for AI Services in Python",
    "codeDescription": "This snippet defines environment variables or configuration constants for API keys from various AI service providers including Groq, OpenAI, and Anthropic. These keys are required for authenticating requests to their respective APIs.",
    "codeLanguage": "python",
    "codeTokens": 97,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/.env.txt#2025-04-11_snippet_0",
    "pageTitle": "API Key Configuration File",
    "codeList": [
      {
        "language": "python",
        "code": "GROQ_API_KEY = \"YOUR_GROQ_API\"\nOPENAI_API_KEY = \"YOUR_OPENAI_API\"\nANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_API\"\n# You can add more API keys here"
      }
    ],
    "relevance": 0.745
  },
  {
    "codeTitle": "Installing Crawl4AI with PyTorch Support",
    "codeDescription": "Installation command for Crawl4AI with PyTorch dependencies for advanced text clustering features.",
    "codeLanguage": "bash",
    "codeTokens": 38,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI Installation Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai[torch]"
      }
    ],
    "relevance": 0.74
  },
  {
    "codeTitle": "Installing Crawl4AI with Transformer Support",
    "codeDescription": "Command to install Crawl4AI with Hugging Face transformer capabilities for summarization and generation.",
    "codeLanguage": "bash",
    "codeTokens": 46,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI Installation and Setup Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai[transformer]\ncrawl4ai-setup"
      }
    ],
    "relevance": 0.74
  },
  {
    "codeTitle": "Docker Compose Local Build Commands",
    "codeDescription": "Commands for building and managing Crawl4AI using local Docker Compose configuration.",
    "codeLanguage": "bash",
    "codeTokens": 41,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#2025-04-11_snippet_1",
    "pageTitle": "Docker Deployment Guide for Crawl4AI",
    "codeList": [
      {
        "language": "bash",
        "code": "docker-compose -f docker-compose.local.yml up -d"
      }
    ],
    "relevance": 0.735
  },
  {
    "codeTitle": "Downloading Required Models",
    "codeDescription": "CLI command to download required models for enhanced performance after installation.",
    "codeLanguage": "bash",
    "codeTokens": 30,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI Installation Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crawl4ai-download-models"
      }
    ],
    "relevance": 0.735
  },
  {
    "codeTitle": "Running Crawl4AI Setup",
    "codeDescription": "Command to initialize Crawl4AI, install browsers, and perform system checks.",
    "codeLanguage": "bash",
    "codeTokens": 34,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI Installation and Setup Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "crawl4ai-setup"
      }
    ],
    "relevance": 0.73
  },
  {
    "codeTitle": "Content Filtering with BM25ContentFilter in Python",
    "codeDescription": "Shows how to use the new BM25ContentFilter for extracting relevant content from web pages based on a user-provided query or page metadata.",
    "codeLanguage": "python",
    "codeTokens": 176,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_8",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.content_filter_strategy import BM25ContentFilter\n\nasync def filter_content(url, query):\n    async with AsyncWebCrawler() as crawler:\n        content_filter = BM25ContentFilter(user_query=query)\n        result = await crawler.arun(url=url, extraction_strategy=content_filter, fit_markdown=True)\n        print(result.extracted_content)  # Or result.fit_markdown for the markdown version\n        print(result.fit_html) # Or result.fit_html to show HTML with only the filtered content\n\nasyncio.run(filter_content(\"https://en.wikipedia.org/wiki/Apple\", \"fruit nutrition health\"))"
      }
    ],
    "relevance": 0.73
  },
  {
    "codeTitle": "Installing Crawl4AI via pip",
    "codeDescription": "Shows the command to install or upgrade to the latest version of Crawl4AI using pip.",
    "codeLanguage": "bash",
    "codeTokens": 44,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-11_snippet_8",
    "pageTitle": "Crawl4AI 0.4.3 Release Notes",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install -U crawl4ai"
      }
    ],
    "relevance": 0.725
  },
  {
    "codeTitle": "Updating README with PruningContentFilter in Markdown",
    "codeDescription": "Updates the README file to include usage and explanation for the new PruningContentFilter feature.",
    "codeLanguage": "diff",
    "codeTokens": 45,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_3",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "diff",
        "code": "Updated to include usage and explanation for the PruningContentFilter."
      }
    ],
    "relevance": 0.725
  },
  {
    "codeTitle": "Manual Playwright Installation Commands",
    "codeDescription": "Alternative commands for manually installing Playwright when automatic installation fails.",
    "codeLanguage": "bash",
    "codeTokens": 36,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_4",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "bash",
        "code": "playwright install"
      },
      {
        "language": "bash",
        "code": "python -m playwright install chromium"
      }
    ],
    "relevance": 0.72
  },
  {
    "codeTitle": "Closing Pages to Prevent Memory Leaks in Python",
    "codeDescription": "This code snippet ensures that pages are closed when no session_id is provided, preventing memory leaks caused by lingering pages after a crawl. It's implemented in a finally block for robust error handling.",
    "codeLanguage": "python",
    "codeTokens": 80,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#2025-04-11_snippet_0",
    "pageTitle": "Changelog Documentation for Crawl4AI",
    "codeList": [
      {
        "language": "python",
        "code": "finally:\n    # If no session_id is given we should close the page\n    if not config.session_id:\n        await page.close()"
      }
    ],
    "relevance": 0.72
  },
  {
    "codeTitle": "Installing Crawl4AI Basic Package",
    "codeDescription": "Basic installation command for the core Crawl4AI library using pip package manager.",
    "codeLanguage": "bash",
    "codeTokens": 35,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI Installation and Setup Guide",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai"
      }
    ],
    "relevance": 0.715
  },
  {
    "codeTitle": "Heuristic Markdown Generation with Clean and Fit Markdown in Python",
    "codeDescription": "This snippet demonstrates how to use Crawl4AI for heuristic markdown generation with clean and fit markdown. It configures the crawler with different content filtering strategies and markdown generation options.",
    "codeLanguage": "python",
    "codeTokens": 321,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_8",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "python",
        "code": "import asyncio\nfrom crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\nfrom crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n\nasync def main():\n    browser_config = BrowserConfig(\n        headless=True,  \n        verbose=True,\n    )\n    run_config = CrawlerRunConfig(\n        cache_mode=CacheMode.ENABLED,\n        markdown_generator=DefaultMarkdownGenerator(\n            content_filter=PruningContentFilter(threshold=0.48, threshold_type=\"fixed\", min_word_threshold=0)\n        ),\n        # markdown_generator=DefaultMarkdownGenerator(\n        #     content_filter=BM25ContentFilter(user_query=\"WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY\", bm25_threshold=1.0)\n        # ),\n    )\n    \n    async with AsyncWebCrawler(config=browser_config) as crawler:\n        result = await crawler.arun(\n            url=\"https://docs.micronaut.io/4.7.6/guide/\",\n            config=run_config\n        )\n        print(len(result.markdown.raw_markdown))\n        print(len(result.markdown.fit_markdown))\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "relevance": 0.715
  },
  {
    "codeTitle": "Text Attribution Format for Crawl4AI",
    "codeDescription": "Simple text format for attributing Crawl4AI usage in documentation.",
    "codeLanguage": "text",
    "codeTokens": 62,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_14",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "text",
        "code": "This project uses Crawl4AI (https://github.com/unclecode/crawl4ai) for web data extraction."
      }
    ],
    "relevance": 0.715
  },
  {
    "codeTitle": "Implementing Memory-Adaptive Dispatcher System in Crawl4AI",
    "codeDescription": "Demonstrates how to configure and use the new memory-adaptive dispatcher system that provides intelligent resource management and real-time monitoring. The system automatically throttles crawling based on memory usage and allows setting maximum concurrent sessions.",
    "codeLanguage": "python",
    "codeTokens": 250,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI 0.4.3 Release Notes",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DisplayMode\nfrom crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher, CrawlerMonitor\n\nasync def main():\n    urls = [\"https://example1.com\", \"https://example2.com\"] * 50\n    \n    # Configure memory-aware dispatch\n    dispatcher = MemoryAdaptiveDispatcher(\n        memory_threshold_percent=80.0,  # Auto-throttle at 80% memory\n        check_interval=0.5,             # Check every 0.5 seconds\n        max_session_permit=20,          # Max concurrent sessions\n        monitor=CrawlerMonitor(         # Real-time monitoring\n            display_mode=DisplayMode.DETAILED\n        )\n    )\n    \n    async with AsyncWebCrawler() as crawler:\n        results = await dispatcher.run_urls(\n            urls=urls,\n            crawler=crawler,\n            config=CrawlerRunConfig()\n        )"
      }
    ],
    "relevance": 0.702
  },
  {
    "codeTitle": "Specifying Python Dependencies with Version Requirements",
    "codeDescription": "This requirements file lists all Python packages needed for the crawl4ai project. It includes web framework components (FastAPI, uvicorn, gunicorn), rate limiting (slowapi), monitoring (prometheus), caching (redis), authentication (jwt), and validation libraries (dnspython, email-validator) with minimum version requirements where specified.",
    "codeLanguage": "plaintext",
    "codeTokens": 153,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/requirements.txt#2025-04-11_snippet_0",
    "pageTitle": "Python Dependency Requirements File",
    "codeList": [
      {
        "language": "plaintext",
        "code": "crawl4ai\nfastapi\nuvicorn\ngunicorn>=23.0.0\nslowapi>=0.1.9\nprometheus-fastapi-instrumentator>=7.0.2\nredis>=5.2.1\njwt>=1.3.1\ndnspython>=2.7.0\nemail-validator>=2.2.0"
      }
    ],
    "relevance": 0.687
  },
  {
    "codeTitle": "Implementing Question-Based Web Crawler",
    "codeDescription": "Example of using the Question-Based Crawler to discover and extract web content based on natural language questions. Features SerpiAPI integration and relevancy scoring.",
    "codeLanguage": "python",
    "codeTokens": 167,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-11_snippet_1",
    "pageTitle": "Crawl4AI Project Strategic Roadmap and Implementation",
    "codeList": [
      {
        "language": "python",
        "code": "from crawl4ai import AsyncWebCrawler\nfrom crawl4ai.discovery import QuestionBasedDiscovery\n\nasync with AsyncWebCrawler() as crawler:\n    discovery = QuestionBasedDiscovery(crawler)\n    results = await discovery.arun(\n        question=\"What are the system requirements for major cloud providers' GPU instances?\",\n        max_urls=5,\n        relevance_threshold=0.7\n    )\n    \n    for result in results:\n        print(f\"Source: {result.url} (Relevance: {result.relevance_score})\")\n        print(f\"Content: {result.markdown}\\n\")"
      }
    ],
    "relevance": 0.645
  },
  {
    "codeTitle": "Configuring Lazy Loading Image Support in Python",
    "codeDescription": "Demonstrates how to enable waiting for lazy-loaded images during crawling by setting wait_for_images parameter to True. This ensures all images are fully loaded before proceeding.",
    "codeLanguage": "python",
    "codeTokens": 80,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-11_snippet_0",
    "pageTitle": "Crawl4AI Release 0.4.1 Documentation",
    "codeList": [
      {
        "language": "python",
        "code": "await crawler.crawl(\n    url=\"https://example.com\",\n    wait_for_images=True  # Add this argument to ensure images are fully loaded\n)"
      }
    ],
    "relevance": 0.625
  },
  {
    "codeTitle": "Example Hooks for Different Crawling Stages",
    "codeDescription": "Provides examples of hook functions for different stages of the crawling process, including browser creation, page navigation, JavaScript execution, and HTML processing.",
    "codeLanguage": "python",
    "codeTokens": 56,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-11_snippet_8",
    "pageTitle": "Crawl4AI Documentation and Examples",
    "codeList": [
      {
        "language": "python",
        "code": "async def on_browser_created_hook(browser):\n    print(\"[Hook] Browser created\")"
      },
      {
        "language": "python",
        "code": "async def before_goto_hook(page):\n    await page.set_extra_http_headers({\"X-Test-Header\": \"test\"})"
      },
      {
        "language": "python",
        "code": "async def after_goto_hook(page):\n    print(f\"[Hook] Navigated to {page.url}\")"
      },
      {
        "language": "python",
        "code": "async def on_execution_started_hook(page):\n    print(\"[Hook] JavaScript execution started\")"
      },
      {
        "language": "python",
        "code": "async def before_return_html_hook(page, html):\n    print(f\"[Hook] HTML length: {len(html)}\")"
      }
    ],
    "relevance": 0.597
  },
  {
    "codeTitle": "Installing Synchronous Version",
    "codeDescription": "Command to install the deprecated synchronous version of Crawl4AI that uses Selenium.",
    "codeLanguage": "bash",
    "codeTokens": 46,
    "codeId": "https://github.com/unclecode/crawl4ai/blob/main/README.md#2025-04-11_snippet_5",
    "pageTitle": "Crawl4AI: Open-source Web Crawler & Scraper for AI Applications",
    "codeList": [
      {
        "language": "bash",
        "code": "pip install crawl4ai[sync]"
      }
    ],
    "relevance": 0.575
  }
]